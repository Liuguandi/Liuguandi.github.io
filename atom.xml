<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Angelo的代码工坊</title>
  
  <subtitle>I shut my eyes in order to see.</subtitle>
  <link href="https://values.keys.moe/atom.xml" rel="self"/>
  
  <link href="https://values.keys.moe/"/>
  <updated>2023-10-22T09:28:57.956Z</updated>
  <id>https://values.keys.moe/</id>
  
  <author>
    <name>Angelo</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Practical Accuracy Estimation for Efficient Deep Neural</title>
    <link href="https://values.keys.moe/2021/12/26/Practical%20Accuracy%20Estimation%20for%20Efficient%20Deep%20Neural/"/>
    <id>https://values.keys.moe/2021/12/26/Practical%20Accuracy%20Estimation%20for%20Efficient%20Deep%20Neural/</id>
    <published>2021-12-25T23:04:01.000Z</published>
    <updated>2023-10-22T09:28:57.956Z</updated>
    
    <content type="html"><![CDATA[<img src="/2021/12/26/Practical%20Accuracy%20Estimation%20for%20Efficient%20Deep%20Neural/pic1.jpg" alt><h1><span id="高效深度神经网络测试的实用准确度估计">高效深度神经网络测试的实用准确度估计</span></h1><h2><span id="摘要">摘要</span></h2><p>​深度神经网络 (Deep neural network, DNN) 愈发流行，DNN的测试对DNN的正确性（即DNN在指定工作中的准确度）至关重要。然而，DNN 测试存在严重的效率问题，即，标注每个测试输入以了解DNN在测试集上的准确度的成本很高，因为需要大量的人（甚至他们还需要特定领域知识）来对测试数据进行手工标注，而测试集本身又是大规模的。为缓解此问题，本文提出了一种叫PACE新颖手法（Practical ACcuracy Estimation的缩写），其选择一小部分测试输入，可以精确地估计整个测试集的准确性。这样一来，只需对这小部分选定样本进行标注即可，这很大程度上降低了标注成本。除了实现精确的准确性估计外，为使PACE更加实用，其还被要求应该是可解释的、确定的，并尽可能地高效的。因此，PACE首先结合聚类，将具有不同测试能力（即测试DNN模型不同功能）的测试输入划分为不同的组。然后，PACE利用MMD-critic算法（一种最先进的基于样例的解释算法，根据组的大小，从每组中选择原型，即选择最具代表性的测试输入）来可以减少聚类带来的噪音影响。同时，PACE还使用了自适应随机测试的思想，从少数空间（即没有聚成任何一组的测试输入）中选择测试输入，以保证在所需的测试输入数量下实现极大的多样性。两个平行的选择过程（即同时从组和少数空间中进行选择）组建了最终的一小部分被选中的测试输入集合。结果表明，PACE能够精确估计整个测试集的准确性，平均偏差只有1.181%∼2.302%，大大超过了最先进的方法。</p><span id="more"></span><h2><span id="简介">简介</span></h2><p>​本文共做出以下主要贡献：</p><p>​<strong>方法</strong>。我们提出了第一个相对可解释、确定和有效的叫做PACE的方法，通过选择DNN测试输入的子集来估计整个测试集的准确性。</p><p>​<strong>实现</strong>。我们基于最先进的框架和库实现了所提出的方法，包括Keras 2.2.4和TensorFlow 1.14.0，以及hdbscan 0.8.22、sklearn与MMD-critic算法的作者分别提供的HDBSCAN、FastICA和MMD-critic算法。</p><p>​<strong>研究</strong>。我们基于测试和测试集下的24对DNN模型进行了广泛的研究，这些测试考虑了模型和测试输入的多样性，证明了PACE的有效性。</p><p>​<strong>制品</strong>。我们已经发布了一个广泛的数据集，供未来使用和研究，其中包括了我们的实现以及实验数据。</p><h2><span id="方法">方法</span></h2><p>​为了提高 DNN 测试的效率，我们的目标是选择测试输入的子集来代表整个测试集，即我们希望通过只标记一小部分选定的测试输入来精确估计整个集合的准确性。这可以大大降低标注成本。然而，如何有效地选择这样一小组测试输入是具有挑战性的。此外，选择的方法要尽可能具有可解释性、高效性和确定性，以使其适用于实践。这也是解决这个问题的关键挑战。</p><p>​在本文中，我们提出了一种新颖实用的方法，称为 PACE，用于从测试输入集中选择更小的子集。一般来说，对于整个测试集，一些测试输入具有相似的测试能力（即测试 DNN 的相似功能），而一些测试输入具有不同的测试能力。直观地说，所选的一小部分测试输入应该涵盖各种测试能力，并保证它们对应的测试能力的原始分布，以使得这些样本能尽可能地代表整个集合，并具有高可解释性。为了实现这个目标，PACE根据识别的特征将所有的测试输入聚类成组，从中反映它们的测试能力，其中不同的组更有可能具有不同的测试能力。由于组的大小在一定程度上反映了不同测试能力的分布，PACE根据不同组大小的比例选择所需的测试输入数量，以保持测试能力的相似分布。</p><p>​一个自然的后续问题是应该从每组中选择哪些测试输入。由于很难通过聚类来完美区分具有不同测试能力的测试输入，我们应该选择最能反映该组测试能力的测试输入，来绕过每组中噪音的影响。为了进一步增强所选测试输入的可解释性，PACE利用最先进的基于样本的解释算法MMD-critic算法来从每组中选择原型（即最具代表性的测试输入）。此外，也可能有一些测试输入不属于任何一组。也就是说，它们中的每一个都可能有独特的测试能力。我们称这些测试输入的空间为少数空间。从少数空间中选择测试输入是合理的，因为它们能更进一步接近整个测试集。为了使用所需数量的测试输入更充分地探索少数空间，PACE 借用了自适应随机测试的思想来选择测试输入。</p><p>​图1展示了我们的方法PACE的概况。</p><img src="/2021/12/26/Practical%20Accuracy%20Estimation%20for%20Efficient%20Deep%20Neural/pic2.png" alt><p>图1 PACE概况</p><h2><span id="特征研究">特征研究</span></h2><p>​我们首先确定测试输入的特征，来区分它们的测试能力。根据传统软件测试，有两类特征可以在一定程度上反映测试能力，即覆盖率特征和输入特征。对于DNN测试，前者指的是在执行测试输入时，哪些DNN元素被覆盖，而后者不依赖于覆盖信息，而是依赖于测试输入信息。本工作中，我们使用输入特征来帮助区分测试能力。主要原因有以下三个：（1）现有的工作已经证明，对于一个给定的DNN来说，许多测试输入可能具有非常相似的神经元覆盖率，因此覆盖率特征不能很好地判别其测试能力。（2）在选择测试输入的问题上，已有工作对覆盖率特征的有效性进行了评估，结果表明其有效性比输入特征的有效性要差。（3）收集覆盖率特征往往会产生额外的代价。</p><p>​DNN逐渐从输入中学习特征来预测标签。DNN的不同层代表不同类型的输入特征。越接近输入层的层代表更多的基本特征，而更接近输出层的层则代表更多的高阶特征。也就是说，测试输入信息包含基本信息（即测试输入本身和从测试输入提取的基本特征）以及高级信息（从测试输入中提取的高阶特征）。更多的高阶特征可以更精确地捕捉到输入和标签之间的关系，但它们更特定于给定的DNN模型，且由于他们需要运行更多层的DNN模型，收集它们的成本更高。相比之下，更基本的特征更通用，其也可更有效地收集，但它们不能反映更复杂的输入特征模式来预测标签。我们的方法并不特定于指定类型的输入特征，本工作中也研究了不同类型的输入特征。本文主要研究以下四种不同类型的输入特征（包括基本特征和高阶特征）为代表：</p><p>​原始特征（ORI）：指的是一个DNN的输入向量，其是最基本的特征，直接代表一个输入本身。</p><p>​第一层特征（FL）：指的是DNN中第一层的输出，最接近DNN的输入向量。</p><p>​最后隐藏层特征（LHL）：指的是DNN中最后一个隐藏层的输出，是可以直接推断输入对应的预测结果的高阶特征。</p><p>​置信特征（CON）：指的是DNN（分类器）的输出，其表示预测结果的置信度。我们还将其归入输入特征中，因为其不依赖覆盖率信息，而是依赖通过DNN模型从测试输入中得到的输出。</p><img src="/2021/12/26/Practical%20Accuracy%20Estimation%20for%20Efficient%20Deep%20Neural/pic3.png" alt><p>图2 特征提取可视化</p><p>​图2使用一个输入样例（CIFAR-10中的汽车图像）说明了DNN中每种类型的特征来源。请注意，DNN的输入和DNN中某些层的输出可能是多维矩阵，因此我们将矩阵降维成向量作为特征向量（如图2中的ORI和FL）。特别的，ORI特征是静态特征，它们不需要运行DNN，而FL、LHL和CON特征都是动态特征。</p><h2><span id="基于聚类的测试能力鉴别">基于聚类的测试能力鉴别</span></h2><p>​基于识别出的测试输入特征（如ORI或FL），每个测试数据皆以一个特征向量来表示。PACE将测试输入聚类到不同组中，以根据特征向量集区分它们的测试能力。聚类前，我们首先对特征进行归一化，以将不同尺度上测量的值调整至同一尺度上。由于特征都是数字类型，PACE采用min-max归一化方法，将这些特征的每个值调整到区间[0,1]中。假设整个测试集表示为T，其大小为st，T中的测试输入集表示为T&#x3D;{t1,t2,…,tst}，ti的特征向量表示为Fi&#x3D;{fi1,fi2,…,fir}，我们用变量xij表示归一化前ti的第j个特征值，用变量x*ij表示归一化后ti的第j个特征值(1≤i≤st且1≤j≤r)。下式展示了归一化的计算过程：</p><img src="/2021/12/26/Practical%20Accuracy%20Estimation%20for%20Efficient%20Deep%20Neural/formula1.png" alt><p>​PACE之后采用HDBSCAN（基于分层密度的噪声应用空间聚类）算法来对测试输入进行聚类，原因如下。(1)几乎不可能事先知道测试能力类型的数量，因此需预先定义集群数量的聚类算法无法使用，如广泛使用的K-means算法。HDBSCAN不需要预先定义聚类的数量，而是基于密度进行聚类。(2)最广泛应用的基于密度的聚类算法之一是DBSCAN，而HDBSCAN是其升级版。特别的，HDBCSAN可以有不同密度的簇，而DBSCAN必须预先定义簇的密度。(3)HDBSCAN已被证明是十分高效的，使我们的方法PACE更加实用。(4)HDBSCAN需要设置的参数很少，并且对参数选择具有鲁棒性。这使得它在实践中更容易使用。</p><img src="/2021/12/26/Practical%20Accuracy%20Estimation%20for%20Efficient%20Deep%20Neural/pic4.png" alt><p>图3 HDBSCAN聚类主要阶段的可视化</p><p>​HDBSCAN是一种基于密度的聚类算法，它将紧密堆积在一起的数据点（即有许多附近邻居的数据点）分组。为了更清楚地说明PACE在HDBSCAN的过程，我们将DNN模型ResNet-20的测试集CIFAR-10中抽取100个测试输入，对HDBSCAN聚类的主要阶段进行可视化（如图3所示）。更具体地说，其首先构建一个加权图，其中数据点是顶点，任意两点间都存在一条边，其权重等于两点之间的相互可达距离。下式显示了基于K近邻的点a与点b之间的相互可达距离（MRD）的计算：</p><img src="/2021/12/26/Practical%20Accuracy%20Estimation%20for%20Efficient%20Deep%20Neural/formula2.png" alt><p>​其中Corek(a)和Corek(b)分别代表a&#x2F;b与其第k个最近邻之间的距离；Dist(a,b)是a和b之间的距离，用广泛使用的欧几里得距离测量。在MRD下，密集点之间的距离保持不变，但稀疏点被移动至与其他点的距离至少为他们的core距离。然后，HDBSCAN通过Prim算法构建加权图的最小生成树，如图3(a)所示。在最小生成树的基础上，通过对树边按距离升序排列，并通过为每条边创建一个新的合并簇进行迭代，将一个完全联通的图转化为一个连通分量的层次结构，如图3(b)所示。接下来，其根据最小集群大小（HDBCSAN的一个参数）将大集群层次结构压缩为较小的树，如图3(c)所示。最后，它通过计算每个聚类的稳定性分数，从树中提取稳定的簇。基于HDBCSAN，PACE将测试输入分为不同的组，不同的组更可能具有不同的测试能力。此外，一些输入没有被聚类到任何组中，构成了少数空间。</p><p>​请注意，当特征维度增加时，HDBCSAN的性能可能会大大降低。因此，PACE在聚类前结合了对高维特征进行降维的过程。更具体地说，PACE使用FastICA算法来进行降维。FastICA旨在通过最大化公式(3)中所定义的负熵来找到独立成分，并有助于找到潜在因素。在这个公式中，YGauss是一个方差与随机变量Y相同的高斯随机变量，E[.]是计算均值，而g(.)是一个非线性函数，用以近似微分熵：</p><img src="/2021/12/26/Practical%20Accuracy%20Estimation%20for%20Efficient%20Deep%20Neural/formula3.png" alt><h2><span id="基于mmd-critic的原型选择">基于MMD-critic的原型选择</span></h2><p>​在将具有不同测试能力的测试输入分为不同组后，PACE再从每个组中选择测试输入，构成测试输入的小集合。由于很难通过聚类来完美区分不同的测试能力，每组都可能存在噪声，即不应将这个测试输入划分到该组的情况。为了绕过每组噪声的影响，可选择最能代表该组测试能力的测试输入，它们也被称为原型。为增强每组所选测试输入的可解释性，PACE利用MMD-critic算法，一种最先进的基于样本的解释算法（用于机器学习模型的可解释性，包括分类和聚类），来从每组中选择原型。</p><p>​更具体地说，它通过计算原型分布（表示为P）和组分布（表示为G）之间的差异来选择原型。所选的原型应具有最小差异。MMD是P和G之间差异的度量，由两个分布期望间差异的函数空间F上的上限值给出，如下式所示。此外，PACE还基于MMD-critic算法选择criticism（criticism指不具代表性的样本），即与该组原型差异最大的测试输入。criticism与原型能够帮助开发人员建立更好的模型来理解组，从而可以提高选择的可解释性：</p><img src="/2021/12/26/Practical%20Accuracy%20Estimation%20for%20Efficient%20Deep%20Neural/formula4.png" alt><p>​通过这种方式，PACE从每组中选择所需数量的原型来构成小集合，同时也选择相应的criticism来帮助理解簇中复杂的测试输入空间。</p><h2><span id="少数空间的自适应随机探索">少数空间的自适应随机探索</span></h2><p>​少数空间的测试输入也是整个测试集的组成部分，因此，所选择的小集合也应该包含这些测试输入的一部分，以更好地代表整个测试集。而选择这些测试输入的一部分来代表整个少数空间是具有挑战性的，因为这些测试输入可能彼此具有不同的测试能力。为使用所需数量的测试输入来尽可能充分探索少数空间，PACE借用了自适应随机测试的思想。这样，PACE就可以在所需的测试输入数量下实现极大的测试能力多样性，从而使整个少数空间尽可能由所需的测试输入数量来表示。更具体地说，它计算未选择的测试输入与每个已选择的测试输入之间的距离，并使用最小距离作为未选择的测试输入与已选择测试输入的距离。然后，它选择与已选择的测试输入具有最大距离的测试输入作为下一个样本。在现有研究中，这种自适应随机策略以及被证明比其他自适应随机策略更有效。根据聚类中使用的距离，PACE还使用欧几里得距离来计算测试输入之间的距离。下式给出了每次选择时的计算结果，其中U和S分别代表未选择的测试输入集和已选择的测试输入集，EucDist(,)旨在计算两个测试输入间的欧几里得距离。</p><img src="/2021/12/26/Practical%20Accuracy%20Estimation%20for%20Efficient%20Deep%20Neural/formula5.png" alt><p>​传统的自适应随机测试随机选择第一个样本，但PACE确定性地选择第一个测试输入，使其更具实用性和可解释性。更具体地说，PACE选择具有最独特测试能力的测试输入作为第一个样本，其与所有组的距离最大。特别的，该测试输入是通过基于HDBSCAN聚类算法提供的outlier_scores（测量与所有组的距离）值来对少数空间中的测试输入进行排名后，处于Top-1位置所对应的输入。</p><h2><span id="pace的使用">PACE的使用</span></h2><p>​本小节中，我们介绍PACE用法，算法1是PACE的高级伪代码。PACE的输入包括一个表示为D的被测试的DNN，一个表示为T的完整测试集，其大小表示为st，以及表示所选测试输入小集合大小，表示为n。为了构建可以精确估计整个测试输入的准确度的小集合，PACE首先通过提取特征（如ORI或FL）将每个测试输入转换为特征向量（如算法1的2-4行所示）。然后，PACE对向量集进行包括归一化或降维的预处理，并通过聚类将其分为m组和少数空间（算法1中的第5行和第6行）。再次，我们表示第i个组的大小为si，少数空间的大小为sm，其中：</p><img src="/2021/12/26/Practical%20Accuracy%20Estimation%20for%20Efficient%20Deep%20Neural/formula5.1.png" alt><p>​选定的小集合测试输入应包含来自每个组的测试输入和来自少数空间的测试输入。在这里，我们定义了一个阈值α来确定他们的比例分布。也就是说，从组中选择的测试输入数量是α·n，而从少数空间中选择测试输入的数量是(1-α)·n（小于sm）。此外，PACE根据不同组大小的比例从每个组中选择测试输入，以保持测试能力的相似分布。即从第k组中选择的测试输入的数量是：</p><img src="/2021/12/26/Practical%20Accuracy%20Estimation%20for%20Efficient%20Deep%20Neural/formula5.2.png" alt><p>​在确定每个组或少数空间中选择的测试输入数量后，PACE根据基于MMD-critic的原型选择方法（算法1中的第7-11行）或自适应随机选择方法（第12-14行）来选择构建一小组测试输入，即PACE的输出（算法1中的第15行）。开发人员只需标注这一小组测试输入即可在实践中估计整个测试集的准确性。</p><img src="/2021/12/26/Practical%20Accuracy%20Estimation%20for%20Efficient%20Deep%20Neural/algo1.png" alt><h2><span id="结论">结论</span></h2><p>​为提高深度神经网络（DNN）的测试效率，我们的目标是选择一小组可以精确估计整个测试集准确性的测试输入。我们只需标注这个小集合，而不必标注整个测试集。为了实现这一目标，我们提出了PACE，一种新颖而实用的方法来选择小集合测试输入集。为了使PACE更加实用，PACE结合了聚类，将不同测试能力的测试输入聚为不同组，然后利用MMD-critic算法从每个组中选择原型，并借助自适应随机测试思想，通过考虑样本下从少数空间（即没有聚类到任何组的测试输入）选择测试输入，以构成最后的小测试输入集合。结果表明，PACE 实现了很好的准确度估计效果。</p>]]></content>
    
    
    <summary type="html">&lt;img src=&quot;/2021/12/26/Practical%20Accuracy%20Estimation%20for%20Efficient%20Deep%20Neural/pic1.jpg&quot; alt&gt;

&lt;h1 id=&quot;高效深度神经网络测试的实用准确度估计&quot;&gt;&lt;a href=&quot;#高效深度神经网络测试的实用准确度估计&quot; class=&quot;headerlink&quot; title=&quot;高效深度神经网络测试的实用准确度估计&quot;&gt;&lt;/a&gt;高效深度神经网络测试的实用准确度估计&lt;/h1&gt;&lt;h2 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要&quot;&gt;&lt;/a&gt;摘要&lt;/h2&gt;&lt;p&gt;​	深度神经网络 (Deep neural network, DNN) 愈发流行，DNN的测试对DNN的正确性（即DNN在指定工作中的准确度）至关重要。然而，DNN 测试存在严重的效率问题，即，标注每个测试输入以了解DNN在测试集上的准确度的成本很高，因为需要大量的人（甚至他们还需要特定领域知识）来对测试数据进行手工标注，而测试集本身又是大规模的。为缓解此问题，本文提出了一种叫PACE新颖手法（Practical ACcuracy Estimation的缩写），其选择一小部分测试输入，可以精确地估计整个测试集的准确性。这样一来，只需对这小部分选定样本进行标注即可，这很大程度上降低了标注成本。除了实现精确的准确性估计外，为使PACE更加实用，其还被要求应该是可解释的、确定的，并尽可能地高效的。因此，PACE首先结合聚类，将具有不同测试能力（即测试DNN模型不同功能）的测试输入划分为不同的组。然后，PACE利用MMD-critic算法（一种最先进的基于样例的解释算法，根据组的大小，从每组中选择原型，即选择最具代表性的测试输入）来可以减少聚类带来的噪音影响。同时，PACE还使用了自适应随机测试的思想，从少数空间（即没有聚成任何一组的测试输入）中选择测试输入，以保证在所需的测试输入数量下实现极大的多样性。两个平行的选择过程（即同时从组和少数空间中进行选择）组建了最终的一小部分被选中的测试输入集合。结果表明，PACE能够精确估计整个测试集的准确性，平均偏差只有1.181%∼2.302%，大大超过了最先进的方法。&lt;/p&gt;</summary>
    
    
    
    <category term="论文阅读" scheme="https://values.keys.moe/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="论文阅读" scheme="https://values.keys.moe/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
  </entry>
  
  <entry>
    <title>Delving into Data: Effectively Substitute Training for Black-box Attack</title>
    <link href="https://values.keys.moe/2021/12/07/Delving%20into%20Data%20Effectively%20Substitute%20Training%20for%20Black-box%20Attack/"/>
    <id>https://values.keys.moe/2021/12/07/Delving%20into%20Data%20Effectively%20Substitute%20Training%20for%20Black-box%20Attack/</id>
    <published>2021-12-07T10:06:33.000Z</published>
    <updated>2022-09-28T07:38:38.409Z</updated>
    
    <content type="html"><![CDATA[<img src="/2021/12/07/Delving%20into%20Data%20Effectively%20Substitute%20Training%20for%20Black-box%20Attack/封面.png" alt><h1><span id="深入研究数据用于黑盒攻击的有效替代训练">深入研究数据：用于黑盒攻击的有效替代训练</span></h1><h2><span id="摘要">摘要</span></h2><p>​深度模型在处理对抗样本时显示了它们的脆弱性。对于黑盒攻击，在无法访问被攻击模型的架构和权重的情况下，大家广泛聚焦于训练对抗攻击的替代模型的方法。以往的替代训练方法主要是基于真实训练数据或合成数据来窃取目标模型的知识，而没有探索什么样的数据可以进一步提高替代模型和目标模型之间的可转移性。本文中，我们提出了一种新视角的替代训练，聚焦于设计知识窃取过程中使用的数据分布。更具体地说，我们提出了一个多样化数据生成模块来合成具有广泛分布的大规模数据。我们还引入了对抗替代训练策略，以关注分布在决策边界附近的数据。这两个模块的结合可以进一步提高替代模型和目标模型的一致性，从而大大提高了对抗攻击的有效性。大量的实验证明了我们的方法在非定向和定向攻击设置下对最先进的竞争对手的有效性。我们还提供了详细的可视化和分析，以帮助理解我们方法的优势。</p><span id="more"></span><h2><span id="1-介绍">1. 介绍</span></h2><p>​尽管在大多数计算机视觉任务中取得了优异的性能，但深度神经网络（DNN）已被证明容易受到难以察觉的对抗噪声或扰动的影响。对抗样本的存在揭示了将DNN部署到现实世界应用中的重要安全风险。当前社区以白盒与黑盒的设置之分来研究对抗攻击（它们的差异在于是否能完全访问目标攻击模型）。事实上，由于白盒攻击所需的完整目标模型信息在现实世界的部署中是不可用的，本文聚焦于黑盒攻击，其通常只基于目标模型的标签或输出分数来生成对抗样本。通常，黑盒攻击包括基于分数的方法或基于决策的方法。然而，在这些攻击中，需要对目标模型进行雪崩式查询，这仍可能限制它们在现实情况下攻击DNN的可用性。<br>​最近，替代训练的想法在黑盒攻击中得到了广泛的探索。通常情况下，该方法并不是直接生成对抗样本，而是训练一个替代模型，在相同的输入数据的查询下，做出与目标模型类似的预测。在一定数量的查询下，这种方法通常能够根据目标模型学习到替代模型。因此可以对替代模型进行攻击，然后可以转移到目标模型上。<br>​从根本上说，替代模型试图通过给出输入数据和相应的查询标签来从目标模型中获取知识。而关键问题是，输入数据是否来自目标模型的训练数据？假设“是”的话，它确实简化了替代训练。然而，在许多现实世界的视觉任务中，收集真实的输入数据甚至是非平凡的。例如，人物照片和视频的数据受到非常严格的控制，个人数据的隐私在很多国家都受到法律的保护。此外，真实图像是最有效的替代训练数据吗？目标模型的训练数据确实有助于在原始任务上得到一个表现良好的替代模型，但其不能保证攻击从替代模型到目标模型的可转移性。为了提高替代训练中的攻击性能，有必要使替代模型和目标模型之间的决策边界距离最小化，这不仅需要大规模和多样化的训练数据，而且特别需要分布在决策边界附近的数据。</p><p>​为了解决真实数据的局限性并探索替代训练数据的更好分布，我们提出了一种新颖的任务驱动统一框架，该框架仅使用专门设计的生成数据进行替代训练，并实现了高攻击性能。如图所示，与使用目标模型的训练数据进行替代训练相比，多样化的合成数据与对抗样本相结合，将促进替代模型进一步接近目标。更具体地说，在我们的框架中，我们首先提出了一个新颖的多样化数据生成模块（Data Generation module, DDG），该模块将噪声采样与标签嵌入信息相结合来生成多样化的训练数据。这样的分布式生成数据基本可以保证替代模型从目标中学习知识。此外，为进一步促使替代模型具有与目标相似的决策边界，我们提出了对抗替代训练策略（Adversarial Substitute Training strategy, AST），其将对抗样本作为边界数据引入训练过程。总的来说，DDG和AST的联合学习保证了替代模型和目标模型之间的一致性，这大大提高了在没有任何真实数据的情况下进行黑盒攻击的替代训练的成功率。</p><img src="/2021/12/07/Delving%20into%20Data%20Effectively%20Substitute%20Training%20for%20Black-box%20Attack/图1.png" alt><p>图1 将真实数据和合成数据应用于替代训练之间的差异。T&#x2F;S表示目标&#x2F;替代模型，(b)中的蓝色+&#x2F;-表示对抗样本，绿色&#x2F;红色虚线表示决策边界。比较(a)和(b)可以得到，我们的方法生成的合成数据可以训练一个与目标模型具有更相似决策边界的替代模型。</p><p>​本工作的主要贡献总结为：（1）我们首次提出了一种新的基于有效生成的替代训练范式，其通过深入研究输入生成的替代训练数据的本质，来提高无数据黑盒攻击的性能。（2）为了实现这一目标，我们首先提出了一个具有多种不同约束的多样化数据生成模块，以扩大合成数据的分布。然后通过对抗替代训练策略进一步提高替代模型和目标模型之间决策边界的一致性。（3）对四个数据集和一个在线机器学习平台的综合实验和可视化证明了我们的方法相对最先进攻击的有效性。</p><h2><span id="2-方法">2. 方法</span></h2><h3><span id="21-框架概述">2.1. 框架概述</span></h3><p>​我们工作的目标是为黑盒对抗攻击有效地训练替代模型，提出的框架如图所示。它由两个模块组成。多样化数据生成模块（DDG）产生多样化的数据，对抗替代训练策略（AST）进一步模仿目标模型的“行为”。在(a)中，DDG根据随机噪声z(i）和标签索引i的标签嵌入向量e(i)生成数据<img src="/2021/12/07/Delving%20into%20Data%20Effectively%20Substitute%20Training%20for%20Black-box%20Attack/公式1.png" alt>。为了保证合成数据的多样性，生成器G通过三个约束进行训练，即自适应标签归一化生成器、噪声&#x2F;标签重建和类间多样性，这将在后面详细说明。此外，为了确保替代模型S接近目标模型T的决策边界，我们将合成的数据和AST的对抗样本一起输入S以进行(b)中的替代训练。从本质上讲，我们把目标模型T作为分类为M类的黑盒，其中只有标签&#x2F;概率输出可用。师生策略在这里被用于从T学习到S。最后，攻击可以在替代模型上进行，然后转移到目标模型上。</p><img src="/2021/12/07/Delving%20into%20Data%20Effectively%20Substitute%20Training%20for%20Black-box%20Attack/图2.png" alt><p>​图2 整体框架图示。其由多样化数据生成模块（DDG）和对抗替代训练模块（AST）组成。（a）DDG旨在生成具有给定标签的不同数据以训练替代模型。（b）AST利用从当前替代模型生成的对抗样本来推动替代模型模仿目标的边界。</p><h3><span id="22-多样化数据生成">2.2. 多样化数据生成</span></h3><p>​为了合成更好的数据用于替代训练，我们首先提出了一个新颖的多样性数据生成模块（DDG），该模块有三个约束条件来操纵生成的合成图像的多样性。这些约束条件原则上鼓励生成器G为每个不同的类学习相对独立的数据分布，并保持类间差异，从而促进替代模型学习目标模型的知识。</p><p>​<strong>自适应标签归一化生成器</strong>。为了更好地从目标模型中学习，我们需要所有类别的平均分布数据进行替代训练，因此有必要生成标签控制的数据。为了实现这一点，我们充分利用了给定的标签和随机噪声。首先，通过从标准高斯分布和标签i中采样的随机噪声向量<img src="/2021/12/07/Delving%20into%20Data%20Effectively%20Substitute%20Training%20for%20Black-box%20Attack/公式2.png" alt>的输入，我们计算出基于嵌入层的标签嵌入向量<img src="/2021/12/07/Delving%20into%20Data%20Effectively%20Substitute%20Training%20for%20Black-box%20Attack/公式3.png" alt>。这种标签嵌入过程可以将单个离散标签编码为一个连续的可学习向量，它在特征空间中的分布更广，包含更多的表示信息。与GAN不同，我们没有使用真实的图像进行监督，这样的标签嵌入过程对数据的生成至关重要。接下来，我们通过两个全连接层从N维标签嵌入向量e(i)中提取平均值μ(i)和方差σ(i)。之后，将μ(i)和σ(i)加入到所有的反卷积块中，迭代合成具有特定类别条件的图像数据，可以表示为：</p><img src="/2021/12/07/Delving%20into%20Data%20Effectively%20Substitute%20Training%20for%20Black-box%20Attack/公式4.png" alt><p>其中总共有五个反卷积块，t表示反卷积块的数量。在获得最终的ˆx(i)之后，输出生成的数据已经被标签归一化的信息进行了修改。这种自适应标签归一化生成器可以更好地利用输入噪声和标签嵌入向量之间的关系来合成标签控制的数据。</p><p><strong>噪声&#x2F;标签重建</strong>。为了进一步确保生成数据ˆx (i)的多样性，我们引入了一个重建网络R来重建输入噪声和标签嵌入<img src="/2021/12/07/Delving%20into%20Data%20Effectively%20Substitute%20Training%20for%20Black-box%20Attack/公式5.png" alt>。而相应的重建损失可以计算为</p><img src="/2021/12/07/Delving%20into%20Data%20Effectively%20Substitute%20Training%20for%20Black-box%20Attack/公式6.png" alt><p>其中我们使用L1来表示输入z(i)和重建的z(i)r之间的差异。对于标签重建，我们应用函数f(∗)来计算e(i)r和e之间的余弦距离，并由Softmax进一步处理以计算与真实标签i的交叉熵损失。在这种约束下，G可以为每个类别的不同输入噪声向量生成更多样的图像。</p><p><strong>类间多样性</strong>。为了进一步增强不同类别的数据多样性，我们使用余弦相似矩阵来最大化所有合成图像的类间距离。特别地，生成器生成一个MB≪M的不同类别的输入合成数据batch，模型S给出这个batch的输出相似度矩阵<img src="/2021/12/07/Delving%20into%20Data%20Effectively%20Substitute%20Training%20for%20Black-box%20Attack/公式7.png" alt>。请注意，我们有真实的相似性矩阵<img src="/2021/12/07/Delving%20into%20Data%20Effectively%20Substitute%20Training%20for%20Black-box%20Attack/公式8.png" alt>，除了对角线元素被设置为1，其他元素都是0。因此，多样性损失函数Ldiv可以表述为：</p><img src="/2021/12/07/Delving%20into%20Data%20Effectively%20Substitute%20Training%20for%20Black-box%20Attack/公式9.png" alt><p>其中，TRI(∗)被定义为提取相似度矩阵中除对角线元素外的上三角元素的操作。通过这种方式，Ldiv确保合成数据拥有每个类别的独立分布。</p><h3><span id="23-对抗替代训练">2.3. 对抗替代训练</span></h3><p>​在DDG生成多样化的训练数据后，为了获得更好的攻击性能，我们还需要进一步鼓励与目标决策边界更相似的替代模型。众所周知，对抗样本拥有视觉上难以区分的扰动，其会被模型错误的分类。由于扰动相对较小，对抗样本可以看作是决策边界周围的样本。因此，我们提出了一种新颖的对抗替代训练策略（AST），它利用对抗样本进一步推动S的决策边界与T的决策边界相似。更具体地说，对于训练期间的每次迭代，我们的生成器首先通过DDG合成图像。然后我们选择白盒攻击算法来获得基于当前S的合成图像的对抗扰动ε。生成对抗图像的目标函数定义为</p><img src="/2021/12/07/Delving%20into%20Data%20Effectively%20Substitute%20Training%20for%20Black-box%20Attack/公式10.png" alt><p>其中L(·) 表示攻击目标，反映了预测xˆ(i)+ε的标签为iadv的概率或交叉熵。如果考虑非定向攻击，则iadv≠i，否则iadv&#x3D;t，t为目标标签。λ是正则化系数，约束ε∈[0, 1]d将扰动ε限制在有效图像空间内。然后，生成的图像和相应的对抗数据被用来一起更新S。</p><h3><span id="24-损失函数">2.4. 损失函数</span></h3><p>​最后，我们应用基本损失函数来训练替代模型</p><img src="/2021/12/07/Delving%20into%20Data%20Effectively%20Substitute%20Training%20for%20Black-box%20Attack/公式11.png" alt><p>其中Ld测量T和S输出之间的距离，Lc表示生成损失。e-Ld表示Ld的“最小-最大”博弈，CE(·)表示S的预测和输入的真实标签i之间的交叉熵损失。因此，凭借这两个损失函数的交替最小化，替代模型S可以学习模仿目标模型T的输出。在DDG和AST的进一步推动下，通过生成的数据和对抗样本，用于训练S和G的统一替代训练损失LS和生成器损失LG被定义为</p><img src="/2021/12/07/Delving%20into%20Data%20Effectively%20Substitute%20Training%20for%20Black-box%20Attack/公式12.png" alt><p>其中Ladvd的定义与等式中的Ld相同。之前的公式使用对抗样本作为输入测量T和S的输出间的距离，Ladvc被定义为Lc，以约束以对抗样本作为输入的生成，Lrec和Ldiv被用来增强数据多样性。β1、β2和β3是DDG的平衡超参数。总的来说，训练过程如下。</p><img src="/2021/12/07/Delving%20into%20Data%20Effectively%20Substitute%20Training%20for%20Black-box%20Attack/算法1.png" alt><h2><span id="3-实验">3. 实验</span></h2><h3><span id="31-实验设置">3.1. 实验设置</span></h3><p>​<strong>数据集和目标模型</strong>。1) MNIST：攻击模型在AlexNet、VGG-16和ResNet-18上进行预训练。默认的替代模型是具有3个卷积层的网络。2)CIFAR-10：攻击者在AlexNet、VGG-16和 ResNet-18上进行了预训练。默认替代模型是VGG-13。3) CIFAR-100：被攻击者在VGG-19和ResNet50上进行了预训练。默认的替代模型是ResNet-18。4)Tiny Imagenet：被攻击者在ResNet-50上进行了预训练。替代模型是ResNet-34。<br>​<strong>比较对象</strong>。为了验证所提出方法的有效性，我们将我们的攻击结果与无数据黑盒攻击（如DaST）和几种需要真实数据的黑盒攻击（如PBBA和Knockoff）。我们还使用被攻击模型的原始训练数据进行替代训练，并利用ImageNet学习替代模型。<br>​<strong>实现细节</strong>。我们使用Pytorch进行实现。我们利用Adam从头开始训练我们的替代模型、生成器和重建网络，所有权重都使用标准差为0.02的截断正态分布随机初始化。所有网络的初始学习率均设置为0.0001，从第80个epoch开始逐渐降低到0，并在第150个epoch后停止。我们将mini-batch大小设置为500，超参数β1、β2和β3值为1。我们的模型由一个 NVIDIA GeForce GTX 1080Ti GPU训练。我们应用PGD作为在AST和评估期间生成对抗图像的默认方法。我们还使用FGSM、BIM和C&amp;W进行广泛实验的攻击。<br>​<strong>评价指标</strong>。 考虑到 DaST中提出的存在两种不同的场景，即只从目标模型中获取输出标签并很好地访问输出概率，我们将这两种场景命名为基于概率和基于标签。在实验中，我们报告了由替代模型产生的对抗样本攻击目标黑盒模型的攻击成功率（ASRs）。按照DaST的设置，在非目标攻击设置中，我们只在被攻击模型正确分类的图像上生成对抗样本。对于目标攻击，我们只在没有被分类到特定错误标签的图像上生成对抗样本。为了公平比较，在所有的对抗样本过程中，设定扰动ε&#x3D;8。我们对每个测试进行五次，并报告平均结果。</p><h3><span id="32-黑盒攻击结果">3.2. 黑盒攻击结果</span></h3><p>​我们与比较对象手在四个数据集和一个在线机器学习平台上评估我们的方法，包括了定向攻击和非定向攻击设置。如表1、2、3所示，我们在基于概率和基于标签的场景下对每个数据集的多个目标模型进行了广泛的比较。</p><img src="/2021/12/07/Delving%20into%20Data%20Effectively%20Substitute%20Training%20for%20Black-box%20Attack/表1.png" alt>表1 使用概率作为我们的方法和比较对象在多个数据集上的目标模型输出来比较ASRs结果<img src="/2021/12/07/Delving%20into%20Data%20Effectively%20Substitute%20Training%20for%20Black-box%20Attack/表2.png" alt>表2 使用标签作为我们的方法和比较对象在多个数据集上的目标模型输出来比较ASRs结果<img src="/2021/12/07/Delving%20into%20Data%20Effectively%20Substitute%20Training%20for%20Black-box%20Attack/表3.png" alt>表3 比较攻击Microsoft Azure示例模型下我们的方法和比较对象的ASRs结果<p>​<strong>与真实数据进行替代训练进行比较</strong>。这里我们研究了用真实图像进行攻击的替代训练，如表1和表2所示。我们直接使用目标模型或ImageNet的原始训练数据进行替代训练，而不是合成训练。结果显示，真实图像可以让替代模型从目标中学习到部分东西，在分类上可能会有更高的准确率，但与生成的数据相比，攻击强度较弱。我们认为，这个问题是由真实图像的数量和多样性限制造成的，这可能导致替代模型学习和模仿目标模型的失败。因此，我们提出了一种DDG策略来合成大规模和多样化的数据。<br>​<strong>与最新方法的比较</strong>。如表1和表2所示，我们将我们的方法与黑盒攻击进行比较。对于定向攻击和非定向攻击，我们的方法在所有数据集下都取得了比基于概率和基于标签的方案更好的ASRs。此外，与相似的生成法DaST相比，我们的方法很大程度上优于它。这些结果验证了所提出的方法让替代模型更好地接近目标的决策边界和实现无数据黑盒攻击的高ASRs这两个方面的有效性。<br>​<strong>在Microsoft Azure上与比较对象的比较</strong>。为了更好地评估实际应用下的攻击手段能力，我们在Microsoft Azure上进行了在线模型攻击实验。通过以攻击Azure上机器学习教程中的MNIST模型为目标，我们比较了我们的方法和竞争对手之间的结果。表3中显示的结果表明了我们的方法可以在在线模型上获得最好的ASR，这进一步证明了我们的方法在没有攻击先验知识的真实场景下的有效性。</p><h2><span id="4-结论">4. 结论</span></h2><p>​本文重点研究黑盒攻击替代训练的生成数据的分布。它提出了一个统一的替代模型训练框架，包含一个多样化数据生成模块（DDG）和一个对抗替代训练策略（AST）。DDG可以生成标签控制的和多样化的数据来训练替代模型。AST利用对抗样本作为边界数据，使替代模型更好地符合目标的决策边界。大量实验表明该方法可以实现高攻击性能。</p>]]></content>
    
    
    <summary type="html">入门介绍</summary>
    
    
    
    <category term="论文阅读" scheme="https://values.keys.moe/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="论文阅读" scheme="https://values.keys.moe/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
  </entry>
  
  <entry>
    <title>Understanding Recurrent Neural Networks Using Nonequilibrium Response Theory</title>
    <link href="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/"/>
    <id>https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/</id>
    <published>2021-06-24T02:27:01.000Z</published>
    <updated>2023-10-22T11:25:07.388Z</updated>
    
    <content type="html"><![CDATA[<img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image002.jpg" alt><h1><span id="使用非平衡响应理论理解循环神经网络">使用非平衡响应理论理解循环神经网络</span></h1><h1><span id="引用">引用</span></h1><p>Lim S H . Understanding Recurrent Neural Networks Using Nonequilibrium Response Theory[J]. 2020.</p><h2><span id="摘要">摘要</span></h2><p>循环神经网络（RNN）是一种受大脑启发的模型，其广泛的应用于机器学习，以进行连续数据的分析。本工作有助于使用非平衡学说的响应理论更深度地理解RNN如何处理输入信号。对于一类由输入信号驱动的连续时间随机RNN（SRNN），我们为其输出推导出一个沃尔泰拉级数的序列表示。这种表示法是可解释的，并将输入信号从SRNN结构中分离出来。序列的核是一些递归定义的相关函数，其与完全决定输出的无扰动动力学相关。利用这种表示的联系及其对粗糙路径理论的影响，我们确定了一个通用特征——响应特征，其被证明是输入信号的张量积的特征与自然支撑基础。特别地，我们展示了仅优化了读出层的权重，而隐藏层的权重保持固定、未被优化的SRNN，这可被看作是在与响应特征相关的再生核希尔伯特空间中执行的核机器。</p><span id="more"></span><h2><span id="介绍">介绍</span></h2><p>从时间序列分析到自然语言处理，序列化数据出现在广泛的场景中。在没有数学模型的情况下，从数据中提取有用信息，以学习一个数据生成系统是很重要的。</p><p>循环神经网络（RNN）是一类受大脑启发的模型，其专门为学习序列数据而设计，被广泛地应用于从物理学到金融的各个领域。RNN是具有反馈连接的神经元网络，从生物学角度比其他适应性模型更具说服力。特别地，RNN可以使用它们的隐藏状态（记忆）来处理输入的可变长度序列。它们是动力系统的通用逼近器，且其本身可被视为一类开放动力系统。</p><p>尽管RNN近期在储备池计算、深度学习和神经生物学方面取得了创新和巨大的经验成功，但很少有研究关注RNN工作机制的理论基础。缺乏严格的分析限制了RNN在解决科学问题方面的实用性，并可能阻碍下一代网络的系统设计。因此，深入了解该机制对于阐明大型自适应架构的特性，以及彻底改变我们对这些系统的理解而言至关重要。</p><p>特别地，人们可能会问的两个自然且基础的问题是：</p><p>Q1：随着时间推移的输入信号如何驱动RNN产生输出？</p><p>Q2：它们的响应是否有一个普遍的机制？</p><p>本工作的主要目标之一是解决上述问题，以非平衡统计动力学中的非线性响应理论为出发点，针对连续时间RNN的随机版本，简称SRNN（其隐藏状态被注入了高斯白噪声）进行分析。我们的方法是跨学科的，为现有的RNN理论增加了令人耳目一新的观点。</p><h2><span id="随机循环神经网络srnn">随机循环神经网络（SRNN）</span></h2><p>本文固定过滤概率空间（filtered probability space）<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image004.jpg" alt>，E代表对P的期望，T&gt;0。C(E, F)代表从E到F的连续映射的巴拿赫空间，其中E和F是巴拿赫空间。<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image005.png" alt>表示Rn上所有有界连续函数的空间。N:&#x3D;{0, 1, 2, . . . }，Z+:&#x3D;{1, 2, . . . }且R+:&#x3D; [0, ∞)。上标T表示转置，∗表示邻接。</p><h3><span id="模型">模型</span></h3><p>我们对我们的SRNN考虑如下模型。所谓激活函数，是指一个非常数的、利普希茨连续且有界的实值函数。激活函数的例子包括sigmoid函数，如实践中常使用的双曲切线等。</p><p><strong>定义2.1</strong>（连续时间SRNN）令t ∈ [0, T]，<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image007.jpg" alt>为确定的输入信号。连续时间的SRNN描述为以下空间状态的模型：</p><img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image009.jpg" alt><p>其中，公式1是隐藏状态<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image010.png" alt>的随机微分方程（SDE），带有漂移系数φ：<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image011.png" alt>、噪声系数<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image012.png" alt>和定义在<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image014.jpg" alt>上的r维维纳过程<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image015.png" alt>，而公式2定义了一个可观测的激活函数<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image016.png" alt>。</p><p>我们考虑SRNN的输入仿射版本，其中：</p><img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image018.jpg" alt><p>其中，<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image019.png" alt>是正稳定的，<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image020.png" alt>为激活函数，<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image021.png" alt>和<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image022.png" alt>为常量，<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image023.png" alt>为转换输入信号的常量矩阵。</p><p>从现在开始，我们将 SRNN 称为由（1）-（3）定义的系统。SRNN的隐藏状态描述了一个处理输入信号的非自主随机动力系统。常数Γ、W、b、C、σ和f中的参数（如果有的话）定义了SRNN（架构）的（可学习）参数或权重。对于 T &gt; 0，与 SRNN 相关联的是输出函数 <img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image024.png" alt>，其定义为可观测的f的期望值（集合平均值）：</p><img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image025.png" alt><h3><span id="srnn的非平衡响应理论">SRNN的非平衡响应理论</span></h3><p><strong>预备知识和符号</strong></p><p>在本小节中，我们简要回顾马尔可夫过程的预备知识并介绍我们的一些符号。</p><p>令t ∈ [0, T]，<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image026.png" alt>且<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image027.png" alt>是归一化的输入信号。在SRNN（1）-（3）中，我们认为信号<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image028.png" alt>是驱动SDE的小振幅γ（t）的扰动：</p><img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image030.jpg" alt><p>未扰动的SDE是Cu设置为零的系统：</p><img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image032.jpg" alt><p>其中，<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image034.jpg" alt>且<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image036.jpg" alt>。过程 h 是时间齐次马尔可夫过程<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image037.png" alt>的扰动，它不一定是稳定的。</p><p>扩散过程h和<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image038.png" alt>分别与一族无穷小生成元<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image039.png" alt>和<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image040.png" alt>相关，它们是二阶椭圆算子，定义为：</p><img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image041.png" alt><p>对于任何可观察的<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image042.png" alt>，其中<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image043.png" alt>。我们将与 h 关联的转移算子<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image044.png" alt>定义为：</p><img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image045.png" alt><p>对于<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image042.png" alt>，和转移算子<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image044.png" alt>（其为一个马尔科夫半群），它们都是与<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image046.png" alt>相关联的。</p><p>此外，可以在概率测度空间上定义上述生成元和转移算子的L2伴随矩阵。我们分别用<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image047.png" alt>和<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image048.png" alt>表示与h和<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image049.png" alt>关联的伴随生成器，分别用<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image050.png" alt>和<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image051.png" alt>表示与h和<img class="small-img" src="file:///C:/Users/Angelo/AppData/Local/Temp/msohtmlclip1/01/clip_image049.png" alt>关联的伴随转移算子。我们假设初始测度和过程定律具有关于勒贝格测度的密度。将初始密度表示为<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image052.png" alt>，<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image053.png" alt>满足与<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image054.png" alt>关联的前向柯尔莫果洛夫方程（FKE）。</p><p>我们采取自然的假设，即扰动和未扰动过程都有相同的初始分布<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image055.png" alt>，这通常不是无扰动动力学的不变分布<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image056.png" alt>。</p><p><strong>关键思想和形式推导</strong></p><p>首先，我们将推导出SRNN的输出函数在驱动输入信号方面的表示。我们的方法源于非平衡统计动力学的响应理论。在下文中，我们假设任何无限级数都是明确定义的，且求和和积分之间的任何互换都是合理的。</p><p>固定一个T&gt;0，令<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image057.png" alt>足够小并且</p><img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image058.png" alt><p>首先，请注意概率密度<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image059.png" alt>的FKE是：</p><img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image060.png" alt><p>其中<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image061.png" alt>，而：</p><img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image062.png" alt><img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image063.png" alt><p>关键思想是，由于ε&gt; 0很小，我们寻求形式为ρ的微扰展开：</p><img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image064.png" alt><p>将其代入FKE并匹配ε中的阶数，我们得到以下方程层次：</p><img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image065.png" alt><p>ρn的形式解可以通过迭代获得。形式化的描述，我们记<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image066.png" alt>。在不变分布是稳定的特殊情况下，<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image067.png" alt>与时间无关。</p><p>请注意，n ≥ 2时，<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image068.png" alt>，在n ≥ 2时，解ρn通过递归关系而得：</p><img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image069.png" alt><p>因此，假设下面的无穷级数绝对收敛，我们有：</p><img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image070.png" alt><p>接下来，我们考虑SRNN的隐性动力学的标量值观测值<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image071.png" alt>，并研究输入信号扰动引起的该观测值的平均偏差：</p><img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image073.jpg" alt><p>对于扰动动力学的可观察值的平均值可写为：</p><img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image074.png" alt><p>在不丧失一般性的情况下，我们在下文中取<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image076.jpg" alt>，即f(h)被认为是均值为零的（相对于ρinit）。</p><p>我们有：</p><img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image078.jpg" alt><p>其中</p><img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image080.jpg" alt><p>是一阶响应核，它们是相对于 ρinit 的仅无扰动动力学函数的平均值。请注意，为了获得上面的最后一行，我们分部积分并假设ρinit&gt;0。</p><p>该式表达了阿加瓦尔型的非平衡波动-耗散关系。在平稳不变分布的情况下，我们使用（向量值）响应核，恢复统计力学中众所周知的平衡波动-耗散关系：</p><img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image081.png" alt><p>其中<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image082.png" alt>。在线性 SRNN（即φ(h, t)在h中是线性的）和f(h) &#x3D; h的特殊情况下，其可简化为<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image083.png" alt>的协方差函数（相对于ρ∞）。</p><p>到目前为止，我们已经研究了线性响应机制，其中，响应线性地依赖于输入。现在我们通过将上述推导扩展到n≥2的情况。我们表示<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image084.png" alt>，可得</p><img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image086.jpg" alt><p>其中<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image087.png" alt>，<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image088.png" alt>是n阶响应核：</p><img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image090.jpg" alt><p>且</p><img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image091.png" alt><p>n &#x3D; 2, 3, . . .时，</p><img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image093.jpg" alt><p>请注意，这些高阶响应核与一阶响应核类似，是相对于 ρinit 的一些仅无扰动动力学的函数的平均值。</p><p>基于上述结果可得：</p><img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image095.jpg" alt><p>其中<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image096.png" alt>是递归定义的时间相关核。更重要的是，这些核完全由SRNN的未扰动动力学以明确的方式确定。因此，SRNN 的输出函数可以写成（实际上是唯一的）上述一系列形式。该陈述在后文中得到了精确表述，从而解决了 (Q1)。</p><p>现在我们关注(Q2)。通过展开技术，我们可以得到：</p><img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image098.jpg" alt><p>其中，<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image099.png" alt>是与时间和信号<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image100.png" alt>无关的常数。该表达式以系统的方式将驱动输入信号从 SRNN 架构中分离出来。粗略地说，它告诉我们，SRNN对输入信号的响应可以通过将两部分的乘积相加得到，其中一个描述了SRNN的未扰动部分，一个是经过时间变换的输入信号的迭加积分。这一声明在后续得到了更精确的阐述，它是解决(Q2)的起点。</p><h2><span id="主要结果">主要结果</span></h2><h3><span id="假设">假设</span></h3><p>为了简单和直观，我们对SRNN使用以下相当严格的假设。 这些假设可以通过增加技术成本（我们不在这里追求）或通过计算近似结果来证明是合理的。</p><p>回想一下，我们正在处理确定性输入信号<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image101.png" alt>。</p><p><strong>假设4.1</strong> 固定T&gt;0并让U成为<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image102.png" alt>的开集。</p><p>(a) <img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image103.png" alt>对所有t∈[0, T]来说都是足够小的。</p><p>(b) 在所有<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image104.png" alt>时，<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image105.png" alt>，并且以概率1存在一个紧集K⊂U，使得在所有<img class="small-img" src="file:///C:/Users/Angelo/AppData/Local/Temp/msohtmlclip1/01/clip_image104.png" alt>情况下，<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image106.png" alt>。</p><p>(c) 系数a：<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image107.png" alt>和f：<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image108.png" alt>为分析函数。</p><p>(d) <img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image109.png" alt>是正定的，<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image110.png" alt>是正稳定的（即，Γ 的所有特征值的实部都是正的）。</p><p>(e) 初始状态<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image111.png" alt>是一个根据概率密度ρinit分布的随机变量。</p><p>假设4.1(a)意味着我们使用幅度足够小的输入信号。这对于确保某些无穷级数以足够大的收敛半径绝对收敛非常重要。(b) 和 (c) 确保一些理想的规律性和有界性。特别地，它们意味着a、f和它们所有的偏导数都是有界的，且在整个t∈[0, T]上，ht和<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image112.png" alt>利普希茨连续。(d) 意味着系统受到的是非退化噪声的抑制和驱动，这确保了无扰动系统可以指数稳定。(e)是我们分析的自然假设，因为h是<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image113.png" alt>的一个扰动。</p><p>除非另有说明，否则假设4.1是本文中隐含的假设。</p><p>进一步符号化。我们现在提供一个空间及其符号的列表：</p><p>* L(E1, E2)：从E1到E2的有界线性算子的巴拿赫空间（其中||·||表示适当空间上的范数）</p><p>* <img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image114.png" alt>：具有紧支撑的类<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image115.png" alt>的实值函数空间</p><p>* <img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image116.png" alt>：类<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image117.png" alt>有界实值函数空间</p><p>* <img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image118.png" alt>：<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image119.png" alt>上有界绝对连续度量的空间，其中<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image121.jpg" alt>，ρ表示度量µ的密度</p><p>* <img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image123.jpg" alt>：ρ加权的Lp空间，即函数f的空间，使得<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image124.png" alt>，其中ρ是加权函数。</p><h3><span id="srnn-输出泛函的表示方法">SRNN 输出泛函的表示方法</span></h3><p>在保证不丧失一般性的情况下，我们将在下文取p&#x3D;1并假设<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image125.png" alt>。</p><p><strong>定义4.1</strong> （响应函数） 令<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image126.png" alt>是一个有界的可观察对象。对于t∈[0,T]，令Ft是C([0, t],R)上的泛函，定义为<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image127.png" alt>，<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image129.jpg" alt>表示Ft相对于γ的n阶泛函导数。对于n∈Z+，如果存在局部可积函数<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image130.png" alt>，对于所有测试函数<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image131.png" alt>，使得</p><img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image132.png" alt><p>则<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image133.png" alt>被称为可观测f的n阶响应函数。</p><p>接下来，在t∈[0,T]中，令<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image135.jpg" alt>是任意可观察函数，且<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image137.jpg" alt>。</p><p><strong>命题4.1</strong> （响应函数的显式表达式） 对于n∈Z+，令<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image138.png" alt>为f的n阶响应函数。那么，对于<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image139.png" alt>：</p><p>(a) <img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image141.jpg" alt></p><p>(b) （高阶A-FDT）此外，如果 ρinit 为正，则</p><img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image142.png" alt><p>其中</p><img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image144.jpg" alt><p><strong>推论4.1</strong> 令n∈Z+，且<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image145.png" alt>。假定在<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image146.png" alt>上有另一个函数<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image147.png" alt>，使得对于所有的<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image148.png" alt>，有</p><img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image150.jpg" alt><p>那么<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image151.png" alt>几乎处处成立。</p><p><strong>定理4.1</strong> （记忆表示） 令t∈[0,T]，SRNN的输出泛函<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image152.png" alt>是N→∞的极限：</p><img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image154.jpg" alt><p>其中<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image155.png" alt>在命题4.1中给出。该极限存在，且是唯一的收敛的沃尔泰拉级数。如果Gt是另一个具有响应函数<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image156.png" alt>的这样的级数，那么Ft&#x3D;Gt。</p><p><strong>定理4.2</strong> （无记忆表示） 假设算子<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image157.png" alt>有一个明确定义的本征函数展开。那么，SRNN的输出函数<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image158.png" alt>有一个收敛级数展开，这就是N, M→∞的极限：</p><img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image160.jpg" alt><p>其中<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image161.png" alt>是常数系数，取决于pi、li、<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image163.jpg" alt>的特征值和特征函数、f和ρinit，但与输入信号和时间无关。在这里，pi∈{0, 1, . . . , M}、li∈{1, 2, . . . , m}。</p><p><strong>命题4.2</strong> （确定的深度SRNN的表示） 令Ft和Gt是两个SRNN的输出函数，相关的截断沃尔泰拉级数分别具有响应核<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image164.png" alt>核<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image165.png" alt>，n&#x3D;1,…,N，m&#x3D;1,…,M。那么<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image166.png" alt>是具有N+M个响应核的截断沃尔泰拉级数：</p><img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image168.jpg" alt><p>当且仅当r&#x3D;1,…,N+M，其中</p><img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image170.jpg" alt><p>如果Ft和Gt是沃尔泰拉级数（即N，M&#x3D;∞），则在r &#x3D; 1, 2, . . . 上，<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image171.png" alt>是具有上述响应核<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image172.png" alt>的沃尔泰拉级数（只要它是明确定义的）。</p><p>此外，定理4.2中的陈述适用于<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image173.png" alt>，即<img class="small-img" src="file:///C:/Users/Angelo/AppData/Local/Temp/msohtmlclip1/01/clip_image173.png" alt>在定理4.2的假设下允许指定形式的收敛级数展开。</p><p><strong>定义4.2</strong> （路径特征） 令X∈C([0, T], E)为有界变差路径。X的特征是T((E))的元素S，定义为</p><img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image174.png" alt><p>其中</p><img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image175.png" alt><p>当且仅当n ∈ Z+，<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image176.png" alt>。</p><p>令<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image177.png" alt>为<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image178.png" alt>的典范基，那么我们有：</p><img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image180.jpg" alt><p>用<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image181.png" alt>表示对偶配对，有</p><img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image182.png" alt><p><strong>定理4.3</strong> （特征方面的无记忆表示） 设p是一个正整数，并假设输入信号u是一个有界变差路径。那么SRNN的输出函数Ft是<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image183.png" alt>在p→∞的极限，其是路径特征的线性泛函，<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image184.png" alt>（可通过向量化与<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image185.png" alt>进行识别），其中<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image186.png" alt>，即</p><img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image187.png" alt><p>其中，bn(t)仅取决于t的系数。</p><h3><span id="将srnn表述为核机器">将SRNN表述为核机器</span></h3><p>我们现在考虑一个监督学习（回归或分类）的环境，我们给定N个训练输入输出对<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image188.png" alt>，其中un∈χ，为<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image189.png" alt>中有界变差的路径空间，yn∈R，使得对于所有n，有<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image190.png" alt>，这里FT是一个连续目标映射。</p><p>考虑优化问题：</p><img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image191.png" alt><p>其中G是具有范数<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image192.png" alt>的假设（巴拿赫）空间，<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image194.jpg" alt>为一个损失函数，R(x)是一个在x中严格增加的实值函数。</p><p>受定理4.3的启发（将G视为由SRNN引入的假设空间）我们将表明，该问题的解决方案可以表示为对训练样本的核扩展。</p><p>在下文中，考虑希尔伯特空间：</p><img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image195.png" alt><p>其中P是适当加权的<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image196.png" alt>序列空间，其遵循序列形式为<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image197.png" alt>，其中</p><p>Pn(t)是[0, T]上的正交多项式。令<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image198.png" alt>表示H上的对称福克空间，<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image199.png" alt>表示L∈Z+时<img class="small-img" src="file:///C:/Users/Angelo/AppData/Local/Temp/msohtmlclip1/01/clip_image198.png" alt>的L折张量积。</p><p><strong>命题4.3</strong> 令L∈Z+。考虑映射<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image200.png" alt>，定义为：</p><img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image201.png" alt><p>其中K是H上的核，存在一个唯一的RKHS，表示为具有范数<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image202.png" alt>的<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image203.png" alt>，其中K为再生核。</p><p><strong>定理4.4</strong> （表示定理） 考虑时间增加的路径<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image204.png" alt>，其中un是χ中<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image205.png" alt>值的输入路径，v是P中<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image206.png" alt>值向量。那么：</p><p>(a) 假设空间为<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image207.png" alt>的前文所述优化问题的任何解都允许以下形式的表示：</p><img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image208.png" alt><p>其中cn∈R，N是训练输入-输出对的数量。</p><p>(b) 令L ∈ Z+。如果我们转而考虑路径，表示为<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image209.png" alt>，在时间ti∈[0, T]上，通过对L+1个数据点进行线性插值获得<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image210.png" alt>，则相应优化问题的任何解都具有<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image211.png" alt>的假设空间，表示形式为：</p><img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image212.png" alt><p>其中αn∈R，l&#x3D;1,…,L时，<img class="small-img" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image213.png" alt>。</p><h2><span id="结论">结论</span></h2><p>在本文中，我们使用非平衡统计动力学的非线性响应理论作为起点，解决了关于一类随机循环神经网络 (SRNN) 的两个基本问题，这些网络可以是人工或生物网络的模型。特别地，我们能够以系统的、逐级的方式来描述SRNN对扰动的确定性输入信号的响应，为这些SRNN的输出函数推导出两种类型的序列表示，以及在驱动输入信号方面的深度变体。这提供了对由这些驱动网络所引起的记忆和无记忆表示的性质的探究。此外，通过将这些表示与路径特征的概念联系起来，我们发现响应特征集是 SRNN 在处理输入信号时从中提取信息的构建块，揭示了SRNN运行的普遍机制。特别地，我们通过表示定理表明，SRNN可以被看作是在与响应特征相关的再生核希尔伯特空间上运行的核机器。</p><p>从数学的角度来看，放宽这里的假设，并在驱动输入信号是粗略路径的一般设置中工作会很有趣，输入信号的规律性可能会发挥重要作用。人们还可以通过采用此处开发的技术来研究 SRNN 如何响应输入信号和噪声驱动（正则化）中的扰动。到目前为止，我们一直专注于介绍中提到的“公式化优先”方法。这里获得的结果表明，可以通过设计有效的算法来利用离散化响应特征和相关特征在涉及时间数据的机器学习任务中的使用，来研究”离散化的下一步”，例如在科学与工程中预测由复杂动力系统产生的时间序列。</p>]]></content>
    
    
    <summary type="html">&lt;img src=&quot;/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image002.jpg&quot; alt&gt;

&lt;h1 id=&quot;使用非平衡响应理论理解循环神经网络&quot;&gt;&lt;a href=&quot;#使用非平衡响应理论理解循环神经网络&quot; class=&quot;headerlink&quot; title=&quot;使用非平衡响应理论理解循环神经网络&quot;&gt;&lt;/a&gt;使用非平衡响应理论理解循环神经网络&lt;/h1&gt;&lt;h1 id=&quot;引用&quot;&gt;&lt;a href=&quot;#引用&quot; class=&quot;headerlink&quot; title=&quot;引用&quot;&gt;&lt;/a&gt;引用&lt;/h1&gt;&lt;p&gt;Lim S H . Understanding Recurrent Neural Networks Using Nonequilibrium Response Theory[J]. 2020.&lt;/p&gt;
&lt;h2 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要&quot;&gt;&lt;/a&gt;摘要&lt;/h2&gt;&lt;p&gt;循环神经网络（RNN）是一种受大脑启发的模型，其广泛的应用于机器学习，以进行连续数据的分析。本工作有助于使用非平衡学说的响应理论更深度地理解RNN如何处理输入信号。对于一类由输入信号驱动的连续时间随机RNN（SRNN），我们为其输出推导出一个沃尔泰拉级数的序列表示。这种表示法是可解释的，并将输入信号从SRNN结构中分离出来。序列的核是一些递归定义的相关函数，其与完全决定输出的无扰动动力学相关。利用这种表示的联系及其对粗糙路径理论的影响，我们确定了一个通用特征——响应特征，其被证明是输入信号的张量积的特征与自然支撑基础。特别地，我们展示了仅优化了读出层的权重，而隐藏层的权重保持固定、未被优化的SRNN，这可被看作是在与响应特征相关的再生核希尔伯特空间中执行的核机器。&lt;/p&gt;</summary>
    
    
    
    <category term="论文阅读" scheme="https://values.keys.moe/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="论文阅读" scheme="https://values.keys.moe/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
  </entry>
  
  <entry>
    <title>新的故事与旧的回忆 —— 2017~2021 我的本科四年</title>
    <link href="https://values.keys.moe/2021/06/21/%E6%96%B0%E7%9A%84%E6%95%85%E4%BA%8B%E4%B8%8E%E6%97%A7%E7%9A%84%E5%9B%9E%E5%BF%86%20%E2%80%94%E2%80%94%202017~2021%20%E6%88%91%E7%9A%84%E6%9C%AC%E7%A7%91%E5%9B%9B%E5%B9%B4/"/>
    <id>https://values.keys.moe/2021/06/21/%E6%96%B0%E7%9A%84%E6%95%85%E4%BA%8B%E4%B8%8E%E6%97%A7%E7%9A%84%E5%9B%9E%E5%BF%86%20%E2%80%94%E2%80%94%202017~2021%20%E6%88%91%E7%9A%84%E6%9C%AC%E7%A7%91%E5%9B%9B%E5%B9%B4/</id>
    <published>2021-06-20T16:54:45.000Z</published>
    <updated>2022-09-28T06:55:23.539Z</updated>
    
    <content type="html"><![CDATA[<img src="/2021/06/21/%E6%96%B0%E7%9A%84%E6%95%85%E4%BA%8B%E4%B8%8E%E6%97%A7%E7%9A%84%E5%9B%9E%E5%BF%86%20%E2%80%94%E2%80%94%202017~2021%20%E6%88%91%E7%9A%84%E6%9C%AC%E7%A7%91%E5%9B%9B%E5%B9%B4/新的故事与旧的回忆.jpg" alt><span id="more"></span>]]></content>
    
    
    <summary type="html">&lt;img src=&quot;/2021/06/21/%E6%96%B0%E7%9A%84%E6%95%85%E4%BA%8B%E4%B8%8E%E6%97%A7%E7%9A%84%E5%9B%9E%E5%BF%86%20%E2%80%94%E2%80%94%202017~2021%20%E6%88%91%E7%9A%84%E6%9C%AC%E7%A7%91%E5%9B%9B%E5%B9%B4/新的故事与旧的回忆.jpg&quot; alt&gt;</summary>
    
    
    
    <category term="杂谈" scheme="https://values.keys.moe/categories/%E6%9D%82%E8%B0%88/"/>
    
    
    <category term="杂谈" scheme="https://values.keys.moe/tags/%E6%9D%82%E8%B0%88/"/>
    
  </entry>
  
  <entry>
    <title>Prioritize Crowdsourced Test Reports via Deep Screenshot Understanding</title>
    <link href="https://values.keys.moe/2021/04/20/Prioritize%20Crowdsourced%20Test%20Reports%20via%20Deep%20Screenshot%20Understanding/"/>
    <id>https://values.keys.moe/2021/04/20/Prioritize%20Crowdsourced%20Test%20Reports%20via%20Deep%20Screenshot%20Understanding/</id>
    <published>2021-04-20T09:24:05.000Z</published>
    <updated>2022-09-28T08:28:30.927Z</updated>
    
    <content type="html"><![CDATA[<img src="/2021/04/20/Prioritize%20Crowdsourced%20Test%20Reports%20via%20Deep%20Screenshot%20Understanding/title.png" alt><h1><span id="基于深度截图理解的众包测试报告优先级排序">基于深度截图理解的众包测试报告优先级排序</span></h1><h2><span id="摘要">摘要</span></h2><p>​众包测试在移动应用程序测试中日益占据主导地位，但对于应用开发者来说，审查数量过多的测试报告是很大的负担。已有许多学者提出基于文本和简单图片特征的测试报告处理方法。然而，在移动应用测试中，测试报告所包含的文本较为精简且信息不够充分，图片则能够提供更丰富的信息。这一趋势促使我们在深度截图理解的基础上，对众包测试报告的优先级进行排序。<br>​本文中，我们提出了一种新的众包测试报告优先级排序方法，即DEEPPRIOR。我们首先引入一个新的特征来代表众包测试报告，即DEEPFEATURE，它基于对应用程序截图的深度分析，涵盖了所有组件（widget）及它们的文本、坐标、类型甚至是意图。DEEPFEATURE包括直接描述bug的bug特征（Bug Feature），和刻画bug完整上下文的上下文特征（Context Feature）。DEEPFEATURE的相似度用于表示测试报告的相似度，并被用于对众包测试报告进行优先级排序。我们形式上将相似度定义为DEEPSIMILARITY。我们还进行了一个实证实验，以评估所提技术在大型数据中的有效性。结果表明，DEEPPRIOR性能最佳，以不足一半的成本获得优于其它方法的结果。</p><p>索引词 众包测试，移动应用测试，深度截图理解</p><span id="more"></span><h2><span id="i-介绍">I. 介绍</span></h2><p>​众包已成为许多领域的主流技术之一。众包的开放性带来了许多优势。例如，可以在多个不同的实际环境中模拟对众包活动的操作。这样的优势有助于缓解移动应用（app）测试中严重的“碎片化问题”。成千上万种不同品牌、不同操作系统（OS）版本、不同硬件传感器就是安卓测试中众所周知的“碎片化问题”[1]。众包测试是解决这一问题的最佳方案之一。应用开发者可以将它们的应用程序分发给拥有不同移动设备的众包工人，并要求他们提交包含应用截图和文本描述的测试报告。这有助于应用开发者尽可能多的发现问题。<br>​然而，众包测试的报告审查效率低是一个严重的问题。众包的开放性会导致有大量的报告被提交，而几乎82%的提交的报告都是重复的[2]。由于报告的复杂性，自动审查报告是一项艰巨的工作。在文字部分，自然语言的复杂性可能导致歧义，并且众包工人可能使用不同的词来描述相同的对象，或使用相同的词来描述不同的场景。在图像部分，由于许多应用使用相似的UI构建函数，截图的相似度也几乎没有帮助。因此，对于应用程序开发人员来说，及时发现报告中的bug是困难但重要的。<br>​在近期的研究中，测试报告的处理通常分为两部分：应用截图和文本描述。现有的研究分别对这两部分进行分析以提取特征。对于文本描述，现有的方法是提取关键词，并根据预定义的词汇对关键词进行标准化处理。对于应用截图，它们将每个截图作为一个整体，提取用数字向量表示的图像特征。在获得了这两部分的结果之后，目前大多数的研究都以文本为依托，将截图作为补充材料，或者简单地将图像信息和文本信息进行拼接。然而，我们认为这样的处理方式会导致许多有价值的信息丢失。文本描述和应用截图之间的关系会被遗漏，报告的去重与确定优先级效果也可能更差。<br>​在本文中，我们提出了一种新颖的方法，即DEEPPRIOR，通过对截图的深入理解来确定众包测试报告的优先级。DEEPPRIOR详细顾及了对应用截图和文本描述的深入理解。对于一份被提交的测试报告，我们会从截图和文本两者中提取信息。在截图中，我们通过计算机视觉（CV）技术收集所有widget，并根据文本描述定位问题widget（表示为WP）。其余的widget则被视为上下文widget（表示为WC）。本研究以自然语言处理（NLP）技术对文本进行处理，并分为两部分：复现步骤（以R表示）和bug描述（以P表示）。复现步骤被进一步标准化为“操作-对象”序列。bug描述也被进一步处理以提取对问题widget的描述，从而进行WP的定位。<br>​我们不对应用截图和文本描述进行单独处理，而是将它们作为一个整体，并将所有信息收集起来作为报告的DEEPFEATURE。根据bug本身的关联性，DEEPFEATURE包括bug特征（Bug Feature，BFT）和上下文特征（Context Feature，CFT）。bug特征由WP和P组成，它表示报告中揭示的和bug直接相关的信息。上下文特征由WC和R组成，其代表的是上下文信息，包括触发bug的操作轨迹和bug发生时的activity的信息。<br>​将上述特征整合到DEEPFEATURE中后，DEEPPRIOR将计算报告中的DEEPSIMILARITY以进行优先级排序。对于bug特征和上下文特征，我们分别计算DEEPSIMILARITY。<br>​对于bug特征，为了计算报告中的WP的DEEPSIMILARITY，我们利用CV技术对特征点进行提取和匹配。P是一个简短的文本描述，因此我们使用NLP技术，在自建词汇表的基础上提取与bug有关的关键词，比较关键词的频率作为DEEPSIMILARITY。<br>​对于上下文特征，WC被输入到一个预先训练好的深度学习分类器中，以识别每个widget的类型，每个类型的数字向量作为WC DEEPSIMILARITY。R由一系列操作和对应的widget组成，代表了从应用启动到bug发生的序列。因此，我们按照R的顺序，利用NLP技术提取操作和对象。我们将“操作-对象”序列作为行为轨迹，并计算DEEPSIMILARITY。<br>​之后进行优先级排序。我们首先构建一个NULL 报告（如章节III-D中定义），并将其添加到优先级报告池中。之后，我们反复计算每个未确定优先级的报告和报告池中所有报告之间的DEEPSIMILARITY。具有与优先级报告池中相比最低的“最小DEEPSIMILARITY”的报告会被放入优先级报告池中。<br>​我们还设计了一个实证实验，使用一个大型活跃的众包测试平台的大规模数据集组。我们将DEEPPRIOR与其他两种方法进行了比较，结果表明，DEEPPRIOR是有效的。<br>​本文的重要贡献如下：<br>​* 我们提出了一个新颖的方法，通过对截图的深入理解和详细的文本分析，对众包测试报告进行优先级排序。我们从截图中提取所有widget，将文本信息分类到不同的类别，并构建DEEPFEATURE。<br>​* 我们构建了一个用于对截图进行深入理解的集成数据集组，包括大规模的widget图像数据集、大规模的测试报告关键词词汇、大规模的文本分类数据集和大规模的众包测试报告数据集。<br>​* 基于数据集，我们对提出的方法DEEPPRIOR进行了实证评估，结果表明，DEEPPRIOR以不到一半的开销胜过了当前最新的方法。</p><h2><span id="ii-背景与动机">II. 背景与动机</span></h2><p>​众包测试在移动应用测试中得到了广泛的应用，其优点是显而易见的，但其弊端也是不可忽视的。在大多数主流的众测平台上，众测工人都需要提交一份报告来描述自己所遇到的bug。报告的主题是对bug的截图和文本描述。应用的截图和文本描述也同样是对众测报告进行优先级排序的主要依据。<br>​目前考虑截图的众测报告处理方案，如[2][3]主要是分析应用截图特征和文本描述信息来衡量所有报告之间的相似度。虽然他们考虑了应用截图，但只是将图片简单地处理为宽x高xRGB的矩阵。然而，这些方法忽略了丰富而有价值的信息，我们认为应该把应用截图看作是有意义的widget的集合，而非无意义的像素的集合。这是因为在回顾众测报告数据集的时候，我们发现了一些生动的例子，现有的方法难以处理它们，因为这些方法只是做了简单的特征提取，而没有对截图进行深度理解。</p><p>A. 样例1：不同的应用主题<br>        现在的应用都支持不同的主题，用户可以根据自己的喜好定制应用的外观（图1）。此外，“深色模式”使得配色方案更加复杂。图像特征提取算法很难处理这样的复杂问题，会出现错误。从这些样本中我们可以发现，三份报告中的应用截图分别为蓝色、白色和绿色主题。这三份报告都是报告音乐资源文件加载失败的。然而，根据文献[2]，图像颜色特征是报告替代的重要组成部分之一。不同颜色的应用截图将被识别为不同的截图。<br><img src="/2021/04/20/Prioritize%20Crowdsourced%20Test%20Reports%20via%20Deep%20Screenshot%20Understanding/图1.png" alt><br>图1 样例1：不同的应用主题<br><img src="/2021/04/20/Prioritize%20Crowdsourced%20Test%20Reports%20via%20Deep%20Screenshot%20Understanding/图1-1.png" alt></p><p>B. 样例2：相同截图上的不同bug<br>        如图2所示，两份报告使用的是相同的应用activity的截图，图像特征提取算法会在这两张截图之间给出一个较高的相似度。然而，根据bug描述，这两份报告描述的是完全不同的bug。在DEEPPRIOR中，对于报告#1128，我们可以提取出“未找到媒体”的文字；对于报告#1127，除了提示信息之外，我们还可以提取出音量widget，DEEPPRIOR可以识别出不同的问题。<br>        <img src="/2021/04/20/Prioritize%20Crowdsourced%20Test%20Reports%20via%20Deep%20Screenshot%20Understanding/图2.png" alt><br>图2 样例2：相同截图上的不同bug<br><img src="/2021/04/20/Prioritize%20Crowdsourced%20Test%20Reports%20via%20Deep%20Screenshot%20Understanding/图2-2.png" alt></p><p>C. 样例3：不同截图上的相同bug<br>        如图3所示，顶部的ImageView widget的内容不同，且其占据了整个页面的很大比例。并且，由于测试时间不同，评论也不同。因此，现有的方法会认为两张截图的相似度低，即使文本描述的相似度很高，也会降低整体相似度。而通过DEEPPRIOR，我们可以提取底部的弹出信息，为“发表评论失败”，并给两个报告分配一个很高的相似度。这样的弹窗被认为是相当重要的、包含bug的widget。<br><img src="/2021/04/20/Prioritize%20Crowdsourced%20Test%20Reports%20via%20Deep%20Screenshot%20Understanding/图3.png" alt><br>图3 样例3：不同截图上的相同bug<br><img src="/2021/04/20/Prioritize%20Crowdsourced%20Test%20Reports%20via%20Deep%20Screenshot%20Understanding/图3-3.png" alt></p><h2><span id="iii-方法">III. 方法</span></h2><p>​本节介绍DEEPPRIOR的详细内容，即通过深度截图理解，对众测报告进行优先级排序。DEEPPRIOR由4个阶段组成，包括特征提取、特征聚合、DEEPSIMILARITY的计算与报告优先级排序。我们从应用截图和文本描述两者中收集4种不同类型的报告特征。然后，我们将提取的特征汇总成一个DEEPFEATURE，其包括bug特征和上下文特征。基于DEEPFEATURE，我们设计了一个算法来计算每两份测试报告之间的DEEPSIMILARITY。基于预先定义的规则（详见章节III-D），我们根据DEEPSIMILARITY对测试报告进行优先级排序。DEEPPRIOR方法的总体框架可参考图4。<br><img src="/2021/04/20/Prioritize%20Crowdsourced%20Test%20Reports%20via%20Deep%20Screenshot%20Understanding/图4.png" alt></p><p>图4 DeepPrior框架</p><h3><span id="a-特征提取">A. 特征提取</span></h3><p>​第一步也是最重要的一步，为特征提取。我们分别对众测报告的应用截图和文本描述进行分析。<br>​\1) 应用截图中的特征。应用截图在众测报告中至关重要。众测人员需要在bug发生时进行截图，以便更好地阐述bug。如文献[2]所描述，文本描述仅能提供有限的信息，可能不足以清晰地描述bug。因此，除了文本描述之外，还要考虑截图来提供更多的信息。在一张截图中，存在许多不同的widget，有些widget可以提示bug信息。因此，对截图的深度理解主要依赖于widget。在DEEPPRIOR中，我们使用CV技术和深度学习（DL）技术来提取所有的widget并分析其信息。DL技术功能强大，CV技术可以处理更多种类的任务[4]。<br>​问题widget。一个应用activity可以被看作是一个有组织的widget集。一般来说，在众测任务中，众测人员能够发现的bug都会通过widget显露出来。因此，找到引发bug的widget，并将这个widget与其他widget区分开来是很重要的，而该widget就是我们所定义的问题widget（WP）。为了区分问题widget，我们分析文本描述。在众测报告中，众测人员会指出在bug发生前操作了哪个widget。如章节III-A2所示，我们可以从文本描述中提取出问题widget，并且为了定位问题widget，我们针对不同的情况可采取两种不同的策略。<br>​* 如果提取的widget包含文本，我们将widget的截图和文本描述中的文本进行匹配。被匹配的widget被认为是问题widget。<br>​* 如果widget上没有文本或文本匹配失败，我们将提取的widget送入深度神经网络，以识别简单的widget意图。该深度神经网络是在Xiao等人[5]的研究基础上修改而来的。该模型通过卷积神经网络（CNN）将widget截图编码为特征向量。输出是使用循环神经网络（RNN）从特征向量解码而得的短文本片段，该短文本片段描述了widget意图。<br>​上下文widget。除了问题widget以外，应用截图中展现的widget集还包含了更多组成上下文的widget，而它们对深度图像理解也是至关重要的。在早期的调查中，我们发现，即使问题widget、复现步骤（activity启动路径）和bug描述相同，应用的activity也可能全然不同（如章节II-C中的启发性样例）。在这个情况下，上下文widget对于识别差异而言十分重要。因此我们将其余的widget收集为上下文widget（WC）。对于每个上下文widget，我们将widget截图输入卷积神经网络，以识别其类型。每个类型由14维向量构成。<br>​卷积神经网络能够识别14种不同类型的、最为广泛使用的widget，包括Button（BTN）、CheckBox（CHB）、CheckTextView（CTV）、EditText（EDT）、ImageButton（IMB）、ImageView（IMV）、ProgressBarHorizontal（PBH）、ProgressBarVertical（PBV）、RadioButton（RBU）、RatingBar（RBA）、SeekBar（SKB）、Switch（SWC）、Spinner（SPN）、TextView（TXV）。为了训练神经网络，我们收集了36573张均匀分布在这14种类型上的widget截图。训练集、验证集和测试集的比例为7:1:2，这是图像分类任务的常见做法。神经网络由多个卷积层、最大池化层和全连接层组成。采用AdaDelta算法作为优化器，模型采用categorical_crossentropy作为损失函数。<br>​\2) 来自文本描述的特征。除了应用截图，文本描述可以更直观、更直接地提供bug信息。同时，文本描述也可以作为应用截图的正向补充。在DEEPPRIOR中，我们采用NLP技术，特别是DL算法，对测试报告中的文本描述进行处理。<br>​在文本描述中，众测人员需要对截图中的bug进行描述，并提供复现步骤，即从应用启动到bug发生的操作顺序。然而，在大多数众测平台上，bug描述和复现步骤是混在一起的，并且由于专业能力不同，众包工人并不被要求遵循特定的模式[6]。因此，将bug描述与复现步骤区分开是很复杂的。为了解决这个问题，我们采用了TextCNN模型[7]。<br>​TextCNN模型可以通过预先训练的词向量完成句子级分类任务。在将文本输入模型之前，我们对数据进行预处理。将测试报告的文本描述分割成句子，再使用jieba库将句子分割成单词，根据停顿词列表过滤掉停顿词。预处理后，我们将文本送入词向量层。在该层中，利用Word2Vec模型[8]将文本转化为128维的向量。之后，我们采用多个卷积层和最大池化层来提取文本特征。在最后一层，我们使用SoftMax激活函数，得到每个句子是bug描述还是复现步骤的概率。最后，我们将所有分类为bug描述或复现步骤的句子进行合并。为了训练TextCNN模型，我们构建了一个大规模的文本分类数据集，由2252个bug描述和2088个复现步骤组成。我们按照惯例，将训练集、验证集和测试集的比例设置为6:2:2。<br>​bug描述。bug描述总是以短句的形式出现。因此，我们用一个向量来表示句子，其也用Word2Vec模型编码。大多数的bug描述都遵循某种特定的模式，如“对某些widget进行了某些操作，发生了某些非预期的行为”，所以即使具体的词语会有所不同，但提取这种特征仍是有效的。<br>​另一个重要的过程是提取问题widget的描述，以帮助对问题widget进行定位。为了实现这一目标，我们采用基于HMM（Hidden Markov Model，隐马尔科夫模型）模型的文本分割算法[9]，并在文本分割之后分析bug描述的各个部分的词性。然后，我们提取对象部分作为问题widget定位的依据，句子中这样的对象部分就是触发bug的widget。在获取对象之后，我们使用上文所述的策略对问题widget进行定位。<br>​复现步骤。除了bug描述外，文本描述的另一个重要部分是复现步骤。复现步骤是一系列的操作，描述了用户从应用启动到bug发生的操作。对于分类到复现步骤的句子，我们按照报告中的初始顺序进行处理。我们使用相同的NLP算法进行文本分割，并对每个句子的每个文本段进行词性分析。然后，收集操作部分和对象部分，形成“操作-对象”对。然后，我们将“操作-对象”对连接成一个“操作-对象”序列。另外，除了操作词和对象，我们还为一些特定的操作添加一些补充信息。比如，假设有一个操作是键入操作，我们会添加输入内容作为补充信息，因为不同的测试输入可能导致不同的处理结果，使应用定向到不同的activity上。最后，经过形式化处理后，我们就可以从文本描述中获得复现步骤。</p><h3><span id="b-特征聚合">B. 特征聚合</span></h3><p>​在从应用截图和文本描述中获取所有特征后，我们将其汇总为两个特征类别。bug特征（Bug Feature, BFT）和上下文特征（Context Feature, CFT）。bug特征指的是众测报告中直接反映或描述bug的特征，而上下文特征是由bug出现时提供环境描述的特征聚合而成。<br>​\1) bug特征（BFT）：bug特征可以直接提供bug的信息。由于众测报告是由应用截图和文本描述组成，这两部分都包含了发生bug的关键信息。在应用截图中，我们提取了问题widget，是widget截图。DEEPPRIOR可以自动提取这样的信息。在文本描述中，bug描述部分直接描述了bug。因此，在平衡考虑应用截图和文本描述的情况下，我们将问题widget和bug描述汇总为bug特征。<br>​\2) 上下文特征（CFT）：上下文特征包括了为bug的发生构建完整上下文的特征。在应用截图中，上下文widget由问题widget以外的所有widget组成。在文本描述中，之所以考虑到复现步骤信息是因为其提供了从应用程序启动到bug发生时的完整操作路径，可以帮助识别两个测试报告的bug是否在同一个应用activity上。因此，将上下文widget和复现步骤汇总在一起，构成上下文特征。<br>​\3) 特征聚合：借助bug特征和上下文特征，我们可以将众测报告中的应用截图和文本描述中所获得的所有特征聚合到最终的DEEPFEATURE中。我们并非直接将应用截图转化为简单的特征向量，而是对应用截图进行深度理解。同时，我们对应用截图和文本描述之间的结合也更加紧密。此外，我们将应用截图和文本描述作为一个整体，根据它们在bug反馈中的作用进行划分。bug特征非常重要，我们认为上下文特征在众包测试报告的优先级确定中也应起到至关重要的作用，bug相似度的计算很大程度上依赖于整个上下文。</p><h3><span id="c-deepsimilarity的计算">C. DEEPSIMILARITY的计算</span></h3><p>​要对众测报告进行优先级排序，一个主要的步骤就是计算所有报告间的相似度。由于我们是第一个将深度截图理解引入到报告的优先级排序中的,我们将这个相似度命名为DEEPSIMILARITY。之前的研究[2][3]普遍采用合并不同特征的做法，我们分别计算不同特征的DEEPSIMILARITY，并为不同特征的结果分配不同的权重。形式化表达如下：<br>​<img src="/2021/04/20/Prioritize%20Crowdsourced%20Test%20Reports%20via%20Deep%20Screenshot%20Understanding/公式1.png" alt><br>​\1) bug特征：我们分别计算问题widget和bug描述的DEEPSIMILARITY，并使用参数α来整合他们。<br>​问题widget。问题widget是根据章节III-A1中介绍的策略，从应用截图中提取的widget截图。为了计算问题widget的DEEPSIMILARITY，我们提取了widget截图的图像特征。为了提取图像特征，我们采用了最先进的SIFT（Scale-Invariant Feature Transform）算法[10]。因此，每个widget由一个特征点集来表示。SIFT算法的优点是可以处理不同尺寸、位置和旋转角度的图像，而这种图像在这样一个移动设备有成千上万种不同型号的时代是相当普遍的。为了对不同众测报告中的问题widget进行对比和匹配，我们使用FLANN库[11]。经过计算，我们可以得到一个0到1间的分数，0表示完全不同，1表示完全相同。这个分数可以看作是问题widget的DEEPSIMILARITY。<br>​bug描述。bug描述是简短的语句，简要描述了众测报告中的bug。因此，我们使用NLP技术对bug描述进行编码。遵循以往研究中的方法，我们使用Word2Vec模型作为编码器。为了提高Word2Vec模型的性能，我们构建了一个测试报告关键词数据库。测试报告关键词 数据库包含了8647个与软件测试、移动应用、测试报告相关的关键词，包括了标注的同义词、反义词和多义词。编码后的bug描述是一个100维的向量。之后，仍参考前人的研究，如[2][3]，我们采用广泛使用的欧氏度量算法来成对地计算不同测试报告中的bug描述的DEEPSIMILARITY。为了统一不同尺度的数值，我们使用函数 (x-min)&#x2F;(max-min)，以将每个结果x归一化到[0,1]区间，其中max是所有结果的最大值，min是所有结果的最小值。<br>​\2) 上下文特征：我们还分别计算上下文widget和复现步骤的DEEPSIMILARITY，并使用参数β来整合他们。<br>​上下文widget。上下文widget也是bug发生时整个上下文的重要组成部分。为了对应用截图有深度理解，特别是应用截图上的widget，我们使用卷积神经网络来识别每个提取的widget截图的widget类型，并形成一个包含14种widget类型数量的向量。之后，我们使用欧氏度量算法来计算获取的14维向量的距离。我们考虑了每个类型的widget的绝对数量和所有widget的分布情况。欧氏度量算法的结果（从0到1）即上下文widget的DEEPSIMILARITY。<br>​复现步骤。在特征提取过程中，复现步骤被转化为“操作-对象”序列。为了计算“操作-对象”序列的DEEPSIMILARITY，我们采用动态时间规整（Dynamic Time Warping, DTW）算法处理待比较的“操作-对象”序列。DTW算法在自动语音识别方面表现出色。在本文中，我们调整了DTW算法来处理相应的众测报告中触发bug的操作路径。DTW算法可以测量时空序列的相似度，尤其是可能存在“速度”变化的时空序列。具体而言，我们任务中的“速度”是指不同的用户操作可以通过不同的路径到达同一个应用activity的情况。与其他轨迹相似度算法相比，DTW由于可以处理不同长度的序列，所以匹配的效果更好，适合处理“操作-对象”序列。</p><h3><span id="d-报告的优先次序">D. 报告的优先次序</span></h3><p>​在聚合了DEEPFEATURE，并定义了DEEPSIMILARITY的计算规则后，我们开始对众测报告进行优先级排序。首先，我们构建两个空报告池：非优先级报告池和优先级报告池。所有的众测报告最初都会被放入非优先级报告池。<br>​与[3]中采用的随机选择一个报告作为初始报告的策略不同，我们认为应该平等的对待所有报告，随机选择报告可能影响最终的优先级。因此，为了使优先级算法形式化、统一化，我们引入了NULL报告的概念，它也包含了四个特点。<br>​* 问题widget：问题widget的截图本质上是一个三维矩阵，分别代表宽度、高度和三个颜色通道。因此，我们将问题widget构造为一个零矩阵。零矩阵的宽度和高度设置为所有实际众测报告的平均大小。直观地说，它是一个全黑的图像。<br>​* bug描述：NULL报告的bug描述直接设置为空字符串，由于字符串长度为0，显然不包含任何单词，经过Word2Vec处理后，特征向量将是一个100维的全部为“0”的向量。<br>​* 上下文widget：对于NULL报告的上下文widget，我们直接构造出代表数量为14种的不同类型widget的向量，并且所有元素均为0。这表示众包的应用截图上“没有”widget。<br>​* 复现步骤：NULL报告的复现步骤也设置为空字符串，“操作-对象”序列的长度也为0。<br>​优先级划分的主要共识是，在某些报告会重复描述bug的情况下，尽早发现所有的bug[3][13][14]。<br>​因此，要尽早为开发者提供尽可能多的描述不同bug的报告。基于这一思想，我们设计了如下的优先级策略，形式化表达式见算法1。<br>​<img src="/2021/04/20/Prioritize%20Crowdsourced%20Test%20Reports%20via%20Deep%20Screenshot%20Understanding/算法1.png" alt><br>​首先，我们根据上述规则构建NULL报告，并将NULL报告追加到空的优先级报告池中。接下来进行一个迭代的过程。我们计算每个未确定优先级的报告与整个优先级报告池的DEEPSIMILARITY，此DEEPSIMILARITY定义为未确定优先级的报告与优先级报告池中的所有报告的最小DEEPSIMILARITY。与优先级报告池中DEEPSIMILARITY最低的报告将被移至优先报告池中。</p><h2><span id="iv-评估">IV. 评估</span></h2><h3><span id="a-实验设置">A. 实验设置</span></h3><p>​为了评估我们提出的DEEPPRIOR，我们设计了一个实证实验。为了完成实验，我们收集了10个不同移动应用的536份众包测试报告（详见表I）。A1到A10代表10个应用，不同应用的测试报告数量在10到152之间。我们还邀请软件测试专家根据测试报告所描述的bug进行人工分类，平均一个bug类别的报告数量为8.06份。</p><p>表I 实验应用<br><img src="/2021/04/20/Prioritize%20Crowdsourced%20Test%20Reports%20via%20Deep%20Screenshot%20Understanding/表1.png" alt></p><p>​在众包测试报告数据集的基础上，我们建立了3个具体的数据集来更好地支持评估，包括1) 大规模widget图像数据集，2) 大规模测试报告关键词集，3) 大规模文本分类数据集。这4个数据集构成了综合数据集。<br>​我们共设计了三个研究问题（RQ）来评估所提出的测试报告优先级确定方法DEEPPRIOR。</p><p>​* RQ1：DEEPPRIOR如何有效识别从应用截图中提取的widget类型？<br>​* RQ2：DEEPPRIOR对众测报告中的文本描述的分类效果如何？<br>​* RQ3：DEEPPRIOR能多有效地确定众测报告的优先级？</p><h3><span id="b-rq1widget类型分类">B. RQ1：widget类型分类</span></h3><p>​第一个研究问题的设定是评估我们对应用截图的处理效果。在应用截图处理中，最重要的部分是widget的提取和分类。因此，我们评估了widget类型分类的CNN的准确性。我们共收集了36573张不同的widget图像，这些图像在14个类别中均匀分布。<br>​CNN的具体介绍详见章节III-A1。数据集按惯例，按照7:2:1的比例划分为训练集、验证集和测试集。在CNN模型训练完成后，我们对测试集的准确性进行评估。widget类型分类的总体准确率达到89.98%。具体而言，我们用精确率（precision）、召回率（recall）和F值（F-Measure）评估网络。计算公式如下，其中TP表示真正例样本，FP表示假正例样本，TN表示真负例样本，FN表示假负例样本。</p><img src="/2021/04/20/Prioritize%20Crowdsourced%20Test%20Reports%20via%20Deep%20Screenshot%20Understanding/公式23.png" alt><p>​评估结果如表II所示，精确率平均值达90.05%，最低精确率为74.36%，最高为99.81%。对于衡量实际检索到的实例总量的召回率方面，平均值为89.98%，召回率值在70.83%至100%之间。F值是精确率和召回率的调和平均数，其平均值达到89.92%。以上结果反映了所提出的分类器的突出能力。</p><p>表II widget类型分类<br><img src="/2021/04/20/Prioritize%20Crowdsourced%20Test%20Reports%20via%20Deep%20Screenshot%20Understanding/表2.png" alt></p><p>​我们也对结果进行了深入的观察。我们发现，有两组widget容易被混淆。第一组包括ImageButton和ImageView。不难理解，从视觉上看，这两种类型几乎是无法识别的。这两种类型之间的唯一区别是，ImageButton可以触发一个操作，而ImageView只是一个简单的图像。不过，有一点很重要的是，在应用设计中，开发者可以在ImageView widget上添加一个超链接来实现等效的效果。第二组包括Button、EditText和TextView。这三个widget都是一个固定的区域，里面包含一个文本片段，从视觉上看也很相似，即使是人类也难以辨别。此外，一些特殊的渲染方式也让这些widget更加难以识别。根据我们的调查，我们发现这两组易混淆的widget不会有太大影响，无论是从视觉角度还是从功能角度，都可以把这些widget当作为等价的widget。<br>​RQ1的结论：CNN对widget类型进行分类的总体准确率达到89.98%，对于每个具体类型，平均精确率达90.05%，最低精确率为74.36%，F值为89.92%。另外，根据我们对实测报告的调查，即使一些精确率较低的模型，其视觉和功能特征也不会对DEEPPRIOR造成负面影响。</p><h3><span id="c-rq2文本描述分类">C. RQ2：文本描述分类</span></h3><p>​在文本描述的处理中，我们将其分为两类：bug描述和复现步骤。不同的文本描述被认为是不同的报告特征。为了对文本描述进行分类，我们将文本描述分割成句子。然后，我们将这些句子输入到TextCNN模型中来完成任务，详细介绍参见章节III-A2。同时，为了更好地训练和评估网络，我们建立了一个大规模的文本分类数据集。该数据集包含了4340个标记的文本片段，其中包括2252个bug描述和2088个复现步骤。数据集按照7:2:1的比例分为训练集、验证集和测试集。</p><p>表III 文本分类<br><img src="/2021/04/20/Prioritize%20Crowdsourced%20Test%20Reports%20via%20Deep%20Screenshot%20Understanding/表3.png" alt></p><p>​RQ2的结论：文本分类的整体准确率达到96.65%。精确率、召回率和F值都在98%以上。这样的结果显示了DEEPPRIOR对文本描述的优秀分析能力，这也为众测报告的优先级的排序打下了坚实的基础。</p><h3><span id="d-rq3众包测试报告的优先级排序">D. RQ3：众包测试报告的优先级排序</span></h3><p>​在本研究问题中，我们评估DEEPPRIOR的测试报告优先级排序效果。我们使用的指标是APFD（Average Percentage of Fault Detected）指标[15]，Feng等人也使用该指标对众测报告进行优先级排序的评估[3]。在公式中，表示最先发现bugi的报告的索引，<img src="file:///C:/Users/Angelo/AppData/Local/Temp/msohtmlclip1/01/clip_image006.png" alt="img">n是报告总数，M是暴露的bug的总数。</p><img src="/2021/04/20/Prioritize%20Crowdsourced%20Test%20Reports%20via%20Deep%20Screenshot%20Understanding/公式4.png" alt><p>​为了更好地说明DEEPPRIOR的优势，我们将DEEPPRIOR与以下优先级策略进行比较。<br>​* IDEAL：这种策略理论上是具有最好的优先级排序，意味着开发人员可以在最短的时间内审查所有报告中的bug。<br>​* IMAGE：这种策略只使用DEEPPRIOR的深度图像理解结果来对测试报告进行排序，因为深度图像理解是我们研究的重要组成部分。<br>​* BDDIV：这种策略参考了Feng等的工作[3]的算法，这也是众包测试报告优先级排序的最先进的方法。<br>​* RANDOM：RANDOM策略指的是没有任何优先级策略的情况。<br>​对于DEEPPRIOR和IMAGE策略，因为我们的方法很稳定，所以我们运行一次，训练后的模型不会因为不同的尝试而产生不同的结果；对于IDEAL策略，我们手动计算APFD，因为对于固定的报告簇而言，它是一个定值；对于BDDIV策略，我们运行30次，然后像原文[3]一样计算平均值；对于RANDOM策略，我们运行100次，以消除偶然情况的影响。<br>​首先，我们将DEEPPRIOR与RANDOM策略进行比较。如表IV所示，我们发现DEEPPRIOR策略比RANDOM策略效果好了许多，从15.15%至38.93%之间不等，并且平均提升幅度达27.04%。由此可见DEEPPRIOR的优越性。</p><p>表IV DEEPPRIOR报告优先级排序结果与比较<br><img src="/2021/04/20/Prioritize%20Crowdsourced%20Test%20Reports%20via%20Deep%20Screenshot%20Understanding/表4.png" alt></p><p>​然后，我们将DEEPPRIOR与单一IMAGE策略的结果进行对比。DEEPPRIOR的平均提升幅度为4.54%，在2个应用（A3和A4）中，DEEPPRIOR的表现远超过了IMAGE策略。对于A8来说，DEEPPRIOR的策略弱于IMAGE策略。我们对A8的报告进行检查，发现是因为文本描述写的不好，不能对报告的优先级有积极的帮助。总的而言，结果证明了文本分析和深度图像理解两者结合的必要性，单一策略可以弥补彼此的缺点，提高确定优先级的准确性。<br>​同时，我们还对DEEPPRIOR和BDDIV策略（即最先进的方法）进行了比较。根据实验结果，DEEPPRIOR的表现优于BDDIV，平均提升幅度为3.81%。在一些应用中的提升尤其明显。此外，我们还记录了从读取报告簇到输出优先级排序好的报告的总时间开销。结果显示，DEEPPRIOR所使用的时间不到BDDIV的一半，表现出极大的性能优势。<br>​DEEPPRIOR相对于BDDIV的另一个优势是，DEEPPRIOR可以输出稳定的结果，而BDDIV的结果会浮动。根据BDDIV策略的详细结果（于在线包中），我们发现BDDIV的波动性很大。<br>​在不同的应用上与基线策略相比的提升情况不同，可以用某些原因解释这一点。首先，“报告-种类”的比率是不同的，所以在一个应用的有限的activity集中，同样的activity重复出现的频率会变得很高。其次，不同的应用有不同的内容。比如A1是一款儿童教育类应用，它由大量的图片、视频、变体文字组成。在这种情况下，想要提取有用的文本信息，并对应用截图有一个全面的理解，就会变得更为复杂。因此，预估值会降低。<br>​RQ3的结论：DEEPPRIOR对众测报告进行优先级排序的能力是非常优秀的，它的性能优于最先进方法BDDIV，但开销不到其一半。同时，IMAGE策略的具体实验也表明了我们深度截图理解算法的有效性。与最先进的方法相比，DEEPPRIOR的表现更加稳定。</p><h3><span id="e-有效性风险">E. 有效性风险</span></h3><p>​本次实验中应用的类别是有限的。我们的15个实验应用涵盖了8个不同的类别（根据应用商店分类法），类别覆盖有限。但是，我们要强调的是，由于我们对应用截图的深度理解涉及到对应用活动的布局特征，因此DEEPPRIOR仅适用于分析具有网格布局或列表布局的应用程序。我们所表述的也仅限于此类布局的应用中。<br>​众包工人的注册不受限制。众包工人的能力不受限制，可能会出现低质量的报告。然而，即使一些报告的质量很低，如果它真的包含一个bug，DEEPPRIOR也可以识别它所描述的bug。如果没有，DEEPPRIOR会将该报告归类为单一类别，不会影响到其他报告的优先级。<br>​我们构建的数据集是中文的。数据集的语言可能是一个威胁，但NLP和OCR技术相当强大。如果我们把文本处理引擎换成其它语言的引擎，文本处理也会很好地完成，不会对DEEPPRIOR产生负面影响。此外，机器翻译的成熟[16]也使其具备了处理跨语言文本信息的强大能力。</p><h2><span id="v-相关工作">V. 相关工作</span></h2><h3><span id="a-众包测试">A. 众包测试</span></h3><p>​众包测试已是一种主流的测试方法。它与传统测试有很大的不同。测试任务被分配给大量来自不同地点、具备不同专业能力的众包工人。众包测试最显著的优势是能够模拟不同的使用条件，经济成本相对较低[17][18]。然而，众包测试的开放性导致了大量的冗余报告。关键问题是如何提高开发人员审核测试报告的效率。一些研究从选择有技能的众包工人来完成任务[19][20][21]。这样的策略是有效的，但同时它仍然难以控制，因为即使是熟练的众包工人也会在任务中偷懒。因此，我们认为在众包测试中，更重要的是处理测试报告，而非其他因素。Liu等[22]和Yu[6]分别提出了由测试报告的截图自动生成描述的方法，这些方法都基于这样的一个共识，即对所有众包工人而言，应用程序的截图容易获得，但文本描述很难编写。这一想法启发了我们对截图的深度理解，以帮助更好地确定测试报告的优先级。</p><h3><span id="b-众包测试报告处理">B. 众包测试报告处理</span></h3><p>​为了更好地帮助开发人员审查报告和修复bug，当前已有许多研究来处理众包测试报告。基本策略包括报告分类、重复检测和报告优先级确定。在本节中，我们将介绍基于不同策略的相关工作。<br>​Banerjee等人提出了FactorLCS[23]，其使用常见的序列匹配，该方法在开放的bug跟踪库上是有效的。他们还提出了一种多标签分类器的方法[24]，以找到报告集群中具有高度相似性的“主要”报告。同样，Jiang等人提出了TERFUR[14]，这是一个使用NLP技术对测试报告进行聚类的工具，他们还过滤掉了低质量的报告。Wang等[25]将众包工人的特征作为测试报告的特征考虑进去，然后进行聚类。Wang等提出了LOAF[26]，这是第一个将操作步骤和结果描述分开处理的报告特征提取方法。<br>​更多的研究专注于检测重复测试报告。Sun等[27]采用信息检索模型，比较准确地检测重复的bug报告。Sureka等[28]采用基于字符n-gram的模型来完成重复检测任务。Prifti等[29]对大规模的开源项目测试报告进行了调查，提出了一种可以将重复报告的搜索集中在整个存储库的特定部分的方法。Sun等人提出了一个衡量相似度的检索函数REP[30]，并且该函数包括部件、版本等非文本字段的相似度。Nguyen等提出了DBTM [31]，该工具同时利用了基于IR的特征和基于主题的特征，并根据技术问题检测重复的bug报告。Alipour等[32]对测试报告上下文进行了较为全面的分析，提高了检测准确率。Hindle[33]通过结合上下文质量属性、架构术语和系统开发主题进行改进，提高了bug重复检测的能力。<br>​上述方法，包括报告分类和重复检测，都是选择部分测试报告来代表所有的测试报告。但是，我们认为，即使存在重复的报告，所有的报告都包含有价值的信息。而且，在检测到重复的报告后，开发人员仍需要对报告进行审查，以推进bug的处理。因此，我们认为报告优先级是一个更好的选择。<br>​当前也已有许多关于报告优先级的研究。Zhou等提出了BugSim[34]，其考虑了文本和统计特征以对测试报告进行排序。Tian等[35]提出的DRONE是一种基于机器学习的方法，其考虑测试报告的不同因素来预测测试报告的优先级。Feng等人提出了一系列方法，DivRisk[36]和BDDiv[3]，以对测试报告进行优先级排序，他们首先考虑了测试报告的截图。随后，Wang等[2]进一步研究并探索出了一种更加完善的测试报告优先级排序方法，并更提高了对截图的关注度。<br>​在以上的所有研究中，只有少数研究，如[2]和[3]，考虑了应用程序的截图，我们认为这是一个在提取特征来处理测试报告方面相当有价值的因素。但这些研究仅将截图作为简单的矩阵，而非有意义的内容。</p><h3><span id="c-深度图像理解">C. 深度图像理解</span></h3><p>​图像理解是计算机视觉（CV）领域的一个热点问题。本节主要介绍在软件测试中利用图像理解的研究。<br>​Lowe[10]提出了SIFT算法，其利用一系列新的图像局部特征，这些特征对于图像本身而言是恒定的，包括平移、缩放和旋转，以匹配目标图像上的特征点，并计算相似度。光学字符识别（Optical Character Recognition，OCR）是一种应用广泛的文字识别工具，它有助于根据图像上丰富的文字信息更好地理解图像。Nguyen等[37]提出了REMAUI，其使用CV技术来识别应用截图中的widget、文本、图像甚至容器。Moran等[38]在REMAUI的基础上提出了REDRAW，更精确地识别widget，并能自动生成应用UI的代码。同样，Chen等[39]也提出了一种结合CV技术和机器学习的工具，以根据应用截图生成GUI骨架。Yu等[1]提出了一种名为LIRAT的工具，可以在透彻了解应用截图的情况下跨平台记录和重新运行移动应用测试脚本。</p><h2><span id="vi-结论">VI. 结论</span></h2><p>​本文通过深度截图理解，提出了一种众包测试报告优先级排序方法DEEPPRIOR。DEEPPRIOR将应用截图和文本描述转化为四个不同的特征，包括问题widget、上下文widget、bug描述和复现步骤。然后，将特征汇总到DEEPFEATURE中，这些特征根据与bug的相关性，包括bug特征和上下文特征。之后，我们根据特征计算DEEPSIMILARITY。最后，根据DEEPSIMILARITY，按照预先设定的规则对报告进行优先级排序。我们还进行了一个实验来评估所提出的方法，结果显示，DEEPPRIOR的表现优于目前的最优方法，且开销不到它的一半。</p><h2><span id="感谢">感谢</span></h2><p>​本工作得到国家重点研发计划（2018AAA0102302）、国家自然科学基金（61802171、61772014、61690201）、中央高校基本科研基金（14380021）、国家大学生创新创业训练计划（202010284073Z）的部分支持。</p><h2><span id="参考文献">参考文献</span></h2><p>[1] S. Yu, C. Fang, Y. Feng, W. Zhao, and Z. Chen, “Lirat: Layout and image recognition driving automated mobile testing of cross-platform,” in 2019 34th IEEE&#x2F;ACM International Conference on Automated Software Engineering (ASE). IEEE, 2019, pp. 1066–1069. </p><p>[2] J. Wang, M. Li, S. Wang, T. Menzies, and Q. Wang, “Images don’t lie: Duplicate crowdtesting reports detection with screenshot information,” Information and Software Technology, vol. 110, pp. 139–155, 2019. </p><p>[3] Y. Feng, J. A. Jones, Z. Chen, and C. Fang, “Multi-objective test report prioritization using image understanding,” in 2016 31st IEEE&#x2F;ACM International Conference on Automated Software Engineering (ASE). IEEE, 2016, pp. 202–213. </p><p>[4] N. O’Mahony, S. Campbell, A. Carvalho, S. Harapanahalli, G. V. Hernandez, L. Krpalkova, D. Riordan, and J. Walsh, “Deep learning vs. traditional computer vision,” in Science and Information Conference. Springer, 2019, pp. 128–144. </p><p>[5] X. Xiao, X. Wang, Z. Cao, H. Wang, and P. Gao, “Iconintent: automatic identification of sensitive ui widgets based on icon classification for android apps,” in 2019 IEEE&#x2F;ACM 41st International Conference on Software Engineering (ICSE). IEEE, 2019, pp. 257–268. </p><p>[6] S. Yu, “Crowdsourced report generation via bug screenshot understanding,” in 2019 34th IEEE&#x2F;ACM International Conference on Automated Software Engineering (ASE). IEEE, 2019, pp. 1277–1279. </p><p>[7] Y. Kim, “Convolutional neural networks for sentence classification,” arXiv preprint arXiv:1408.5882, 2014. </p><p>[8] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, “Distributed representations of words and phrases and their compositionality,” in Advances in neural information processing systems, 2013, pp. 3111–3119. </p><p>[9] L. R. Rabiner, “A tutorial on hidden markov models and selected applications in speech recognition,” Proceedings of the IEEE, vol. 77, no. 2, pp. 257–286, 1989. </p><p>[10] D. G. Lowe et al., “Object recognition from local scale-invariant features.” in iccv, vol. 99, no. 2, 1999, pp. 1150–1157. </p><p>[11] M. Muja and D. G. Lowe, “Fast approximate nearest neighbors with automatic algorithm configuration.” VISAPP (1), vol. 2, no. 331-340, p. 2, 2009. </p><p>[12] D. F. Silva and G. E. Batista, “Speeding up all-pairwise dynamic time warping matrix calculation,” in Proceedings of the 2016 SIAM International Conference on Data Mining. SIAM, 2016, pp. 837–845. </p><p>[13] T. Y. Chen, F.-C. Kuo, R. G. Merkel, and T. Tse, “Adaptive random testing: The art of test case diversity,” Journal of Systems and Software, vol. 83, no. 1, pp. 60–66, 2010. </p><p>[14] B. Jiang, Z. Zhang, W. K. Chan, and T. Tse, “Adaptive random test case prioritization,” in 2009 IEEE&#x2F;ACM International Conference on Automated Software Engineering. IEEE, 2009, pp. 233–244. </p><p>[15] G. Rothermel, R. H. Untch, C. Chu, and M. J. Harrold, “Prioritizing test cases for regression testing,” IEEE Transactions on software engineering, vol. 27, no. 10, pp. 929–948, 2001. </p><p>[16] S. Karimi, F. Scholer, and A. Turpin, “Machine transliteration survey,” ACM Computing Surveys (CSUR), vol. 43, no. 3, pp. 1–46, 2011. </p><p>[17] R. Gao, Y. Wang, Y. Feng, Z. Chen, and W. E. Wong, “Successes, challenges, and rethinking–an industrial investigation on crowdsourced mobile application testing,” Empirical Software Engineering, vol. 24, no. 2, pp. 537–561, 2019. </p><p>[18] K. Mao, L. Capra, M. Harman, and Y. Jia, “A survey of the use of crowdsourcing in software engineering,” Journal of Systems and Software, vol. 126, pp. 57–84, 2017. </p><p>[19] Q. Cui, S. Wang, J. Wang, Y. Hu, Q. Wang, and M. Li, “Multi-objective crowd worker selection in crowdsourced testing.” </p><p>[20] Q. Cui, J. Wang, G. Yang, M. Xie, Q. Wang, and M. Li, “Who should be selected to perform a task in crowdsourced testing?” in 2017 IEEE 41st Annual Computer Software and Applications Conference (COMPSAC), vol. 1. IEEE, 2017, pp. 75–84. </p><p>[21] M. Xie, Q. Wang, G. Yang, and M. Li, “Cocoon: Crowdsourced testing quality maximization under context coverage constraint,” in 2017 IEEE 28th International Symposium on Software Reliability Engineering (ISSRE). IEEE, 2017, pp. 316–327. </p><p>[22] D. Liu, X. Zhang, Y. Feng, and J. A. Jones, “Generating descriptions for screenshots to assist crowdsourced testing,” in 2018 IEEE 25th International Conference on Software Analysis, Evolution and Reengineering (SANER). IEEE, 2018, pp. 492–496. </p><p>[23] S. Banerjee, B. Cukic, and D. Adjeroh, “Automated duplicate bug report classification using subsequence matching,” in 2012 IEEE 14th International Symposium on High-Assurance Systems Engineering. IEEE, 2012, pp. 74–81. </p><p>[24] S. Banerjee, Z. Syed, J. Helmick, and B. Cukic, “A fusion approach for classifying duplicate problem reports,” in 2013 IEEE 24th International Symposium on Software Reliability Engineering (ISSRE). IEEE, 2013, pp. 208–217. </p><p>[25] J. Wang, Q. Cui, Q. Wang, and S. Wang, “Towards effectively test report classification to assist crowdsourced testing,” in Proceedings of the 10th ACM&#x2F;IEEE International Symposium on Empirical Software Engineering and Measurement, 2016, pp. 1–10. </p><p>[26] J. Wang, S. Wang, Q. Cui, and Q. Wang, “Local-based active classification of test report to assist crowdsourced testing,” in Proceedings of the 31st IEEE&#x2F;ACM International Conference on Automated Software Engineering, 2016, pp. 190–201. </p><p>[27] C. Sun, D. Lo, X. Wang, J. Jiang, and S.-C. Khoo, “A discriminative model approach for accurate duplicate bug report retrieval,” in Proceedings of the 32nd ACM&#x2F;IEEE International Conference on Software Engineering-Volume 1, 2010, pp. 45–54. </p><p>[28] A. Sureka and P. Jalote, “Detecting duplicate bug report using character n-gram-based features,” in 2010 Asia Pacific Software Engineering Conference. IEEE, 2010, pp. 366–374. </p><p>[29] T. Prifti, S. Banerjee, and B. Cukic, “Detecting bug duplicate reports through local references,” in Proceedings of the 7th International Conference on Predictive Models in Software Engineering, 2011. </p><p>[30] C. Sun, D. Lo, S.-C. Khoo, and J. Jiang, “Towards more accurate retrieval of duplicate bug reports,” in 2011 26th IEEE&#x2F;ACM International Conference on Automated Software Engineering (ASE 2011). IEEE, 2011, pp. 253–262. </p><p>[31] A. T. Nguyen, T. T. Nguyen, T. N. Nguyen, D. Lo, and C. Sun, “Duplicate bug report detection with a combination of information retrieval and topic modeling,” in 2012 Proceedings of the 27th IEEE&#x2F;ACM International Conference on Automated Software Engineering. IEEE, 2012, pp. 70–79. </p><p>[32] A. Alipour, A. Hindle, and E. Stroulia, “A contextual approach towards more accurate duplicate bug report detection,” in 2013 10th Working Conference on Mining Software Repositories (MSR). IEEE, 2013, pp. 183–192. </p><p>[33] A. Hindle, A. Alipour, and E. Stroulia, “A contextual approach towards more accurate duplicate bug report detection and ranking,” Empirical Software Engineering, vol. 21, no. 2, pp. 368–410, 2016. </p><p>[34] J. Zhou and H. Zhang, “Learning to rank duplicate bug reports,” in Proceedings of the 21st ACM international conference on Information and knowledge management, 2012, pp. 852–861. </p><p>[35] Y. Tian, D. Lo, and C. Sun, “Drone: Predicting priority of reported bugs by multi-factor analysis,” in 2013 IEEE International Conference on Software Maintenance. IEEE, 2013, pp. 200–209. </p><p>[36] Y. Feng, Z. Chen, J. A. Jones, C. Fang, and B. Xu, “Test report prioritization to assist crowdsourced testing,” in Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering, 2015, pp. 225–236. </p><p>[37] T. A. Nguyen and C. Csallner, “Reverse engineering mobile application user interfaces with remaui (t),” in IEEE&#x2F;ACM International Conference on Automated Software Engineering, 2016. </p><p>[38] K. Moran, C. Bernal-Cardenas, M. Curcio, R. Bonett, and D. Poshy- ´ vanyk, “Machine learning-based prototyping of graphical user interfaces for mobile apps,” arXiv preprint arXiv:1802.02312, 2018. </p><p>[39] C. Chen, T. Su, G. Meng, Z. Xing, and Y. Liu, “From ui design image to gui skeleton: a neural machine translator to bootstrap mobile gui implementation,” in Proceedings of the 40th International Conference on Software Engineering. ACM, 2018, pp. 665–676.</p>]]></content>
    
    
    <summary type="html">&lt;img src=&quot;/2021/04/20/Prioritize%20Crowdsourced%20Test%20Reports%20via%20Deep%20Screenshot%20Understanding/title.png&quot; alt&gt;

&lt;h1 id=&quot;基于深度截图理解的众包测试报告优先级排序&quot;&gt;&lt;a href=&quot;#基于深度截图理解的众包测试报告优先级排序&quot; class=&quot;headerlink&quot; title=&quot;基于深度截图理解的众包测试报告优先级排序&quot;&gt;&lt;/a&gt;基于深度截图理解的众包测试报告优先级排序&lt;/h1&gt;&lt;h2 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要&quot;&gt;&lt;/a&gt;摘要&lt;/h2&gt;&lt;p&gt;​		众包测试在移动应用程序测试中日益占据主导地位，但对于应用开发者来说，审查数量过多的测试报告是很大的负担。已有许多学者提出基于文本和简单图片特征的测试报告处理方法。然而，在移动应用测试中，测试报告所包含的文本较为精简且信息不够充分，图片则能够提供更丰富的信息。这一趋势促使我们在深度截图理解的基础上，对众包测试报告的优先级进行排序。&lt;br&gt;​		本文中，我们提出了一种新的众包测试报告优先级排序方法，即DEEPPRIOR。我们首先引入一个新的特征来代表众包测试报告，即DEEPFEATURE，它基于对应用程序截图的深度分析，涵盖了所有组件（widget）及它们的文本、坐标、类型甚至是意图。DEEPFEATURE包括直接描述bug的bug特征（Bug Feature），和刻画bug完整上下文的上下文特征（Context Feature）。DEEPFEATURE的相似度用于表示测试报告的相似度，并被用于对众包测试报告进行优先级排序。我们形式上将相似度定义为DEEPSIMILARITY。我们还进行了一个实证实验，以评估所提技术在大型数据中的有效性。结果表明，DEEPPRIOR性能最佳，以不足一半的成本获得优于其它方法的结果。&lt;/p&gt;
&lt;p&gt;索引词 众包测试，移动应用测试，深度截图理解&lt;/p&gt;</summary>
    
    
    
    <category term="论文阅读" scheme="https://values.keys.moe/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="论文阅读" scheme="https://values.keys.moe/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
  </entry>
  
  <entry>
    <title>Syntactic Data Augmentation Increases Robustness to Inference Heuristics</title>
    <link href="https://values.keys.moe/2020/09/16/Syntactic%20Data%20Augmentation%20Increases%20Robustness%20to%20Inference%20Heuristics/"/>
    <id>https://values.keys.moe/2020/09/16/Syntactic%20Data%20Augmentation%20Increases%20Robustness%20to%20Inference%20Heuristics/</id>
    <published>2020-09-16T08:52:33.000Z</published>
    <updated>2022-10-05T09:40:33.660Z</updated>
    
    <content type="html"><![CDATA[<img src="/2020/09/16/Syntactic%20Data%20Augmentation%20Increases%20Robustness%20to%20Inference%20Heuristics/title.png" alt><h1><span id="通过语法数据扩增提升推理启发法的鲁棒性">通过语法数据扩增提升推理启发法的鲁棒性</span></h1><h2><span id="摘要">摘要</span></h2><p>​诸如BERT的预训练的神经模型在微调以执行自然语言推理（NLI）时，常常在标准数据集上展现出了高度准确性，但在受控的挑战集上，它们却表现出对语序敏感度的出奇缺乏。我们假设此问题并不主要因为预训练模型的局限性引起，而是由于缺乏众包的NLI样例引起的，而这些样例可能在微调阶段传递了语法结构的重要性。我们探索了几种方法来扩增标准训练集中语法丰富的实例，这些实例是通过对MNLI语料库的句子应用语法转换而生成的。而表现最好的扩增方法，主语&#x2F;宾语倒置法，可以在不影响BERT对MNLI测试集性能的情况下，将BERT对受控实例的词序敏感度诊断从0.28提升至0.73。这种改进全面超过了用于数据扩增的特定结构，这表明了扩增可以使BERT学习到抽象语法的表现形式。</p><span id="more"></span><h2><span id="1-介绍">1. 介绍</span></h2><p>​在NLP里常见的监督学习范例中，特定分类任务的大量标记实例被随机地分为训练集和测试集。系统在训练集上进行训练，然后在测试集上进行评估。神经网络，尤其是对单词预测对象的进行预训练的系统，如ELMo(Peters et al.,2018)或BERT(Devlin et al.,2019)——在这种范例中表现出色：在具有足够大的预训练语料库的情况下，这些模型在许多测试集上所表现出的准确性达到甚至超过了未经训练的人类标注者(Raffel et al.,2019)。</p><p>​同时，越来越多的证据表明，从与训练集相同的范围中提取的测试集上的高精度并不表示模型已经掌握了该任务。当模型应用于表示相同任务的不同数据集中，这种差异可能表现为准确性的急剧下降(Talmor and Berant, 2019;Yogatama et al., 2019)，或者表现为对输入语言无关扰动的过度敏感(Jia and Liang, 2017; Wallace et al., 2019)。</p><p>​在自然语言推理（NLI）任务中，McCoy等人(2019b)记录了这样的一种差异，即模型在标准测试集上的出色性能并不对应表明它能像人类定义的那样精通于此任务。在这个任务中，系统将获得两个句子，其被期望确定一个句子（前提）是否蕴含另一个句子（假设）。即使不是所有人，大多数人也都会同意NLI需要对语法结构敏感。例如，以下句子即使包含了相同的单词，但它们并不相互蕴含：</p><p>​(1)  演员看到了律师 (The lawyer saw the actor.)</p><p>​(2)  律师看到了演员 (The actor saw the lawyer.)</p><p>​McCoy等人构造了HANS挑战集，其包含了一系列此类构造的例子，并且其被用来表明，当BERT在MNLI语料库进行微调时，该微调模型在从该语料库提取的测试集上取得了较高的准确率，但其对语法几乎没有敏感性；该模型会错误地得出结论，如（1）蕴含（2）。</p><p>​我们考虑用两种解释来说明为什么在MNLI上微调的BERT会在HANS上失败。在代表性不足假设下，BERT在HANS上失败，是因为它的预训练表现形式缺失了一些必要的语法信息。而在缺失连接的假设下，BERT从输入中提取相关语法信息(参见 Goldberg 2019;Tenney et al. 2019)，但是它无法在HANS上使用这个信息，因为很少有MNLI训练实例可以表明语法应该如何支持NLI的(McCoy et al., 2019b)。这两种假设都有可能是正确的：部分语法方面BERT可能根本未学习到，还有部分方面已经学过了，但并没有应用被用于进行推理。</p><p>​缺失连接假设预测，从一个语法结构中使用少量的实例进行训练集的扩增将使BERT知道任务需要它使用它的语法表现形式。这不仅将使得用于数据扩增的结构的改进，并且也可推广到其他结构上。相反，代表性不足假设预测，模型想要在HANS上具有更好的表现，BERT必须从头开始学习每种语法结构是如何影响NLI的。这预计需要有更大的数据扩增集来获得足够的性能，并且整个结构几乎不能泛化。</p><p>​本文旨在验证这些假设。我们通过对MNLI的少量实例进行语法转换以构造扩增集。尽管在MNLI上只扩增了约400个主语与宾语互换的实例（大约是MNLI训练集大小的0.1%），但模型对语法上具有挑战性的案例的准确率得到了显著的提高。更关键的是，即使在扩增中仅使用了一个单一的变换，但在一系列的结构中，准确度都得到了提高。例如，BERT在涉及关系从句的实例（例如，演员给游客看到的银行家打电话(The actors called the banker who the tourists saw) 无法推导出银行家打电话给游客(The banker called the tourists)）在未扩增的实例中准确性为0.33，而扩增后为0.83。这表明我们的方法并不会过度适应于一个结构，而是利用了BERT现有的语法表示形式，从而为缺失连接假设提供了支持。同时，我们还观察到了泛化的局限性，在这些情况下，它们支持了代表性不足的假设。</p><h2><span id="2-背景">2. 背景</span></h2><p>​HANS是一个模板生成的挑战集，旨在测试NLI模型是否采用了三种语法启发法。首先是词汇重叠启发法，其假定所有时间内所有在假设中的单词也都在前提中，且标签需是蕴含标签。在MNLI训练集中，这种启发法通常会做出正确的预测，几乎从不做出错误的预测。这可能是由于MNLI生成的过程所致：众包工作者被给予一个前提，并被要求生成与该前提相矛盾，或是蕴含这个前提的句子。为了最大程度地减少工作量，工作人员可能过度地使用了词汇重叠，将其作为一种生成含义假设的捷径。当然，词汇重叠启发法并不是一种普遍有效的推理策略，并且在许多HANS实例中都是失败的。例如上文所述，律师看到了演员(the lawyer saw the actor)，并不意味着演员看到了律师(the actor saw the lawyer)。</p><p>​HANS还包括诊断子序列启发法（假定前提蕴含任何与其相邻的子序列的假设）和成分启发法（假设前提蕴含其自身所有构成要素）的实例情形。当我们专注于对抗词汇重叠启发法时，我们还将测试其他启发法的泛化情况，这可以看作是词汇重叠中特别具有挑战性的案例。表A.5，A.6，A.7给出了用于诊断这三种启发法的所有结构的实例。</p><p>​数据扩增通常用于增强视觉的鲁棒性(Perez and Wang, 2017)与语言的鲁棒性(Belinkov and Bisk, 2018; Wei and Zou, 2019)，包括了NLI (Minervini and Riedel, 2018; Yanaka et al., 2019)。在许多情况下，使用一种实例进行扩增可以提高特定情况下的准确性，但不能泛化到其他情况，这表明模型过拟合于扩增集(Jia and Liang, 2017; Ribeiro et al., 2018; Iyyer et al., 2018; Liu et al., 2019)。特别的，McCoy等人(2019b)发现，HANS的实例的扩增可以泛化推广到不同的单词重叠挑战集(Dasgupta et al., 2018)，但这仅适用于长度与HANS实例相似的实例。我们通过生成各种基于语料库的实例来减轻对表面属性的过度拟合，这些实例与挑战集上的词法与语法均不同。最后，Kim等人(2018)使用了与我们相似的数据扩增方法，但没有研究对不在扩增集中的实例类型的泛化。</p><h2><span id="3-生成扩增数据">3. 生成扩增数据</span></h2><p>​我们使用两种语法转换从MNLI生成扩增实例：倒置INVERSION（互换原句的主语与宾语）和被动化PASSIVIZATION。对于每个转换，我们都有两个系列的扩增集。原始前提(ORIGINAL PREMISE)策略保留了原有的MNLI前提，并对假设进行了转换；转换假设(TRANSFORMED HYPOTHESIS)使用原始MNLI假设作为新前提，转换后的假设作为新假设（实例见表1，具体请参见§A.2）。我们尝试了三种扩增集的大小：小型（101个实例），中型（405个实例），大型（1215个实例）。所有的扩增集都比MNLI训练集（297k）小得多。</p><img src="/2020/09/16/Syntactic%20Data%20Augmentation%20Increases%20Robustness%20to%20Inference%20Heuristics/表1.png" alt><p>​我们没有试图确保生成实例的自然性；例如，在倒置转换中，车厢造成了大量噪音(The carriage made a lot of noise)被转换成大量噪音造成了车厢(A lot of noise made the carriage)。此外，扩增数据集的标签存在一些噪音；例如，我们假设倒置将正确的标签从蕴含改为中性，但是也并非必然如此（如果买方遇到卖方(The buyer met the seller)，那么卖方遇到买方(The seller met the buyer)是有可能的）。如下所示，这种噪声不会损害MNLI的准确性。</p><p>​最后，我们包括一个随机的打乱条件，其中MNLI前提及其假设都被随机打乱。我们使用这个情况来测试语法上不知情的方法是否能教会这个模型：当忽略单词顺序时，就无法做出可靠的推论。</p><h2><span id="4-试验设置">4. 试验设置</span></h2><p>​我们将每个扩增集分别添加到MNLI的训练集中，并对每个生成的训练集进行微调BERT的训练。微调的更多细节在附录A.1中。我们为扩增策略与扩增集大小的每种组合重复了五个随机种子的过程，但最成功的策略（倒置+转换假设（INVERSION+TRANSFORMED HYPOTHESIS））除外。且对于每个扩增的范围，均进行了15次运行。参照McCoy等人(2019b)，在对HANS进行评估时，我们将模型产生的中性与矛盾标签合并为一个单一的非蕴含(non-entailment)标签。</p><p>​对于原始前提(ORIGINAL PREMISE)与转换假设(TRANSFORMED HYPOTHESIS)，我们尝试了分别使用每一种转换，并使用了包含倒置与被动化的数据集进行了实验。我们还分别对仅使用带有蕴含标签的被动化例子和仅使用带有非蕴含标签的被动化例子进行了单独的实验。作为基线，我们使用了100次在未进行数据扩增的MNLI上训练出的微调的BERT模型(McCoy et al., 2019a)。</p><p>​我们会报告模型在HANS上的准确性和在MNLI的开发集上的准确性（MNLI测试集的标签不公开）。我们没有调整这个开发集的任何参数。我们下面讨论的所有比较都在p&lt;0.01的水平上，比较结果都是十分显著的（基于双向t检验）。</p><h2><span id="5-结果">5. 结果</span></h2><p>​MNLI的准确性在不同的扩增策略中都非常相似，并且与未经扩增的基线(0.84)相匹配，这表明最多有1215个实例的语法扩增不会损害数据集的整体表现。相比之下，HANS的准确度差异很大，大多数模型在非蕴含的实例中的表现得比置信准确度差（在HANS上为0.5），这表明了它们采用了启发法（图1）。很大程度上，最有效的扩增策略是倒置结合转换假设。HANS在单词重叠案例（其中正确的标签都是非蕴含的，例如：医生看了律师(the doctor saw the lawyer)↛律师看了医生(the lawyer saw the doctor)）的准确度在没有数据扩增的情况下为0.28，在大型扩增集上为0.73。同时，在启发法做出正确预测的情况下（如演员旁的游客们给作家们打电话(The tourists by the actor called the authors) → 游客们给作家们打电话(The tourists called the authors)），这种策略降低了BERT的准确性；实际上，在词汇重叠做出正确与不正确的预测情况下，最佳模型的准确度都是相似的，这表明了这种干预阻止了模型采用启发法。</p><img src="/2020/09/16/Syntactic%20Data%20Augmentation%20Increases%20Robustness%20to%20Inference%20Heuristics/图1.png" alt><p>​随机打乱的方法并未使模型在未经扩增的基线上得到了改善，表明关注语法的转换是必要的(表A.2)。被动化比倒置的收益要小得多，这可能是由于存在显式的标记引起的（如单词by），这可能导致模型仅在这些单词出现时才考虑词序。有趣的是，即使是在HANS的被动实例中，倒置也仍比被动化更有效（大型倒置扩增：0.13；大型被动化扩增：0.01）。最后，自身倒置比倒置与被动化的结合更有效。</p><p>​现在，我们来更详细地分析最有效的策略，即倒置结合转换假设。首先，该策略在抽象层面上与HANS的主语&#x2F;宾语交换类别相似，但是两者的词汇与语法特性均有不同。尽管存在这些差异，但模型在HANS类别上的表现在中型与大型扩增条件下都是完美的（1.00），这表明BERT能从转换的高级语法结构中受益。对于小的扩增集，此类别的准确性为0.53，表明有101个实例不足以使BERT知道不能随意的交换主语与宾语的对象。相反，将扩增大小从中型变至大型，能在HANS的子案例中产生适度且易变的效果（见附录A.3了解具体个案的结果）；为了更清楚的了解扩增大小的影响，可能还需要对该参数进行更密集的采样。</p><img src="/2020/09/16/Syntactic%20Data%20Augmentation%20Increases%20Robustness%20to%20Inference%20Heuristics/图2.png" alt><p>​尽管倒置是该扩充集中的唯一转换，但是除了主语&#x2F;宾语交换之外，其他结构的性能也得到了显著的提高（图2）；例如，模型能够更好地处理包含介词短语的实例，例如，经理背后的法官看医生(The judge behind the manager saw the doctors)并不蕴含医生看经理(The doctors saw the manager)，（未扩增：0.41；大型扩增：0.89）。在以子序列启发法方法为目标的案例中，有一个更平缓，但仍然十分明显的提升；这种较小程度的提升表明，对连续子序列从词汇重叠中分离处理更具泛化性。一个例外是对“NP&#x2F;S”推论的准确性，例如经理们听说秘书辞职了(the managers heard the secretary resigned) ↛经理们听见了秘书说话(The managers heard the secretary)，这一准确度从0.02（未扩增）大幅提升至0.5（大型扩增）。因此，对子序列案例的进一步改进可能需要涉及子序列的数据扩增。</p><p>​在过去的一年中，人们提出了一系列技术来提高HANS的性能。这些模型包括语法感知模型(Moradshahi et al., 2019; Pang et al., 2019)，旨在捕获预定义浅层启发法的辅助模型，以使主模型可以专注于稳健策略(Clark et al., 2019; He et al., 2019; Mahabadi and Henderson, 2019)以及提高难度训练实例权重的方法(Yaghoobzadeh et al., 2019)。尽管其中的一些方法在HANS上比我们的方法具有更高的准确性，包括更好的泛化了成分和子序列的情况（参见表A.4），但它们并不具有直接的可比性：我们的目标是在不修改模型或训练程序的情况下，评估训练集中的具有语法挑战性的实例是如何影响BERT的NLI的表现的。</p><h2><span id="6-讨论">6. 讨论</span></h2><p>​我们最佳效果的策略涉及通过对MNLI实例的主语&#x2F;宾语倒置转换而生成的少量MNLI实例来扩增MNLI训练集。这产生了可观的泛化能力：既是对另一个域而言的（HANS挑战集），更重要的是，其也适用于其他结构，如关系从句和介词短语。这支持了缺失连接假设：对一个结构进行少量扩增会引起抽象的语法敏感性，而不是仅仅通过为模型建立来自同一分布的案例样本来“接种(inoculating)”模型，以防止在挑战集上失败(Liu et al., 2019)。</p><p>​同时，倒置转换并未完全抵消启发法，特别是，这些模型在被动句上的表现较差。因此，对于这些结构，BERT的预训练可能在通过一个较小的扩增后，也仍无法产生强有力的语法表现形式；换句话说，这可能是我们的代表性不足假设成立的情况。该假设预测，作为单词预测模型的预训练BERT对于被动词处理困难，并且可能需要专门针对NLI任务学习其对应结构的特性；这可能需要大量的扩增实例。</p><p>​表现最佳的扩增策略是从一个单独原句生成前提&#x2F;假设对，这意味着该策略不依赖于NLI语料库。我们可以从任何语料库生成扩增实例，这使得我们有可能测试非常大的扩增集是否有效（当然，请注意，来自不同领域的扩增语句可能会影响在MNLI上本身的表现）。</p><p>​最终，我们希望有一个能在跨语言理解任务中也能在使用语法方面具有强归纳偏差的模型，即使在重叠启发法导致其在训练集上的高精度时也是如此。事实上，很难想象人们在理解一个句子时会完全忽略语法。另一种可选的方法是创建足以代表各种语言现象的训练集；众包工作者（理性的人）偏爱使用尽可能简单的生成策略，这可以通过对抗性过滤等方法来抵消(Nie et al., 2019)。然而，在此期间，我们得出结论，数据扩增是一个简单而有效的策略，其可以缓解BERT等模型中已知的推理启发法。</p><h2><span id="a-附录">A. 附录</span></h2><h3><span id="a1-微调细节">A.1 微调细节</span></h3><p>​我们在所有实验中都使用了bert-base-uncased版本的模型。按照标准，我们通过在MNLI上训练线性分类器从CLS标记的最终层嵌入预测标签，同时继续更新BERT参数，来对该预训练模型进行微调(Devlin et al., 2019)。每个模型的训练实例顺序都进行了打乱。所有模型都经过了三个epoch的训练。</p><h3><span id="a2-生成扩增实例">A.2 生成扩增实例</span></h3><p>​以下列表描述了我们使用的扩增策略。表A.1说明了应用于特定原句的所有策略。注意，倒置通常会改变句子的含义（侦探跟随着嫌疑犯(the detective followed the suspect)与嫌疑犯跟随着侦探(the suspect followed the detective)描述的并不是一个场景），但是被动语句并不会（侦探跟随着嫌疑犯(the detective followed the suspect)与嫌疑犯被侦探跟随(the suspect was followed by the detective)描述的是同一个场景）。</p><ul><li><p>倒置（原始前提）：对于原实例(p,h, →)，生成(p,INV(h), ↛)，其中INV返回主语与宾语调换了的原句，忽略原实例中标签为↛的实例。</p></li><li><p>倒置（转换假设）：对于原实例(p,h) （带有任意标签），丢弃前提p并且生成(h,INV(h), ↛)</p></li><li><p>被动（原始前提）：对于原实例(p,h) （带有任意标签），生成(p,PASS(h))，保持标签不变，其中PASS返回原句的被动语态版本（在不改变原意的情况下）</p></li><li><p>被动（转换假设）：对于原实例(p,h)，丢弃前提p，并且生成两个实例，一个携带蕴含标签——(h,PASS(h), →)，一个携带非蕴含标签——(h,PASS(h), ↛)</p></li></ul><p>​我们使用MNLI提供的选区分析，识别出MNLI中可以作为原句的及物句，但噪声较大的TELEPHONE类型除外。为此，我们搜索了恰好带有VP的一个NP子节点的矩阵S节点，其中主宾都是完整的名词短语（即，都不是像me这样的人称代词），而动词词缀不是be或have。我们保留了动词的原始时态，并在必要时修改了他们的一致性特征。（如：电影是Matt Dillon和Gary Sinise 出演(the movie stars Matt Dillon and Gary Sinise)改为Matt Dillon和Gary Sinise 出演了这部电影(Matt Dillon and Gary Sinise star the movie)）。</p><p>​所有策略中，最大的扩增集大小为1215。这个大小是我们可以从MNLI生成的最大扩增数据集确定的，该数据集是使用了上述的倒置结合原始前提方法。为了公平比较，即使对于可以生成更大数据集的策略，我们仍保持相同的大小。我们还通过随机抽样405个使用上述过程识别的案例创造了中型数据集，以及包含了101个实例的小数据集。我们对于每种策略仅执行一次这个过程：因此，运行仅在分类器的权重初始化和实例顺序方面有所不同，而在训练中包含的扩增实例没有变化。</p><p>​为了创建组合的扩增数据集，我们将倒置和被动化的数据集进行串联，然后随即丢掉一半实例（以使组合数据集的大小与其他数据集相匹配）。与其他数据集一样，我们只执行了一次：合并的扩增集在每次运行中相同。该过程的一个结果是，倒置和被动化的实例数量不完全相同。</p><h3><span id="a3-详细结果">A.3 详细结果</span></h3><p>​下列图表提供了我们试验的详细结果。</p><p>​表A.2显示了每种策略在MNLI上的平均准确率，以及在诊断三种启发法（词汇重叠启发法，子序列启发法和成分启发法）中的每一种的HANS案例的平均准确率，正确的标签是非蕴含（↛）。表A.3探究了最佳扩增策略—具有转换假设的主宾倒置，包括了在正确的标签是蕴含（→）和非蕴含（↛）两种情况。</p><p>​最后，末尾三个表格详细说明了通过倒置结合转换假设来对30个HANS子案例进行扩增的效果，并按设计按照诊断启发法来细分成表：词汇重叠启发法（表A.5）；子序列启发法（表A.6）和成分启发法（表A.7）。</p><img src="/2020/09/16/Syntactic%20Data%20Augmentation%20Increases%20Robustness%20to%20Inference%20Heuristics/表A.1.png" alt><p>表A.1：语法扩充策略（完整表格）</p><img src="/2020/09/16/Syntactic%20Data%20Augmentation%20Increases%20Robustness%20to%20Inference%20Heuristics/表A.2.png" alt><img src="/2020/09/16/Syntactic%20Data%20Augmentation%20Increases%20Robustness%20to%20Inference%20Heuristics/表A.3.png" alt><p>表A.3：使用主语&#x2F;宾语倒置结合转换假设的数据扩增对HANS准确性的影响。图表展示了在使用三种扩增集大小（101，405，1215个实例）来扩增的MNLI训练集后BERT的微调结果，也展示了在未经扩增的MNLI训练集上BERT的微调结果。</p><img src="/2020/09/16/Syntactic%20Data%20Augmentation%20Increases%20Robustness%20to%20Inference%20Heuristics/表A.4.png" alt><p>表A.4：不同的架构与训练方式所得到的HANS准确率，其被按照实例诊断的启发法与实例明确的标签拆分开来。除了MT-DNN+LF外，其余均采用了BERT作为基本模型。L，S与C分别代表词汇重叠、子序列和成分启发法。扩增集的大小为n&#x3D;101（小型），n&#x3D;405（中型），n&#x3D;1215（大型）。</p><img src="/2020/09/16/Syntactic%20Data%20Augmentation%20Increases%20Robustness%20to%20Inference%20Heuristics/表A.5.png" alt><p>表A.5：主语&#x2F;宾语倒置结合转换假设：图表展示了HANS子案例在诊断为词法重叠启发法时的结果，这些案例包括四种训练方案-未经扩增（只在MNLI上进行训练），和小型（n&#x3D;101），中型（n&#x3D;405），大型（n&#x3D;1215）扩增集的情况。置信准确度为0.5。图表上半部分：标签是非蕴含的案例情况。图表下半部分：标签是蕴含的案例情况。</p><img src="/2020/09/16/Syntactic%20Data%20Augmentation%20Increases%20Robustness%20to%20Inference%20Heuristics/表A.6.png" alt><p>表A.6：主语&#x2F;宾语倒置结合转换假设：图表展示了HANS子案例在诊断为子序列启发法时的结果，这些案例包括四种训练方案-未经扩增（只在MNLI上进行训练），和小型（n&#x3D;101），中型（n&#x3D;405），大型（n&#x3D;1215）扩增集的情况。图表上半部分：标签是非蕴含的案例情况。图表下半部分：标签是蕴含的案例情况。</p><img src="/2020/09/16/Syntactic%20Data%20Augmentation%20Increases%20Robustness%20to%20Inference%20Heuristics/表A.7.png" alt><p>表A.7：主语&#x2F;宾语倒置结合转换假设：图表展示了HANS子案例在诊断为成分启发法时的结果，这些案例包括四种训练方案-未经扩增（只在MNLI上进行训练），和小型（n&#x3D;101），中型（n&#x3D;405），大型（n&#x3D;1215）扩增集的情况。置信准确度为0.5。图表上半部分：标签是非蕴含的案例情况。图表下半部分：标签是蕴含的案例情况。</p>]]></content>
    
    
    <summary type="html">&lt;img src=&quot;/2020/09/16/Syntactic%20Data%20Augmentation%20Increases%20Robustness%20to%20Inference%20Heuristics/title.png&quot; alt&gt;

&lt;h1 id=&quot;通过语法数据扩增提升推理启发法的鲁棒性&quot;&gt;&lt;a href=&quot;#通过语法数据扩增提升推理启发法的鲁棒性&quot; class=&quot;headerlink&quot; title=&quot;通过语法数据扩增提升推理启发法的鲁棒性&quot;&gt;&lt;/a&gt;通过语法数据扩增提升推理启发法的鲁棒性&lt;/h1&gt;&lt;h2 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要&quot;&gt;&lt;/a&gt;摘要&lt;/h2&gt;&lt;p&gt;​	诸如BERT的预训练的神经模型在微调以执行自然语言推理（NLI）时，常常在标准数据集上展现出了高度准确性，但在受控的挑战集上，它们却表现出对语序敏感度的出奇缺乏。我们假设此问题并不主要因为预训练模型的局限性引起，而是由于缺乏众包的NLI样例引起的，而这些样例可能在微调阶段传递了语法结构的重要性。我们探索了几种方法来扩增标准训练集中语法丰富的实例，这些实例是通过对MNLI语料库的句子应用语法转换而生成的。而表现最好的扩增方法，主语&amp;#x2F;宾语倒置法，可以在不影响BERT对MNLI测试集性能的情况下，将BERT对受控实例的词序敏感度诊断从0.28提升至0.73。这种改进全面超过了用于数据扩增的特定结构，这表明了扩增可以使BERT学习到抽象语法的表现形式。&lt;/p&gt;</summary>
    
    
    
    <category term="论文阅读" scheme="https://values.keys.moe/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="论文阅读" scheme="https://values.keys.moe/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
  </entry>
  
  <entry>
    <title>Adversarial Robustness through Disentangled Representations</title>
    <link href="https://values.keys.moe/2020/06/19/Adversarial%20Robustness%20through%20Disentangled%20Representations/"/>
    <id>https://values.keys.moe/2020/06/19/Adversarial%20Robustness%20through%20Disentangled%20Representations/</id>
    <published>2020-06-18T16:38:38.000Z</published>
    <updated>2022-10-05T10:07:04.695Z</updated>
    
    <content type="html"><![CDATA[<img src="/2020/06/19/Adversarial%20Robustness%20through%20Disentangled%20Representations/封面.png" alt><h1><span id="通过解耦表征实现对抗鲁棒性">通过解耦表征实现对抗鲁棒性</span></h1><h2><span id="摘要">摘要</span></h2><p>​尽管深度学习模型具有优秀的实证性能，但诸多研究已发现了它们对对抗样本的脆弱性。它们易对具有难以察觉的对抗扰动的输入做出易受其影响的预测。尽管最近的研究已明显提升了在对抗训练策略下模型的鲁棒性，但自然准确性与对抗鲁棒性之间的差距依旧显著。为缓解此问题，本文中，我们假设鲁棒性与非鲁棒性表征是整体表征中相互耦合的两个基本成分。为实现对抗鲁棒性，自然与对抗样本的鲁棒表征应与非鲁棒部分分离，鲁棒表征的一致化可弥补准确性与鲁棒性之间的差距。受此启发，我们提出了一种称为深度鲁棒解耦表征网络（Deep Robust Representation Disentanglement Network, DRRDN）的新防御方法。具体而言，DRRDN使用解耦器从对抗样本与自然样本中提取并一致化鲁棒表征。理论分析保证了我们的方法能在具有良好解耦与一致化性能的情况下平衡鲁棒性与准确性。在基准数据集上的实验结果证明了我们方法的实证优势。</p><span id="more"></span><h2><span id="1-介绍">1. 介绍</span></h2><p>​我们提出了一种新的防御方法，称为深度鲁棒解耦表征网络（DRRDN）。具体而言，DRRDN遵循自动编码器（Auto-Encoder, AE）框架，通过编码器提取隐藏表征，使用解码器恢复原始输入。与典型的AE不同，DRRDN独特地使用两个解耦器将隐藏表征解耦为两个分支，一个是为了提取类特定的表征以进行分类，另一个是提取与类型无关的表征，以确定此表征是来自自然域还是对抗域。为了获得更好的解耦性能，我们使用互信息最小化以进一步正则化解耦器。与传统的AE类似，为保证一致性，我们亦使用了一个重构器以恢复解耦之前的整体表征。在推理阶段，我们使用特定类的表征进行预测。因此，无论输入的样本来源于哪个域，用于分类而提取的表征都能对对抗攻击有始终与一的鲁棒性。此外，我们从理论上证明，在满足解耦和一致化性能的前提下，自然表征与对抗表征之间的分布差异较小，所以自然准确度和对抗准确度应更接近。最后，我们在MNIST和CIFAR10数据集上实证了DRRDN在各种对抗攻击下的有效性。本工作主要贡献如下：</p><ul><li><p>我们提出了一种新的对抗防御方法DRRDN，它通过解耦与一致化来源于自然与对抗样本的鲁棒表征来消除对抗扰动的影响。</p></li><li><p>我们所提出的DRRDN是模型无关的，因此其可适用于任何模型架构。此外，DRRDN利用扰动的信息以实现完整的解耦能力。</p></li><li><p>我们通过理论分析，证明了我们方法的自然准确度和对抗鲁棒性之间的差距更小。</p></li><li><p>基准数据集的实验结果证明了DRRDN的优越性。</p></li></ul><h2><span id="2-方法">2. 方法</span></h2><p>​我们提出的DRRDN方法是在对抗训练的框架下更新的。为简单起见，我们在训练阶段使用PGD方法来生成对抗样本。本节先对对抗训练与PGD攻击流程做简单回顾。</p><h3><span id="准备工作">准备工作</span></h3><p>​考虑一个深度神经网络f(·)&#x3D;c(g(·; θ);w)∈Rk，其中g(·; θ)是以θ为参数的表征编码器，c(·;w)是以w为参数的分类器，k是分类的类型数量。对抗样本x’∈R HxWxC是由自然样本x∈R HxWxC通过添加对抗扰动δ生成的，而该扰动被限定在一个半径为ε的小球中，即：</p><img src="/2020/06/19/Adversarial%20Robustness%20through%20Disentangled%20Representations/公式0.png" alt><p>​除此之外，x’应导致网络的预测发生改变。对抗训练的基本思想是交替地生成对抗样本与使用它们训练。总目标如下：</p><img src="/2020/06/19/Adversarial%20Robustness%20through%20Disentangled%20Representations/公式1.png" alt><p>​然而，对于复杂的高维数据，内部最大化通常难以实现。因此，PGD使用以下T步投影来近似生成ε球内的最强对抗样本：</p><img src="/2020/06/19/Adversarial%20Robustness%20through%20Disentangled%20Representations/公式2.png" alt><p>​其中，Proj Bεp (x)(·)表示投影操作，其将扰动样本投影至ε半径的球Bεp (x)中，以确保对抗样本的相似度。t∈[1,T]，是投影步数的数量，α是单步幅度，sign(·)返回梯度中每个元素的符号。注意，如果总的更新步数T为1，则PGD方法就会退化为较弱的FGSM攻击。在通过公式(2)得到x’后，把它喂给模型，像公式(1)那样优化外部最小化，以加强模型鲁棒性。</p><h3><span id="深度鲁棒解耦表征网络drrdn">深度鲁棒解耦表征网络（DRRDN）</span></h3><p>​DRRDN的基础理论假设整体表征z&#x3D;g(x;θ)∼pθ(z)（相似的，z’&#x3D;g(x’;θ)∼pθ(z’)）由两个耦合的分支组成，分别为特定类表征zs与类型无关表征zi，其中pθ(z)是原始数据分布p(x)的前推度量。</p><p>​具体而言，纯zs被指定为分类任务，因此，无论是zs还是zs’，他们都应能被正确分类。在DRRDN中，我们使用一个解耦器ds(·; φs)来分离z和z’的特定类表征：</p><img src="/2020/06/19/Adversarial%20Robustness%20through%20Disentangled%20Representations/公式3.png" alt><p>​回顾特定类表征的工作，我们将分类损失（如交叉熵）应用于ds(·; φs)上。</p><img src="/2020/06/19/Adversarial%20Robustness%20through%20Disentangled%20Representations/公式4.png" alt><p>​其中，cs(·, ws) ∈ Rk是以ws为参数的特定类分类器。</p><p>​然而，仅分离特定类的表征可能并不足以实现完全解耦。捕获扰动噪声信息的类型无关的部分也应被建模，并从整体表征中去除。与ds类似，我们设计了一个类型无关解耦器di(·; φi)来建模类型无关的信息：</p><p>​<img src="/2020/06/19/Adversarial%20Robustness%20through%20Disentangled%20Representations/公式5.png" alt></p><p>​请注意，类型无关的表征应捕捉自然表征与对抗表征的特点，即zi应是可与zi’解耦的。因此，我们在di上应用二元分类损失，为：</p><p>​<img src="/2020/06/19/Adversarial%20Robustness%20through%20Disentangled%20Representations/公式6.png" alt></p><p>​其中ci(·,wi)∈[0,1]是以wi为参数的类型无关分类器，其试图将不管是特定类的，还是类型无关的自然表征与对抗表征都进行区分。</p><p>​一个好的解耦应产生相互独立的部分，这些部分不共享彼此之间的信息。为了实现这一目标，我们考虑以下两个方面：功能排他性与互信息最小化。首先，功能排他性是指被解耦的分支应有且仅有一项特定任务来胜任，而对其余任务无能为力。公式(4)与(6)已确保每个解耦的表征都能执行相应的任务，但并未对对应表征施加任何约束，这可能导致扰动信息的残留，或特定类表征中鲁棒信息的损失。因此，受生成对抗网络的启发，我们通过使用对抗损失来更新ds，以欺骗训练好的ci*：</p><p>​<img src="/2020/06/19/Adversarial%20Robustness%20through%20Disentangled%20Representations/公式7.png" alt></p><p>​其中ci*(·,wi*)是公式6中训练好的类型无关的分类器。除此之外，我们正则化优化好的cs*对di的预测熵，使其尽可能大，以保证di不能被正确分类：</p><img src="/2020/06/19/Adversarial%20Robustness%20through%20Disentangled%20Representations/公式8.png" alt><p>​其中，cs*(·,ws*)使用公式(4)优化，公式(8)被用来最小化cs*(·,ws*)对di的负信息熵。</p><p>​其次，对于完全解耦，ds与di的输出之间的依赖性应尽可能小。在我们的方法中，我们对ds与di进行了互信息最小化（MI）操作，以更好地解耦对抗扰动的影响。然而，MI的计算通常是难以实现的。因此，在我们的模型中，我们采用互信息神经估计器（Mutual Information Neural Estimator, MINE）来进行MI的无偏估计。具体而言，给定联合分布的特定类与类型无关表征对<img src="/2020/06/19/Adversarial%20Robustness%20through%20Disentangled%20Representations/公式8.1.png" alt>的n个样本，以及来源于边缘分布的<img src="/2020/06/19/Adversarial%20Robustness%20through%20Disentangled%20Representations/公式8.2.png" alt>的n对样本（请注意，被解耦的表征可能来源于自然或对抗表征），MINE通过蒙特卡洛积分经验地估计ds与di之间的MI，如下所示：</p><img src="/2020/06/19/Adversarial%20Robustness%20through%20Disentangled%20Representations/公式9.png" alt><p>​其中，T(·; ψ)是以ψ为参数的神经网络。</p><p>​最后，遵循标准的AE架构，我们使用了一个重构器以恢复完整表征，以保持解耦表征的跨周期一致性。本文中，为简单起见，我们使用了一个L2重构器：</p><img src="/2020/06/19/Adversarial%20Robustness%20through%20Disentangled%20Representations/公式10.png" alt><p>​其中，r(·,·; θrec)是以θrec为参数的重构器。</p><img src="/2020/06/19/Adversarial%20Robustness%20through%20Disentangled%20Representations/算法1.png" alt><p>​请注意，在推理阶段与生成对抗样本时，我们仅使用基本的表征提取器g(·; φ)，特定类解耦器ds(·; φ)与特定类分类器cs(·;ws)。为了更好的理解DRRDN的结构，我们在图1中给出了说明。我们还在算法1中总结了DRRDN的具体训练步骤。具体来说，DRRDN的总体目标可总结如下：</p><p>​<img src="/2020/06/19/Adversarial%20Robustness%20through%20Disentangled%20Representations/公式11.png" alt></p><img src="/2020/06/19/Adversarial%20Robustness%20through%20Disentangled%20Representations/图1.png" alt><p>图1 DRRDN架构示意图。g(·;θ) 表示以θ为参数的特征提取器，ds(·; φs)和di(·; φi)分别是以φs和φi为参数的特定类和类型无关表征的解耦器，cs是预测输入表征类别的分类器，ci是区分表征来源于哪个域的鉴别器，r是重构解码器，z表示每个模块的输出表征。</p><h2><span id="3-理论分析">3. 理论分析</span></h2><p>​本节中，我们提供对所提方法的理论理解。Zhang等人(2019)指出，模型在自然样本和对对抗样本的鲁棒性之间存在一种权衡。我们假设这种权衡的一个可能原因是分类器不能泛化来自自然与对抗分布的表征。在公式(6)和(7)中，我们定义了一个对抗损失以正则化特定类的解耦器。在下面的命题中，我们证明，通过优化(6)和(7)中的目标，来自自然和对抗样本的特定类表征可以相互一致。因为我们只使用特定类表征进行推理，因此可以减少模型准确性与鲁棒性之间的差距。</p><p><strong>命题1</strong> 当达到全局最小值<img src="/2020/06/19/Adversarial%20Robustness%20through%20Disentangled%20Representations/命题1.png" alt>和<img src="/2020/06/19/Adversarial%20Robustness%20through%20Disentangled%20Representations/命题2.png" alt>时，pφs(zs)等于pφs (zs’)。</p><p>证明：根据Goodfellow等的研究，给定自然与对抗的特定类表征zs∼ pφs(zs)，zs’ ∼ pφs(zs’)，最优类型无关分类器ci*(·,wi*)满足：</p><img src="/2020/06/19/Adversarial%20Robustness%20through%20Disentangled%20Representations/公式12.png" alt><p>​使用固定的最优类型无关分类器ci*(·,wi*)，公式(7)可被转换为：</p><img src="/2020/06/19/Adversarial%20Robustness%20through%20Disentangled%20Representations/公式13.png" alt><p>因此，当pφs(zs)等于pφs (zs’)时，可达到最优<img src="/2020/06/19/Adversarial%20Robustness%20through%20Disentangled%20Representations/命题3.png" alt>，命题证明完毕。</p><h2><span id="4-实验结果">4. 实验结果</span></h2><p>​本节中，我们通过各种实验验证我们方法的有效性。我们首先进行白盒与黑盒攻击，以比较DRRDN与最先进方法的鲁棒性。本节还对解耦的影响进行了进一步的分析。</p><h3><span id="设置">设置</span></h3><p><strong>数据集</strong>。我们使用MNIST和CIFAR10作为评估防御方法的基准数据集。我们保持默认训练&#x2F;测试集的分割比例。</p><p><strong>实现</strong>。我们使用一个四层卷积网络作为MNIST数据集的基本表征提取器。DRRDN的解耦器和分类器是全连接层，为公平比较，DRRDN分类器的容量与基准模型相同。对于CIFAR10数据集，我们采用WRN-28-10作为表征编码器。在设计解耦器和分类器时，我们遵循了同对MNIST数据集相同的原则。在MINE和重构器模块中，为简单起见，我们亦采用了全连接层。针对优化方面，我们使用SGD优化器，初始学习率在CIFAR10上为1e-2和1e-1。训练总epoch为100次，在此期间，分别在第55、75和90个epoch上衰减0.01的学习率。</p><p><strong>防御设置</strong>。我们在对抗训练框架下训练DRRDN。对于训练攻击，我们设置对MNIST的扰动ε&#x3D;0.3，步长α&#x3D;0.01；对CIFAR10的扰动ε&#x3D;0.031，步长α&#x3D;0.007。对于MNIST和CIFAR10，生成训练攻击的更新迭代次数分别为40和10。为简单起见，我们仅验证l∞范数攻击。</p><h3><span id="不同攻击下对抗防御的表现">不同攻击下对抗防御的表现</span></h3><p><strong>白盒攻击</strong>。白盒攻击假定攻击者可以访问目标模型的结构与参数，并可直接根据梯度生成对抗样本。因此，标准训练模型的鲁棒性非常脆弱。我们采用三种类型的攻击进行验证，分别为FGSM、PGD和CW。PGD和CW对MNIST和CIFAR产生的攻击步长分别为0.01和0.003，FGSM的步长放大了10倍。我们遵循Carlini和Wagner的模式进行CW攻击。我们将DRRDN的性能与标准方式下训练的模型（即只用自然数据训练），以及在Madry和TRADES下训练的模型进行鲁棒性的比较。表1总结了自然准确度和鲁棒准确度。</p><img src="/2020/06/19/Adversarial%20Robustness%20through%20Disentangled%20Representations/表1.png" alt><p>表1 在不同对抗攻击下模型对(a)MNIST和(b)CIFAR10的表现</p><p>​从表1我们可以得到，标准模型几乎无法防御任何方式的攻击。当用对抗样本进行测试时，除如FGSM这种稍弱的攻击外，标准模型的鲁棒准确性会突然下降到几乎为0。Madry和TRADES证明了它们在防御对抗攻击方面的有效性，然而，它们自然准确度和鲁棒准确度之间的差距依旧较大。我们归因该现象为自然样本与对抗样本间表征分布不一致，所以模型难以同时泛化自然样本与对抗样本。相比之下，我们提出的DRRDN不仅在对抗分类准确度上取得了更高的效果，且在自然和对抗准确度之间保持了较小的差距。这表明，通过解耦和一致化特定类表征，可以让自然准确度和对抗鲁棒性之间的联系更加紧密。</p><p><strong>黑盒攻击</strong>。我们进一步用黑盒攻击验证我们的模型。顾名思义，黑盒攻击假设攻击者并不知道目标模型防御的细节。尽管黑盒攻击通常比白盒弱得多，但在现实应用中，黑盒攻击设置更自然，因模型的具体设计往往是并不明确的，且其可证明模型对不同攻击的模型鲁棒性的可迁移性。</p><p>​对于黑盒攻击，我们先独立地训练代用模型，包括Standard、Madry、TRADES和DRRDN，根据他们的梯度使用PGD方法来产生对抗样本。之后，由代用模型产生的对抗样本将用于攻击其他目标模型。具体而言，在生成黑盒攻击时，我们先使用与白盒攻击相同的模型结构。为方便起见，PGD方法也使用同一组参数（即MINIST ε&#x3D;0.3，α&#x3D;0.01，40次迭代；CIFAR10 ε&#x3D;0.31，α&#x3D;0.003，20次迭代）。黑盒攻击下的分类结果如表2所示。</p><img src="/2020/06/19/Adversarial%20Robustness%20through%20Disentangled%20Representations/表2.png" alt><p>表2 在不同对抗攻击下模型对(a)MNIST和(b)CIFAR10的表现</p><p>​垂直观察表2，我们可以看到DRRDN赋予了模型对黑盒攻击最强的鲁棒性，这意味着DRRDN的鲁棒性并非针对单种攻击，而是普适的。这可能是因为类型无关的表征可以很好地区分自然分布与对抗分布，并将不同的攻击引导至非自然分布上。而有一个有趣的现象是，当我们横向观察表2，DRRDN提供了最弱的攻击。事实上，对这一现象存在自然且直观的解释。我们使用DRRDN生成基于特定类分支的梯度的对抗扰动。特定类表征可捕捉到输入的基本特征（理想情况下，比只用自然数据训练的标准模型所提取出的表征更纯粹，因为自然分布的冗余信息也被消除了）。因此，DRRDN产生的对抗扰动应比标准模型产生的扰动更弱。</p><h3><span id="解耦的影响">解耦的影响</span></h3><p>​上述实验表明，DRRDN能使DNN模型对白盒与黑盒对抗攻击具有更强的鲁棒性。我们想知道，我们精心设计的解耦机制是否真的有利于模型的鲁棒性。本节中，我们进行两个探索性实验来验证解耦的有效性。</p><p><strong>混合重构表征</strong>。在方法部分，我们详细介绍了特定类表征和分类无关表征的功能。特定类解耦器目标是仅保留分类信息的表征，而分类无关表征的目的是吸收扰动的信息。当然，从自然样本和对抗样本中解耦出的特定类的表征应能很好地逐个泛化，因为它们的分布基本相同。因此，如果DRRDN的解耦机制真的有作用，不难想象，若将具有不同分布标签（即自然分布和对抗分布）的特定类和类型无关的表征结合起来，重建混合表征，我们将在标准模型的、在混合表征上微调的分类器上得到不同的分类结果。因为混合表征实际上是从两个分布中还原了原始表征。在与MNIST数据集上的白盒攻击相同的设置下，我们使用标准训练模型进行基于混合重构表征的分类实验。结果如图3所示。请注意，通过排列组合，我们可以重建四种混合表征，分别为zs+zi，zs‘+zi，zs+zi’和zs’+zi’。</p><img src="/2020/06/19/Adversarial%20Robustness%20through%20Disentangled%20Representations/表3.png" alt><p>表3 混合表征的分类能力</p><p>​表3中，非常明显，与zi’混合的重建表征（即来自对抗样本的类型无关表征）通常更难被标准模型正确分类。相反，包含zi的混合表征（即来自自然样本的类型无关表征）可以被很好地区分。这意味着分布特征依被成功分解至类型无关表征中去了。</p><p><strong>解耦表征的可视化</strong>。为更好展示拆分效果，我们对拆分后的表征进行了可视化实验。可视化实验亦基于MNIST数据集的白盒攻击的实验设置。在得到用DRRDN训练的鲁棒模型后，我们首先从自然与对抗测试数据中提取特定类和类型无关的表征，然后通过t-SNE将高维表征压缩成一个二维特征。我们在图2展示了可视化效果。不同类别的压缩特征用不同颜色表示。除了特定类和类型无关的表征外，我们亦对标准模型的表征进行了可视化比较。</p><img src="/2020/06/19/Adversarial%20Robustness%20through%20Disentangled%20Representations/图2.png" alt><p>图2：MNIST上的特定类、类型无关和标准表征</p><p>​图2 的第一和第三行分别展示了来自自然分布和对抗分布的压缩表征，中间一行为两个分布的压缩表征的组合，其用来表明分布的一致性。我们对图2的压缩表征有如下三个重要发现：1）自然与对抗样本中。只有特定类的表征是可被分类的；2）只有类型无关表征可区分自然与对抗分布；3）在中间一行，自然与对抗样本中提取的特定类表征比标准表征更紧凑与吻合。上述特点亦证明解耦的效果。</p><p><strong>消融实验</strong>。我们进一步进行了消融实验，分别从整个目标函数中去除互信息项（公式(9)）或重构项（公式(10)）。如表4所示，我们发现在没有互信息的情况下，DRRDN的准确度下降比没有重构项的情况下要严重得多，这也展示了完全解耦的重要性。</p><img src="/2020/06/19/Adversarial%20Robustness%20through%20Disentangled%20Representations/表4.png" alt><p>表4 对MNIST数据集的消融实验。DRRDN（最终算法）、DRRDN-MI（无互信息项）与DRRDN-REC（无重构项）。</p><h2><span id="5-总结">5. 总结</span></h2><p>​本文中，我们研究了针对对抗攻击的防御问题。我们提出了DRRDN模型来拆分特定类表征，其中关于自然与对抗领域特征的信息已被完全去除。我们亦通过使用GAN理论为我们提出的模型提供了理论保证。基于基准数据集的实证评估进一步证明了DRRDN与最先进防御方法相比的优越性。</p>]]></content>
    
    
    <summary type="html">&lt;img src=&quot;/2020/06/19/Adversarial%20Robustness%20through%20Disentangled%20Representations/封面.png&quot; alt&gt;

&lt;h1 id=&quot;通过解耦表征实现对抗鲁棒性&quot;&gt;&lt;a href=&quot;#通过解耦表征实现对抗鲁棒性&quot; class=&quot;headerlink&quot; title=&quot;通过解耦表征实现对抗鲁棒性&quot;&gt;&lt;/a&gt;通过解耦表征实现对抗鲁棒性&lt;/h1&gt;&lt;h2 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要&quot;&gt;&lt;/a&gt;摘要&lt;/h2&gt;&lt;p&gt;​	尽管深度学习模型具有优秀的实证性能，但诸多研究已发现了它们对对抗样本的脆弱性。它们易对具有难以察觉的对抗扰动的输入做出易受其影响的预测。尽管最近的研究已明显提升了在对抗训练策略下模型的鲁棒性，但自然准确性与对抗鲁棒性之间的差距依旧显著。为缓解此问题，本文中，我们假设鲁棒性与非鲁棒性表征是整体表征中相互耦合的两个基本成分。为实现对抗鲁棒性，自然与对抗样本的鲁棒表征应与非鲁棒部分分离，鲁棒表征的一致化可弥补准确性与鲁棒性之间的差距。受此启发，我们提出了一种称为深度鲁棒解耦表征网络（Deep Robust Representation Disentanglement Network, DRRDN）的新防御方法。具体而言，DRRDN使用解耦器从对抗样本与自然样本中提取并一致化鲁棒表征。理论分析保证了我们的方法能在具有良好解耦与一致化性能的情况下平衡鲁棒性与准确性。在基准数据集上的实验结果证明了我们方法的实证优势。&lt;/p&gt;</summary>
    
    
    
    <category term="论文阅读" scheme="https://values.keys.moe/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="论文阅读" scheme="https://values.keys.moe/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
  </entry>
  
  <entry>
    <title>通过Xrdp实现Windows远程访问Ubuntu 16.04</title>
    <link href="https://values.keys.moe/2019/03/05/%E9%80%9A%E8%BF%87Xrdp%E5%AE%9E%E7%8E%B0Windows%E8%BF%9C%E7%A8%8B%E8%AE%BF%E9%97%AEUbuntu-16-04/"/>
    <id>https://values.keys.moe/2019/03/05/%E9%80%9A%E8%BF%87Xrdp%E5%AE%9E%E7%8E%B0Windows%E8%BF%9C%E7%A8%8B%E8%AE%BF%E9%97%AEUbuntu-16-04/</id>
    <published>2019-03-05T03:25:45.000Z</published>
    <updated>2022-09-28T06:24:41.208Z</updated>
    
    <content type="html"><![CDATA[<p>​目前网上的大量教程都是需要安装xfac4或者xubuntu桌面系统才能实现远程连接。因为xrdp支持在13.10之后版本就已经不支持的Gnome了和原生Unity桌面，所以网上很多方法都是安装能够被xdrp支持的第三方xfac4或者xubuntu桌面系统间接达到远程控制Ubuntu。<br>​本文提供如何使用Xrdp访问原生Ubuntu桌面。</p><span id="more"></span><h2><span id="step1-下载tigervnc-server软件包">Step.1 下载TigerVNC Server软件包</span></h2><p>下载地址：<br><a href="http://www.c-nergy.be/downloads/tigervncserver_1.6.80-4_amd64.zip">http://www.c-nergy.be/downloads/tigervncserver_1.6.80-4_amd64.zip</a></p><h2><span id="step2-安装tigervnc-server">Step.2 安装TigerVNC Server</span></h2><p>1.打开终端，进入到刚刚你你下载TigerVNC Server的存放目录</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">cd</span> 下载</span><br></pre></td></tr></table></figure><p>2.执行安装指令</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo dpkg -i tigervncserver_1.<span class="number">6.80</span>-<span class="number">4</span>_amd64.<span class="keyword">deb</span></span><br><span class="line">或者</span><br><span class="line">sudo apt-<span class="built_in">get</span> install tightvncserver</span><br></pre></td></tr></table></figure><p>过程中如果出现警告信息和错误信息，原因是没有相对应的依赖包。<br>执行</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-<span class="built_in">get</span> install -<span class="keyword">f</span></span><br></pre></td></tr></table></figure><p>然后在执行之前的安装命令。</p><h2><span id="step3-安装xrdp">Step.3 安装Xrdp</span></h2><p>终端输入安装命令</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-<span class="built_in">get</span> install xrdp -<span class="keyword">y</span></span><br></pre></td></tr></table></figure><h2><span id="step4-配置xrdp">Step.4 配置Xrdp</span></h2><p>需要通过xrdp连接到桌面，<br>需要正确配置相关信息并填充到.xsession文件（针对每个用户）<br>或&#x2F;etc&#x2F;startwm.sh（针对所有用户）<br>命令如下：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">echo</span> unity&gt;~/.xsession</span><br><span class="line">或者</span><br><span class="line">sudo sed -i.bak <span class="string">&#x27;/fi/a #xrdp multi-users \n unity \n&#x27;</span> /etc/xrdp/startwm.<span class="keyword">sh</span></span><br></pre></td></tr></table></figure><h2><span id="step5-重启xrdp服务">Step.5 重启Xrdp服务</span></h2><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo service xrdp restart</span><br></pre></td></tr></table></figure><h2><span id="step6-开启桌面共享功能">Step.6 开启桌面共享功能</span></h2><img src="/2019/03/05/%E9%80%9A%E8%BF%87Xrdp%E5%AE%9E%E7%8E%B0Windows%E8%BF%9C%E7%A8%8B%E8%AE%BF%E9%97%AEUbuntu-16-04/桌面共享.jpg" alt><p>系统-&gt;首选项-&gt;桌面共享，或者直接搜索桌面共享功能<br>进入后<br>将【允许其他人查看您的桌面】勾上，<br>【自动配置UPnP路由器开放和转发接口】勾上，如图所示</p><img src="/2019/03/05/%E9%80%9A%E8%BF%87Xrdp%E5%AE%9E%E7%8E%B0Windows%E8%BF%9C%E7%A8%8B%E8%AE%BF%E9%97%AEUbuntu-16-04/桌面共享2.jpg" alt><p>之后配置基本结束。Windows可以通过mstc直接通过IP访问Ubuntu。</p><hr><p>注：Ubuntu18尝试后似乎没有桌面共享功能的选项。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;​		目前网上的大量教程都是需要安装xfac4或者xubuntu桌面系统才能实现远程连接。因为xrdp支持在13.10之后版本就已经不支持的Gnome了和原生Unity桌面，所以网上很多方法都是安装能够被xdrp支持的第三方xfac4或者xubuntu桌面系统间接达到远程控制Ubuntu。&lt;br&gt;​		本文提供如何使用Xrdp访问原生Ubuntu桌面。&lt;/p&gt;</summary>
    
    
    
    <category term="Ubuntu" scheme="https://values.keys.moe/categories/Ubuntu/"/>
    
    
    <category term="Ubuntu" scheme="https://values.keys.moe/tags/Ubuntu/"/>
    
  </entry>
  
  <entry>
    <title>树莓派人脸识别-face-recognition的安装与应用</title>
    <link href="https://values.keys.moe/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB-face-recognition%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E5%BA%94%E7%94%A8/"/>
    <id>https://values.keys.moe/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB-face-recognition%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E5%BA%94%E7%94%A8/</id>
    <published>2019-03-02T03:06:57.000Z</published>
    <updated>2022-09-27T17:08:54.800Z</updated>
    
    <content type="html"><![CDATA[<img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB-face-recognition%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E5%BA%94%E7%94%A8/人脸识别.gif" alt><span id="more"></span><img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB-face-recognition%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E5%BA%94%E7%94%A8/face_recongnition.png" alt>该库可以通过python或者命令行即可实现人脸识别的功能。使用dlib深度学习人脸识别技术构建，在户外脸部检测数据库基准（Labeled Faces in the Wild）上的准确率为99.38%。<h2><span id="安装过程"><strong>安装过程</strong></span></h2><p>先在终端下安装一大堆需要的库：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-<span class="built_in">get</span> <span class="keyword">update</span> sudo apt-<span class="built_in">get</span> install build-essential \</span><br><span class="line">cmake \</span><br><span class="line">gfortran \</span><br><span class="line">git \</span><br><span class="line">wget \</span><br><span class="line">curl \</span><br><span class="line">graphicsmagick \</span><br><span class="line">libgraphicsmagick1-dev \</span><br><span class="line">libatlas-dev \</span><br><span class="line">libavcodec-dev \</span><br><span class="line">libavformat-dev \</span><br><span class="line">libboost-<span class="keyword">all</span>-dev \</span><br><span class="line">libgtk2.<span class="number">0</span>-dev \</span><br><span class="line">libjpeg-dev \</span><br><span class="line">liblapack-dev \</span><br><span class="line">libswscale-dev \</span><br><span class="line">pkg-config \</span><br><span class="line"><span class="keyword">python3</span>-dev \</span><br><span class="line"><span class="keyword">python3</span>-numpy \</span><br><span class="line"><span class="keyword">python3</span>-pip \</span><br><span class="line">zip</span><br></pre></td></tr></table></figure><p>如果使用树莓派的摄像头（CSI接口），执行下面的命令：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-<span class="built_in">get</span> install <span class="keyword">python3</span>-picamera</span><br><span class="line">sudo pip3 install --upgrade picamera[array]</span><br></pre></td></tr></table></figure><p>下载安装dlib：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mkdir</span> -<span class="keyword">p</span> dlib </span><br><span class="line">git clone -<span class="keyword">b</span> <span class="string">&#x27;v19.6&#x27;</span> --single-branch https://github.<span class="keyword">com</span>/davisking/dlib.git dlib/ </span><br><span class="line"><span class="keyword">cd</span> ./dlib</span><br><span class="line">sudo <span class="keyword">python3</span> setup.<span class="keyword">py</span> install --<span class="keyword">compiler</span>-flags <span class="string">&quot;-mfpu=neon&quot;</span></span><br></pre></td></tr></table></figure><p>安装face_recognition：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo pip3 install face_recognition</span><br></pre></td></tr></table></figure><p>下载示例代码并尝试运行（可选）：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git clone --single-branch https://github.<span class="keyword">com</span>/ageitgey/face_recognition.git</span><br><span class="line"><span class="keyword">cd</span> ./face_recognition/examples</span><br><span class="line"><span class="keyword">python3</span> facerec_on_raspberry_pi.<span class="keyword">py</span></span><br></pre></td></tr></table></figure><p><strong>注</strong>：过程缺少库时，使用</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">apt-cache <span class="built_in">search</span> 库名</span><br></pre></td></tr></table></figure><p>来搜索到那个库的安装包，然后用</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-<span class="built_in">get</span> install 包名</span><br></pre></td></tr></table></figure><p>来安装。</p><p><strong>例如</strong>：缺少了libatlas.so.3，那我们就用<strong>apt-cache search libatlas</strong>来搜索，发现它的包名叫libatlas3-base，所以我们用<strong>sudo apt-get install libatlas3-base</strong>来安装。<br>后面测试摄像头的时候也会遇到这样的问题，解决办法是一样的。 </p><p>待把CSI接口树莓派摄像头装上后，在raspi-config中启用摄像头，然后重启。<br>（详见博客内关于CSI摄像头的另一篇文章）</p><p>运行一下实时人脸识别的代码（可选）：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">python3</span> facerec_from_webcam_faster.<span class="keyword">py</span></span><br></pre></td></tr></table></figure><p>过程可能报错，import cv2的时候缺少库，然后根据提示用之前安装方法安装就好了。装完一个库再运行的时候，发现又提示缺少别的库，然后再安装缺少的库。反复个多次，把缺少的库都装好即可。</p><p>再次运行的时候，会报别的错误，出错的代码是</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">small_frame = cv2.<span class="keyword">resize</span>(frame, (<span class="number">0</span>, <span class="number">0</span>), fx=<span class="number">0.25</span>, fy=<span class="number">0.25</span>)</span><br></pre></td></tr></table></figure><p>这是因为video_capture.read()没有读到图片。<br>树莓派中的camera module是放在&#x2F;boot&#x2F;目录中以固件形式加载的，不是一个标准的V4L2的摄像头驱动，所以加载起来之后会找不到&#x2F;dev&#x2F;video0的设备节点。<br><strong>解决方法</strong>：转载<a href="https://blog.csdn.net/deiki/article/details/71123947">https://blog.csdn.net/deiki/article/details/71123947</a></p><p>之后可以使用下面的命令来加载驱动模块：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo modprobe bcm2835-v4l2</span><br></pre></td></tr></table></figure><p>如果想开机自动加载，我们可以修改&#x2F;etc&#x2F;modules文件，添加一行：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bcm2835-v4l2</span><br></pre></td></tr></table></figure><p>如下图所示：<br><img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB-face-recognition%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E5%BA%94%E7%94%A8/bcm2835-v4l2.png" alt></p><p>安装过程基本完毕。</p><h2><span id="个人提供的样例代码"><strong>个人提供的样例代码：</strong></span></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env </span></span><br><span class="line"><span class="comment">#coding=utf-8</span></span><br><span class="line"><span class="comment">#从目录中读取一堆文件</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> face_recognition</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> RPi.GPIO</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">RPi.GPIO.setmode(RPi.GPIO.BCM)</span><br><span class="line"></span><br><span class="line">RPi.GPIO.setwarnings(<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">buzzer=<span class="number">4</span></span><br><span class="line">RPi.GPIO.setup(buzzer,RPi.GPIO.OUT)</span><br><span class="line"></span><br><span class="line">RPi.GPIO.output(buzzer,<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">strangerAppear=<span class="literal">False</span></span><br><span class="line">strangerNum=<span class="number">0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">demo_filelist=[]</span><br><span class="line">demo_face_encodings=[]</span><br><span class="line">demo_face_names=[]</span><br><span class="line"><span class="keyword">for</span> f <span class="keyword">in</span> os.listdir(<span class="string">&#x27;/home/pi/demo/face_recognition/jpg//&#x27;</span>):</span><br><span class="line">    demo_filelist.append(f)</span><br><span class="line">    demo_face_names.append(f[:-<span class="number">4</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> filename <span class="keyword">in</span> demo_filelist:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">u&#x27;正在加载....&#x27;</span>+filename)</span><br><span class="line">    demo_image = face_recognition.load_image_file(<span class="string">&#x27;/home/pi/demo/face_recognition/jpg//&#x27;</span>+filename)</span><br><span class="line">    face_encoding = face_recognition.face_encodings(demo_image)[<span class="number">0</span>]</span><br><span class="line">    demo_face_encodings.append(face_encoding)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize some variables</span></span><br><span class="line">face_locations = []</span><br><span class="line">face_encodings = []</span><br><span class="line">face_names = []</span><br><span class="line">process_this_frame = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Get a reference to webcam #0 (the default one)</span></span><br><span class="line">video_capture = cv2.VideoCapture(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    <span class="comment"># Grab a single frame of video</span></span><br><span class="line">    ret, frame = video_capture.read()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Resize frame of video to 1/4 size for faster face recognition processing</span></span><br><span class="line">    small_frame = cv2.resize(frame, (<span class="number">0</span>, <span class="number">0</span>), fx=<span class="number">0.3</span>, fy=<span class="number">0.3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Only process every other frame of video to save time</span></span><br><span class="line">    <span class="keyword">if</span> process_this_frame==<span class="number">1</span>:</span><br><span class="line">        <span class="comment"># Find all the faces and face encodings in the current frame of video</span></span><br><span class="line">        face_locations = face_recognition.face_locations(small_frame)</span><br><span class="line">        face_encodings = face_recognition.face_encodings(small_frame, face_locations)</span><br><span class="line">   <span class="comment">#     print(u&quot;我检测到了&#123;&#125;张脸。&quot;.format(len(face_locations)))</span></span><br><span class="line">        face_names = []</span><br><span class="line">        <span class="keyword">for</span> face_encoding <span class="keyword">in</span> face_encodings:</span><br><span class="line">        <span class="comment"># See if the face is a match for the known face(s)</span></span><br><span class="line">            <span class="keyword">match</span> = face_recognition.compare_faces(demo_face_encodings, face_encoding,tolerance=<span class="number">0.6</span>)<span class="comment">#</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="keyword">match</span>)</span><br><span class="line">            name = <span class="string">&quot;Unknown&quot;</span></span><br><span class="line">            i=-<span class="number">1</span></span><br><span class="line">            <span class="keyword">for</span> m <span class="keyword">in</span> <span class="keyword">match</span>:</span><br><span class="line">                i+=<span class="number">1</span></span><br><span class="line">                <span class="keyword">if</span> m:</span><br><span class="line">                    name= demo_face_names[i]</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span>(name==<span class="string">&quot;Unknown&quot;</span>):</span><br><span class="line">                RPi.GPIO.output(buzzer,<span class="literal">False</span>)</span><br><span class="line">                frame_1=cv2.flip(frame,<span class="number">1</span>)</span><br><span class="line">                path=<span class="string">&quot;/home/pi/demo/face_recognition/&quot;</span>+<span class="built_in">str</span>(strangerNum)+<span class="string">&quot;.png&quot;</span></span><br><span class="line">                cv2.imwrite(path,frame_1)</span><br><span class="line">                strangerNum=strangerNum+<span class="number">1</span></span><br><span class="line">                strangerAppear=<span class="literal">True</span></span><br><span class="line">            face_names.append(name)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Display the results</span></span><br><span class="line">    <span class="keyword">for</span> (top, right, bottom, left), name <span class="keyword">in</span> <span class="built_in">zip</span>(face_locations, face_names):</span><br><span class="line">        <span class="comment"># Draw a box around the face</span></span><br><span class="line">        <span class="comment">#cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Draw a label with a name below the face</span></span><br><span class="line"><span class="comment">#        cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 0, 255), 5)</span></span><br><span class="line">        font = cv2.FONT_HERSHEY_DUPLEX<span class="comment">#FONT_HERSHEY_DUPLEX</span></span><br><span class="line"><span class="comment">#        print(name)</span></span><br><span class="line"><span class="comment">#        boxsize, _ = cv2.getTextSize(fs.string, fs.face, fs.fsize, fs.thick)</span></span><br><span class="line"> <span class="comment">#       locx = int((right+left)/2-25 - 14*len(name)/2)</span></span><br><span class="line">        cv2.putText(frame, name, (<span class="number">20</span>,<span class="number">20</span>), font, <span class="number">1.0</span>, (<span class="number">0</span>, <span class="number">0</span>, <span class="number">255</span>), <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    process_this_frame += <span class="number">1</span><span class="comment"># not process_this_frame</span></span><br><span class="line">    <span class="keyword">if</span> process_this_frame&gt;<span class="number">1</span>:</span><br><span class="line">        process_this_frame=<span class="number">1</span></span><br><span class="line">    <span class="comment"># Display the resulting image</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">############</span></span><br><span class="line">    cv2.imshow(<span class="string">&#x27;Video&#x27;</span>, frame)</span><br><span class="line">    cv2.moveWindow(<span class="string">&#x27;Video&#x27;</span>,<span class="number">600</span>,<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Hit &#x27;q&#x27; on the keyboard to quit!</span></span><br><span class="line">    <span class="keyword">if</span> cv2.waitKey(<span class="number">1</span>) &amp; <span class="number">0xFF</span> == <span class="built_in">ord</span>(<span class="string">&#x27;q&#x27;</span>):</span><br><span class="line">        RPi.GPIO.output(buzzer,<span class="literal">True</span>)</span><br><span class="line">        strangerAppear=<span class="literal">False</span></span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Release handle to the webcam</span></span><br><span class="line">video_capture.release()</span><br><span class="line">cv2.destroyAllWindows()</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">入门介绍</summary>
    
    
    
    <category term="树莓派" scheme="https://values.keys.moe/categories/%E6%A0%91%E8%8E%93%E6%B4%BE/"/>
    
    
    <category term="树莓派" scheme="https://values.keys.moe/tags/%E6%A0%91%E8%8E%93%E6%B4%BE/"/>
    
    <category term="Python" scheme="https://values.keys.moe/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>树莓派CSI摄像头的连接与常用指令</title>
    <link href="https://values.keys.moe/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BECSI%E6%91%84%E5%83%8F%E5%A4%B4%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%B8%8E%E5%B8%B8%E7%94%A8%E6%8C%87%E4%BB%A4/"/>
    <id>https://values.keys.moe/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BECSI%E6%91%84%E5%83%8F%E5%A4%B4%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%B8%8E%E5%B8%B8%E7%94%A8%E6%8C%87%E4%BB%A4/</id>
    <published>2019-03-02T01:28:19.000Z</published>
    <updated>2022-09-28T06:03:44.256Z</updated>
    
    <content type="html"><![CDATA[<img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BECSI%E6%91%84%E5%83%8F%E5%A4%B4%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%B8%8E%E5%B8%B8%E7%94%A8%E6%8C%87%E4%BB%A4/CSI摄像头.jpg" alt><span id="more"></span><h1><span id="安装树莓派摄像头模块">安装树莓派摄像头模块</span></h1><p>​1、找到 CSI 接口(CSI接口在以太网接口旁边)，掀起深色胶带。<br>​2、拉起 CSI 接口挡板。<br>​3、拿起你的摄像头模块，将贴在镜头上的塑料保护膜撕掉。确保黄色部分的PCB(有字的一面)是安装完美的（可以轻轻按一下黄色的部分来保证安装完美）。<br>​4、将排线插入CSI接口。记住，有蓝色胶带的一面应该面向以太网接口方向。同样，这时也确认一下排线安装好了之后，将挡板拉下。</p><img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BECSI%E6%91%84%E5%83%8F%E5%A4%B4%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%B8%8E%E5%B8%B8%E7%94%A8%E6%8C%87%E4%BB%A4/安装图.jpg" alt><h1><span id="在树莓派上启用摄像头模块">在树莓派上启用摄像头模块</span></h1><p>在安装完摄像头模块之后，首先要确认你已经升级了树莓派系统并应用了最新的固件。<br>终端下可以输入以下命令来操作：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-<span class="built_in">get</span> <span class="keyword">update</span>   </span><br><span class="line">sudo apt-<span class="built_in">get</span> upgrade </span><br></pre></td></tr></table></figure><p>运行树莓派配置工具来激活摄像头模块：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo raspi-config</span><br></pre></td></tr></table></figure><p>移动光标至菜单中的 “Enable Camera（启用摄像头）”，将其设为Enable（启用状态）。完成之后重启树莓派。</p><h1><span id="安装驱动使能树莓派的相关模块">安装驱动使能树莓派的相关模块</span></h1><h2><span id="1-添加驱动程序文件进来">1、添加驱动程序文件进来：</span></h2><p>终端输入指令</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo <span class="keyword">vim</span> /etc/modules</span><br></pre></td></tr></table></figure><p>在最后添加如下的代码：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bcm2835-v4l2</span><br></pre></td></tr></table></figure><p>这样就完成了在启动过程中加载camera驱动的前提。</p><h2><span id="2-修改raspberry的启动配置使能项">2、修改Raspberry的启动配置使能项：</span></h2><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo raspi-config</span><br></pre></td></tr></table></figure> <img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BECSI%E6%91%84%E5%83%8F%E5%A4%B4%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%B8%8E%E5%B8%B8%E7%94%A8%E6%8C%87%E4%BB%A4/配置1.jpg" alt><p>(由于系统版本，树莓派版本不同，显示设置可能不同，但基本大同小异。)<br>选择Interfacing Option，选中Select然后Enter进入，如下图所示： </p><img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BECSI%E6%91%84%E5%83%8F%E5%A4%B4%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%B8%8E%E5%B8%B8%E7%94%A8%E6%8C%87%E4%BB%A4/配置2.jpg" alt><h2><span id="3-检查x2fdev下面是否存在摄像头设备">3、检查&#x2F;dev下面是否存在摄像头设备</span></h2><p>重启完之后，我们的基本的操作就完成了，下来来看看&#x2F;dev下面是否存在摄像头设备的问题：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">ls</span> -al /dev/ | grep video</span><br></pre></td></tr></table></figure><img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BECSI%E6%91%84%E5%83%8F%E5%A4%B4%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%B8%8E%E5%B8%B8%E7%94%A8%E6%8C%87%E4%BB%A4/检查.jpg" alt><h1><span id="使用操作树莓派的摄像头">使用操作树莓派的摄像头</span></h1><p>下面简单的使用操作树莓派的摄像头：<br>1、我们使用rapistill指令来截图</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">raspistill -<span class="keyword">o</span> image.jpg</span><br></pre></td></tr></table></figure><p>raspistill命令的相关参数和实验的具体效果：<br>-v：调试信息查看<br>-w：图像宽度<br>-h：图像高度<br>-rot：图像旋转角度，只支持 0、90、180、270 度（这里说明一下，测试发现其他角度的输入都会被转换到这四个角度之上）<br>-o：图像输出地址，例如image.jpg，如果文件名为“-”，将输出发送至标准输出设备<br>-t：获取图像前等待时间，默认为5000，即5秒<br>-tl：多久执行一次图像抓取<br>使用raspivid指令来生成.h246的文件</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">raspivid -<span class="keyword">o</span> mykeychain.h264 -t <span class="number">10000</span> -<span class="keyword">w</span> <span class="number">1280</span> -h <span class="number">720</span> </span><br></pre></td></tr></table></figure><p>如果你想改变拍摄时长，只要通过 “-t” 选项来设置你想要的长度就行了（单位是毫秒）。<br>如果你想改变图像的分辨率，使用 “-w” 和 “-h” 选项将分辨率降为 1280x720等等。</p>]]></content>
    
    
    <summary type="html">入门介绍</summary>
    
    
    
    <category term="树莓派" scheme="https://values.keys.moe/categories/%E6%A0%91%E8%8E%93%E6%B4%BE/"/>
    
    
    <category term="树莓派" scheme="https://values.keys.moe/tags/%E6%A0%91%E8%8E%93%E6%B4%BE/"/>
    
    <category term="Python" scheme="https://values.keys.moe/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>树莓派常见问题与解决方案</title>
    <link href="https://values.keys.moe/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%E4%B8%8E%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/"/>
    <id>https://values.keys.moe/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%E4%B8%8E%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/</id>
    <published>2019-03-02T01:26:05.000Z</published>
    <updated>2022-09-28T06:28:27.634Z</updated>
    
    <content type="html"><![CDATA[<p>又是老问题？（笑</p><span id="more"></span><h1><span id="树莓派有密码的网络联网失败的问题与解决">树莓派有密码的网络联网失败的问题与解决</span></h1><p><strong>在保证系统无任何修改的情况下：</strong><br>树莓派出现无法连接有密码的网络，而可以连接无密码网络的情况。<br>问题出现原因：<strong>电池电源电流供应不足</strong>。<br>解决方案：给树莓派micrioUSB处通以<strong>正常电源</strong>，问题得以解决。</p><h1><span id="修改树莓派热点的名称和密码">修改树莓派热点的名称和密码</span></h1><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo nano /etc/hostapd/hostapd.<span class="keyword">conf</span> </span><br></pre></td></tr></table></figure><p>其中ssid为热点的名称、wpa_passphrase为热点的密码<br><img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%E4%B8%8E%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/修改树莓派热点的名称和密码.jpg" alt></p><h1><span id="树莓派更换国内可用镜像源">树莓派更换国内可用镜像源</span></h1><p>***最新系统不用折腾换源了，亲测官方源可以使用。</p><p><strong>查看树莓派的镜像列表</strong><br><a href="http://www.raspbian.org/RaspbianMirrors">http://www.raspbian.org/RaspbianMirrors</a><br>操作</p><h2><span id="1-编辑sourceslist">1. 编辑sources.list</span></h2><p>打开终端 输入</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo nano /etc/apt/sources.<span class="keyword">list</span></span><br></pre></td></tr></table></figure><p>用#注释或直接删除原有的内容，新增两条：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">deb</span> http://mirrors.tuna.tsinghua.edu.<span class="keyword">cn</span>/raspbian/raspbian/ stretch main contrib non-free rpi</span><br><span class="line">#deb-src http://mirrors.tuna.tsinghua.edu.<span class="keyword">cn</span>/raspbian/raspbian/ stretch main contrib non-free rpi</span><br></pre></td></tr></table></figure><p>ctrl+x 保存并退出。</p><h2><span id="2-编辑raspilist">2. 编辑raspi.list</span></h2><p>sudo nano &#x2F;etc&#x2F;apt&#x2F;sources.list.d&#x2F;raspi.list<br>用#注释或直接删除原有的内容，新增两条：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">deb</span> http://mirror.tuna.tsinghua.edu.<span class="keyword">cn</span>/raspberrypi/ stretch main ui</span><br><span class="line">#deb-src http://mirror.tuna.tsinghua.edu.<span class="keyword">cn</span>/raspberrypi/ stretch main ui</span><br></pre></td></tr></table></figure><p>ctrl+x 保存并退出。</p><h2><span id="3-更新软件源列表">3. 更新软件源列表</span></h2><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-<span class="built_in">get</span> <span class="keyword">update</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;又是老问题？（笑&lt;/p&gt;</summary>
    
    
    
    <category term="树莓派" scheme="https://values.keys.moe/categories/%E6%A0%91%E8%8E%93%E6%B4%BE/"/>
    
    
    <category term="树莓派" scheme="https://values.keys.moe/tags/%E6%A0%91%E8%8E%93%E6%B4%BE/"/>
    
  </entry>
  
  <entry>
    <title>树莓派空气质量检测仪-攀藤G5003ST的连接与使用</title>
    <link href="https://values.keys.moe/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%E6%A3%80%E6%B5%8B%E4%BB%AA-%E6%94%80%E8%97%A4G5003ST%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%B8%8E%E4%BD%BF%E7%94%A8/"/>
    <id>https://values.keys.moe/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%E6%A3%80%E6%B5%8B%E4%BB%AA-%E6%94%80%E8%97%A4G5003ST%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%B8%8E%E4%BD%BF%E7%94%A8/</id>
    <published>2019-03-02T01:23:34.000Z</published>
    <updated>2022-09-28T05:44:45.667Z</updated>
    
    <content type="html"><![CDATA[<img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%E6%A3%80%E6%B5%8B%E4%BB%AA-%E6%94%80%E8%97%A4G5003ST%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%B8%8E%E4%BD%BF%E7%94%A8/5003ST正.jpg" alt><span id="more"></span><img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%E6%A3%80%E6%B5%8B%E4%BB%AA-%E6%94%80%E8%97%A4G5003ST%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%B8%8E%E4%BD%BF%E7%94%A8/5003ST反.jpg" alt><h1><span id="攀藤g5003st数字接口定义">攀藤G5003ST数字接口定义</span></h1><p><strong>pin1(VCC)</strong>:连接电压为3.3V&#x2F;5V电压<br><strong>pin2(GND)</strong> 电源-<br>**pin3(SET待机位置)**，设置管脚?&#x2F;TTL电平@3.3V，高电平或悬空为正常工作状态，低电平为休眠状态，该引脚可悬空。<br><strong>pin4(RXD串口接收管脚)</strong> 传感器接收来自树莓派的信号数据，如果不需要可以悬空<br><strong>pin5(TXD串口发送管脚)</strong> 传感器将信号发送给树莓派<br><strong>pin6(RESET)</strong> 模块复位信号&#x2F;TTL电平@3.3V，低复位，如果不需要使用可以悬空<br><strong>pin7(NC)</strong> : No internal connection. 无内部连接，不需连接。<br><strong>Pin8(NC&#x2F;PWM)</strong> : PWM周期为1s，其中低电平对应大气环境下的PM2.5质量浓度数据，每1ms低电平代表1ug&#x2F;m?。例如低电平时间长度为210ms，则代表此时PM2.5质量浓度值（大气环境）为210ug&#x2F;m?<br><strong>（pin7和pin8为程序内部调试使用，应用电路中应该使其悬空）</strong></p><p>树莓派Pi3的UART（ttyAMA0）是被蓝牙默认占用的，更改起来十分困难，在实体机上尝试多次无果后决定使用USB TO TTL转接口，直接将PMS5003ST接至树莓派的USB接口上，这样可以直接在&#x2F;dev&#x2F;tty中直接检索到USB0，即为传感器。</p><img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%E6%A3%80%E6%B5%8B%E4%BB%AA-%E6%94%80%E8%97%A4G5003ST%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%B8%8E%E4%BD%BF%E7%94%A8/接线.jpg" alt><img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%E6%A3%80%E6%B5%8B%E4%BB%AA-%E6%94%80%E8%97%A4G5003ST%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%B8%8E%E4%BD%BF%E7%94%A8/接线2.jpg" alt><img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%E6%A3%80%E6%B5%8B%E4%BB%AA-%E6%94%80%E8%97%A4G5003ST%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%B8%8E%E4%BD%BF%E7%94%A8/pin口.png" alt><h1><span id="攀藤g5003st技术指标">攀藤G5003ST技术指标</span></h1><img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%E6%A3%80%E6%B5%8B%E4%BB%AA-%E6%94%80%E8%97%A4G5003ST%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%B8%8E%E4%BD%BF%E7%94%A8/技术指标1.png" alt><img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%E6%A3%80%E6%B5%8B%E4%BB%AA-%E6%94%80%E8%97%A4G5003ST%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%B8%8E%E4%BD%BF%E7%94%A8/技术指标2.png" alt><img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%E6%A3%80%E6%B5%8B%E4%BB%AA-%E6%94%80%E8%97%A4G5003ST%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%B8%8E%E4%BD%BF%E7%94%A8/技术指标3.png" alt><img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%E6%A3%80%E6%B5%8B%E4%BB%AA-%E6%94%80%E8%97%A4G5003ST%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%B8%8E%E4%BD%BF%E7%94%A8/技术指标4.png" alt><h1><span id="攀藤g5003st输出结果分析">攀藤G5003ST输出结果分析</span></h1><p>1.颗粒物浓度：主要输出为单位体积内各浓度颗粒物质量以及个数，其中颗粒物个数的单位体积为0.1升，质量浓度单位为：微克&#x2F;立方米（μg&#x2F;m3）。<br>此外传感器输出分为主动输出和被动输出两种状态。<br>传感器上电后默认状态为主动输出，即传感器主动向主机发送串行数据，时间间隔为200 ~ 800ms，空气中颗粒物浓度越高，时间间隔越短。<br>主动输出又分为两种模式：平稳模式和快速模式。<br>在空气中颗粒物浓度变化较小时，传感器输出为平稳模式，即每三次输出同样的一组数值，实际数据更新周期约为2s。<br>当空气中颗粒物浓度变化较大时，传感器输出自动切换为快速模式，每次输出都是新的数值，实际数据更新周期为200~800ms。<br>PWM输出：PMS3XXXP系列产品带有PWM输出，PWM周期为1秒，低电平时间长度代表PM2.5浓度（大气环境下），每1ms低电平代表1ug&#x2F;m3。<br>例如：低电平时间长度为210ms，则代表此时PM2.5质量浓度值（大气环境）为210ug&#x2F;m3<br>2.甲醛浓度输出：单位体积内甲醛质量浓度，单位为毫克&#x2F;立方米<br>3.温湿度输出：输出吸入传感器内部的采样空气温度及湿度。</p><h1><span id="攀藤g5003st对外输出格式">攀藤G5003ST对外输出格式</span></h1><img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%E6%A3%80%E6%B5%8B%E4%BB%AA-%E6%94%80%E8%97%A4G5003ST%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%B8%8E%E4%BD%BF%E7%94%A8/输出格式0.png" alt><img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%E6%A3%80%E6%B5%8B%E4%BB%AA-%E6%94%80%E8%97%A4G5003ST%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%B8%8E%E4%BD%BF%E7%94%A8/输出格式1.png" alt><img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%E6%A3%80%E6%B5%8B%E4%BB%AA-%E6%94%80%E8%97%A4G5003ST%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%B8%8E%E4%BD%BF%E7%94%A8/输出格式2.png" alt><img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%E6%A3%80%E6%B5%8B%E4%BB%AA-%E6%94%80%E8%97%A4G5003ST%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%B8%8E%E4%BD%BF%E7%94%A8/输出格式3.png" alt><h1><span id="python程序实现">python程序实现</span></h1><p>读取数据：<br><img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%E6%A3%80%E6%B5%8B%E4%BB%AA-%E6%94%80%E8%97%A4G5003ST%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%B8%8E%E4%BD%BF%E7%94%A8/读取数据.png" alt></p><p>分析数据：<br><img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%E6%A3%80%E6%B5%8B%E4%BB%AA-%E6%94%80%E8%97%A4G5003ST%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%B8%8E%E4%BD%BF%E7%94%A8/分析数据.png" alt></p><p>得出数据结论：<br><img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%E6%A3%80%E6%B5%8B%E4%BB%AA-%E6%94%80%E8%97%A4G5003ST%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%B8%8E%E4%BD%BF%E7%94%A8/数据结论.png" alt></p>]]></content>
    
    
    <summary type="html">入门介绍</summary>
    
    
    
    <category term="树莓派" scheme="https://values.keys.moe/categories/%E6%A0%91%E8%8E%93%E6%B4%BE/"/>
    
    
    <category term="树莓派" scheme="https://values.keys.moe/tags/%E6%A0%91%E8%8E%93%E6%B4%BE/"/>
    
    <category term="Python" scheme="https://values.keys.moe/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>使用lirc红外控制树莓派</title>
    <link href="https://values.keys.moe/2019/03/02/%E4%BD%BF%E7%94%A8lirc%E7%BA%A2%E5%A4%96%E6%8E%A7%E5%88%B6%E6%A0%91%E8%8E%93%E6%B4%BE/"/>
    <id>https://values.keys.moe/2019/03/02/%E4%BD%BF%E7%94%A8lirc%E7%BA%A2%E5%A4%96%E6%8E%A7%E5%88%B6%E6%A0%91%E8%8E%93%E6%B4%BE/</id>
    <published>2019-03-02T01:20:33.000Z</published>
    <updated>2022-09-28T06:09:54.964Z</updated>
    
    <content type="html"><![CDATA[<img src="/2019/03/02/%E4%BD%BF%E7%94%A8lirc%E7%BA%A2%E5%A4%96%E6%8E%A7%E5%88%B6%E6%A0%91%E8%8E%93%E6%B4%BE/红外传感器.png" alt><span id="more"></span><p>红外传感器<br>引脚从上到下分别为<br>IO GND VCC</p><h1><span id="lirc的安装与使用">lirc的安装与使用</span></h1><p>使用红外，首先需要安装树莓派的lirc模块<br>LIRC (Linux Infrared remote control)是一个linux系统下开源的软件包。这个软件可以让Linux系统接收及发送红外线信号。<br>注意事项：<br><strong>安装：</strong></p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-<span class="built_in">get</span> install lirc</span><br></pre></td></tr></table></figure><p>修改以下几处：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo leafpad /etc/lirc/hardware.<span class="keyword">conf</span></span><br><span class="line">LIRCD_ATGS=<span class="string">&quot;&quot;</span></span><br><span class="line">DRIVER=<span class="string">&quot;default&quot;</span></span><br><span class="line">DEVICE=<span class="string">&quot;/dev/lirc0&quot;</span></span><br><span class="line">MODULES=<span class="comment">&quot;lirc-rpi</span></span><br></pre></td></tr></table></figure><p>终端执行</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo leafpad /etc/modules</span><br></pre></td></tr></table></figure><p>添加下面两行到模块配置文件：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">lirc-dev</span><br><span class="line"><span class="comment">#红外接收模块的OUT接口接到了树莓派的GPIO18</span></span><br><span class="line"><span class="comment">#因为本例中未用到红外发射模块，所以后面的gpio_out_pin可以不写</span></span><br><span class="line">lirc-rpi gpio_in_pin=<span class="number">18</span> gpio_out_pin=<span class="number">17</span></span><br></pre></td></tr></table></figure><p><strong>如测试时报错-ERROR: could not insert ‘lirc_rpi’: No&amp;nbs</strong><br>解决办法：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo <span class="keyword">vi</span> /boot/config.txt</span><br></pre></td></tr></table></figure><p>找到：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">#dtoverlay=lirc-rpi</span><br></pre></td></tr></table></figure><p>把前面的“#”号去掉， 然后重启系统即可</p><p><strong>测试红外线接收功能</strong><br>首先关闭lirc软件，然后执行如下命令：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo /etc/init.d/lirc <span class="keyword">stop</span></span><br><span class="line">mode2 -d /dev/lirc0</span><br></pre></td></tr></table></figure><p>这时候提示</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">no</span> such <span class="keyword">file</span> <span class="built_in">or</span> directory</span><br></pre></td></tr></table></figure><p>经过查看发现 &#x2F;dev 下面没有 lirc0 这个module，发现在&#x2F;boot&#x2F;config.txt里面dtoverlay&#x3D;lirc-rpi<br>取消注释，然后reboot，问题解决。</p><p>再次执行</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mode2 -d /dev/lirc0</span><br></pre></td></tr></table></figure><p>如果弹出</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Partial <span class="keyword">read</span> <span class="number">8</span> bytes <span class="keyword">on</span> /dev/lirc0pi@raspberrypi:~ $</span><br></pre></td></tr></table></figure><p>发生错误，解决方案：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">changed the following two lines in </span><br><span class="line">    /etc/lirc/lirc_options.<span class="keyword">conf</span></span><br><span class="line">driver = default</span><br></pre></td></tr></table></figure><p>（尝试过程中第一次仍然无效，但是第二次重装系统后正常，目前未知原理）<br>如果显示下面内容</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">pulse <span class="number">629</span></span><br><span class="line">space <span class="number">518</span></span><br><span class="line">pulse <span class="number">627</span></span><br><span class="line">space <span class="number">523</span></span><br><span class="line">pulse <span class="number">628</span></span><br><span class="line">space <span class="number">523</span></span><br><span class="line">pulse <span class="number">631</span></span><br><span class="line">space <span class="number">517</span></span><br><span class="line">pulse <span class="number">629</span></span><br></pre></td></tr></table></figure><p>则说明接收正常.</p><p>#协议<br>采用脉宽调制的串行码，以脉宽为0.565ms、间隔0.56ms、周期为1.125ms的组合表示二进制的”0”；以脉宽为0.565ms、间隔1.685ms、周期为2.25ms的组合表示二进制的”1<br><img src="/2019/03/02/%E4%BD%BF%E7%94%A8lirc%E7%BA%A2%E5%A4%96%E6%8E%A7%E5%88%B6%E6%A0%91%E8%8E%93%E6%B4%BE/协议.png" alt><br>协议：<br>上述“0”和“1”组成的32位二进制码经38kHz的载频进行二次调制以提高发射效率，达到降低电源功耗的目的。然后再通过红外发射二极管产生红外线向空间发射，如下图。<br><img src="/2019/03/02/%E4%BD%BF%E7%94%A8lirc%E7%BA%A2%E5%A4%96%E6%8E%A7%E5%88%B6%E6%A0%91%E8%8E%93%E6%B4%BE/协议2.png" alt><br>|    引导码    |  用户识别码   |用户识别码反码 |   操作码    |  操作码反码   |<br>一个命令只发送一次，即使遥控器上的按键一直按着。但是会每110mS发送一次代码，直到遥控器按键释放。</p><p>重复码比较简单：一个9mS的AGC脉冲、2.25mS间隔、560uS脉冲。<br><img src="/2019/03/02/%E4%BD%BF%E7%94%A8lirc%E7%BA%A2%E5%A4%96%E6%8E%A7%E5%88%B6%E6%A0%91%E8%8E%93%E6%B4%BE/协议3.png" alt><br><img src="/2019/03/02/%E4%BD%BF%E7%94%A8lirc%E7%BA%A2%E5%A4%96%E6%8E%A7%E5%88%B6%E6%A0%91%E8%8E%93%E6%B4%BE/协议4.png" alt></p><h1><span id="读取并校验接收-对应到的红外信号">读取并校验接收、对应到的红外信号</span></h1><img src="/2019/03/02/%E4%BD%BF%E7%94%A8lirc%E7%BA%A2%E5%A4%96%E6%8E%A7%E5%88%B6%E6%A0%91%E8%8E%93%E6%B4%BE/代码1.png" alt>]]></content>
    
    
    <summary type="html">入门介绍</summary>
    
    
    
    <category term="树莓派" scheme="https://values.keys.moe/categories/%E6%A0%91%E8%8E%93%E6%B4%BE/"/>
    
    
    <category term="树莓派" scheme="https://values.keys.moe/tags/%E6%A0%91%E8%8E%93%E6%B4%BE/"/>
    
    <category term="Python" scheme="https://values.keys.moe/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>树莓派GPIO控制的初级应用-多色二极管的亮度调节与颜色变化</title>
    <link href="https://values.keys.moe/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BEGPIO%E6%8E%A7%E5%88%B6%E7%9A%84%E5%88%9D%E7%BA%A7%E5%BA%94%E7%94%A8-%E5%A4%9A%E8%89%B2%E4%BA%8C%E6%9E%81%E7%AE%A1%E7%9A%84%E4%BA%AE%E5%BA%A6%E8%B0%83%E8%8A%82%E4%B8%8E%E9%A2%9C%E8%89%B2%E5%8F%98%E5%8C%96/"/>
    <id>https://values.keys.moe/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BEGPIO%E6%8E%A7%E5%88%B6%E7%9A%84%E5%88%9D%E7%BA%A7%E5%BA%94%E7%94%A8-%E5%A4%9A%E8%89%B2%E4%BA%8C%E6%9E%81%E7%AE%A1%E7%9A%84%E4%BA%AE%E5%BA%A6%E8%B0%83%E8%8A%82%E4%B8%8E%E9%A2%9C%E8%89%B2%E5%8F%98%E5%8C%96/</id>
    <published>2019-03-02T01:16:06.000Z</published>
    <updated>2022-09-28T05:55:12.340Z</updated>
    
    <content type="html"><![CDATA[<img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BEGPIO%E6%8E%A7%E5%88%B6%E7%9A%84%E5%88%9D%E7%BA%A7%E5%BA%94%E7%94%A8-%E5%A4%9A%E8%89%B2%E4%BA%8C%E6%9E%81%E7%AE%A1%E7%9A%84%E4%BA%AE%E5%BA%A6%E8%B0%83%E8%8A%82%E4%B8%8E%E9%A2%9C%E8%89%B2%E5%8F%98%E5%8C%96/多色二极管.png" alt><span id="more"></span><p>引脚R：控制红色二极管亮&#x2F;灭<br>引脚G：控制绿色二极管亮&#x2F;灭<br>引脚B：控制蓝色二极管亮&#x2F;灭<br>GND：接地</p><p>在这里我们令各个引脚：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">R=<span class="number">13</span> </span><br><span class="line">G=<span class="number">26</span> </span><br><span class="line">B=<span class="number">16</span></span><br></pre></td></tr></table></figure><p>初始化各个引脚</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">RPi.GPIO.setup(R,RPi.GPIO.OUT)</span><br><span class="line">RPi.GPIO.setup(G,RPi.GPIO.OUT)</span><br><span class="line">RPi.GPIO.setup(B,RPi.GPIO.OUT)</span><br></pre></td></tr></table></figure><p>初始化脉宽调制为最大并启动</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">pwmR = RPi.GPIO.PWM(R,<span class="number">100</span>)</span><br><span class="line">pwmG = RPi.GPIO.PWM(G,<span class="number">100</span>)</span><br><span class="line">pwmB = RPi.GPIO.PWM(B,<span class="number">100</span>)</span><br><span class="line">pwmR.start(<span class="number">0</span>)</span><br><span class="line">pwmG.start(<span class="number">0</span>)</span><br><span class="line">pwmB.start(<span class="number">0</span>)</span><br></pre></td></tr></table></figure><p>以红色灯为例</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">红灯亮：RPi.GPIO.output(R,<span class="literal">True</span>)</span><br><span class="line">红灯灭：RPi.GPIO.output(R,<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><p>亮度调节：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">红灯<span class="number">1</span>级亮度：pwmR.ChangeDutyCycle(blightness[count1])  <span class="comment"># blightness[count1]=1</span></span><br><span class="line">红灯<span class="number">2</span>级亮度：pwmR.ChangeDutyCycle(blightness[count1])  <span class="comment"># blightness[count1]=20</span></span><br><span class="line">红灯<span class="number">3</span>级亮度：pwmR.ChangeDutyCycle(blightness[count1])  <span class="comment"># blightness[count1]=50</span></span><br><span class="line">红灯<span class="number">4</span>级亮度：pwmR.ChangeDutyCycle(blightness[count1])  <span class="comment"># blightness[count1]=100</span></span><br></pre></td></tr></table></figure><p>颜色调节：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">红灯：红色二极管亮</span><br><span class="line">蓝灯：蓝色二极管亮</span><br><span class="line">绿灯：绿色二极管亮</span><br><span class="line">黄色灯：红色、绿色二极管同时亮</span><br><span class="line">白色灯：红色、蓝色、绿色灯同时亮</span><br></pre></td></tr></table></figure><p>同时控制亮度和颜色方法：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">当接收到红外信号时，内部计数器+<span class="number">1</span> 并对<span class="number">25</span>求余</span><br><span class="line">当计数器为<span class="number">4</span> <span class="number">9</span> <span class="number">14</span> <span class="number">19</span> <span class="number">24</span>时，表示灯熄灭</span><br><span class="line"><span class="number">0</span>~<span class="number">3</span>时表示红灯，具体计数器数值表示亮度对应的字典下标</span><br><span class="line"><span class="number">5</span>~<span class="number">8</span>时表示绿灯，具体计数器数值表示亮度对应的字典下标</span><br><span class="line"><span class="number">10</span>~<span class="number">13</span>时表示蓝灯，具体计数器数值表示亮度对应的字典下标</span><br><span class="line"><span class="number">15</span>~<span class="number">18</span>时表示黄灯，具体计数器数值表示红灯和绿灯亮度对应的字典下标</span><br><span class="line"><span class="number">20</span>~<span class="number">23</span>时表示白灯，具体计数器数值表示红灯、绿灯和蓝灯亮度对应的字典下标</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">基于树莓派GPIO控制的介绍</summary>
    
    
    
    <category term="树莓派" scheme="https://values.keys.moe/categories/%E6%A0%91%E8%8E%93%E6%B4%BE/"/>
    
    
    <category term="树莓派" scheme="https://values.keys.moe/tags/%E6%A0%91%E8%8E%93%E6%B4%BE/"/>
    
    <category term="Python" scheme="https://values.keys.moe/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>树莓派引脚介绍与GPIO的初步认识与应用</title>
    <link href="https://values.keys.moe/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E5%BC%95%E8%84%9A%E4%BB%8B%E7%BB%8D%E4%B8%8EGPIO%E7%9A%84%E5%88%9D%E6%AD%A5%E8%AE%A4%E8%AF%86%E4%B8%8E%E5%BA%94%E7%94%A8/"/>
    <id>https://values.keys.moe/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E5%BC%95%E8%84%9A%E4%BB%8B%E7%BB%8D%E4%B8%8EGPIO%E7%9A%84%E5%88%9D%E6%AD%A5%E8%AE%A4%E8%AF%86%E4%B8%8E%E5%BA%94%E7%94%A8/</id>
    <published>2019-03-02T01:03:20.000Z</published>
    <updated>2022-09-27T16:58:33.957Z</updated>
    
    <content type="html"><![CDATA[<p>​下图所示为树莓派3b+开发板。上方引脚处左下角引脚为1号引脚。</p><img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E5%BC%95%E8%84%9A%E4%BB%8B%E7%BB%8D%E4%B8%8EGPIO%E7%9A%84%E5%88%9D%E6%AD%A5%E8%AE%A4%E8%AF%86%E4%B8%8E%E5%BA%94%E7%94%A8/树莓派.png" alt style="zoom:10%;"><span id="more"></span><h1><span id="一认识gpio">一．认识GPIO</span></h1><p>所谓GPIO，就是“通用输入&#x2F;输出”接口，树莓派系统中已经编译自带了GPIO的驱动。</p><p><strong>树莓派GPIO的编号方式：</strong></p><img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E5%BC%95%E8%84%9A%E4%BB%8B%E7%BB%8D%E4%B8%8EGPIO%E7%9A%84%E5%88%9D%E6%AD%A5%E8%AE%A4%E8%AF%86%E4%B8%8E%E5%BA%94%E7%94%A8/引脚.png" alt><p><strong>1.功能物理引脚（physical）：</strong><br>从左到右，从上到下。左边为奇数，右边为偶数。共计40个引脚，计数为1-40。<br><strong>2.BCM：</strong><br>编号侧重于CPU寄存器，根据BCM2835的GPIO寄存器编号。具体编号参照上图中BCM一栏。<br><strong>3.wiringPi</strong><br>编号侧重实现逻辑，把GPIO端口从0开始编号，这种编号方便编程，参考上图wPi一栏。</p><p>三种编号的方式均指代的对象相同，只是编码方式不同。</p><p><strong>其中</strong><br>GPIO.**: 通用输入输出接口，GPIO端口，可通过软件分别配置成输入或输出。<br>3.3V&#x2F;5.0V(VCC):提供3.3V&#x2F;5.0V的固定电压<br>0V（GND）：接地<br>SDA<em>：SDA 是I2C 数据传输口。<br>SCL：I2C时钟信号。<br>RXD：接收数据的引脚。<br>TXD：发送数据的引脚。<br>MOSI：为主输出从输入。<br>MISO：为主输入从输出。<br>SCLK: 系统时钟,指晶振频率。<br>CE</em>：片选（芯片有效）-表示低电平有效</p><h1><span id="二python-gpio">二．Python GPIO</span></h1><p>默认的python GPIO均已集成入raspbian系统，不需要另外安装。<br>如果需要安装，请按以下顺序：<br>1、先安装python-dev，输入以下指令。<br>      sudo apt-get install python-dev<br>2、安装RPi.GPIO，依次输入以下指令。<br>1)下载：wget <a href="http://raspberry-gpio-python.googlecode.com/files/RPi.GPIO-0.5.3a.tar.gz">http://raspberry-gpio-python.googlecode.com/files/RPi.GPIO-0.5.3a.tar.gz</a><br>2)解压缩：tar xvzf RPi.GPIO-0.5.3a.tar.gz<br>3)进入解压之后的目录： cd RPi.GPIO-0.5.3a<br>4)启动安装 ： sudo python setup.py install</p><h1><span id="三应用">三．应用</span></h1><p><strong>导入模块</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">import RPi.GPIO as GPIO</span><br></pre></td></tr></table></figure><p><strong>设置引脚引用模式：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">GPIO.setmode(GPIO.BOARD)</span><br><span class="line">#or</span><br><span class="line">GPIO.setmode(GPIO.BCM)</span><br></pre></td></tr></table></figure><p><strong>检测使用的哪种模式可以使用：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mode=GPIO.getmode()</span><br><span class="line">#mode的取值有GPIO.BOARD, GPIO.BCM, None</span><br></pre></td></tr></table></figure><p>以下代码如无特殊说明，均使用GPIO.BOARD引脚映射模式。</p><p><strong>设置引脚方向（输入，输出）:</strong></p><p>如 设置40号引脚为输入方向：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pin = <span class="number">40</span></span><br><span class="line">GPIO.setup(pin,GPIO.IN)</span><br></pre></td></tr></table></figure><p>输出同理：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">GPIO.setup(pin,GPIO.OUT)</span><br><span class="line"><span class="comment">#输出还可以加初始电平：</span></span><br><span class="line">GPIO.setup(pin,GPIO.OUT,initial=GPIO.HIGH)</span><br><span class="line">如果要同时设置多个引脚：</span><br><span class="line"><span class="built_in">list</span>=[<span class="number">11</span>,<span class="number">12</span>]</span><br><span class="line">GPIO.setup(<span class="built_in">list</span>,GPIO.OUT)</span><br></pre></td></tr></table></figure><p>如果要同时设置多个引脚：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">list</span>=[<span class="number">11</span>,<span class="number">12</span>]</span><br><span class="line">GPIO.setup(<span class="built_in">list</span>,GPIO.OUT)</span><br></pre></td></tr></table></figure><p><strong>释放</strong><br>一般来说，程序到达最后都需要释放资源，这个好习惯可以避免偶然损坏树莓派。释放脚本中的使用的引脚：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GPIO.cleanup()</span><br></pre></td></tr></table></figure><p><strong>警告</strong><br>如果RPi.GRIO检测到一个引脚已经被设置成了非默认值，那么你将看到一个警告信息。你可以通过下列代码禁用警告：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GPIO.setwarnings(<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><p>注意，GPIO.cleanup()只会释放掉脚本中使用的GPIO引脚，并会清除设置的引脚编号规则。</p><p><strong>读取</strong><br>我们也常常需要读取引脚的输入状态，获取引脚输入状态如下代码：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">GPIO.input(channel)</span><br><span class="line">#低电平返回0 / GPIO.LOW / False，高电平返回1 / GPIO.HIGH / True。</span><br></pre></td></tr></table></figure><p>如果输入引脚处于悬空状态，引脚的值将是漂动的。<br>换句话说，读取到的值是未知的，因为它并没有被连接到任何的信号上，直到按下一个按钮或开关。<br>由于干扰的影响，输入的值可能会反复的变化。<br>使用如下代码可以解决问题：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">GPIO.setup(channel, GPIO.IN, pull_up_down=GPIO.PUD_UP)  </span><br><span class="line"><span class="comment"># or</span></span><br><span class="line">GPIO.setup(channel, GPIO.IN, pull_up_down=GPIO.PUD_DOWN)</span><br><span class="line"><span class="comment">#需要注意的是，上面的读取代码只是获取当前一瞬间的引脚输入信号。</span></span><br></pre></td></tr></table></figure><p>如果需要实时监控引脚的状态变化，可以有两种办法。<br>最简单原始的方式是每隔一段时间检查输入的信号值，这种方式被称为轮询。<br>如果你的程序读取的时机错误，则很可能会丢失输入信号。<br>轮询是在循环中执行的，这种方式比较占用处理器资源。<br><strong>轮询方式</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">while GPIO.input(channel) == GPIO.LOW:</span><br><span class="line">    time.sleep(0.01)  # wait 10 ms to give CPU chance to do other things</span><br></pre></td></tr></table></figure><p>另一种响应GPIO输入的方式是使用中断（边缘检测），这里的边缘是指信号从高到低的变换（下降沿）或从低到高的变换（上升沿）。<br><strong>边缘检测</strong><br>边缘是指信号状态的改变，从低到高（上升沿）或从高到低（下降沿）。通常情况下，我们更关心于输入状态的该边而不是输入信号的值。这种状态的该边被称为事件。</p><p>wait_for_edge() 函数<br>wait_for_edge()被用于阻止程序的继续执行，直到检测到一个边缘。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">channel = GPIO.wait_for_edge(channel, GPIO_RISING, timeout=<span class="number">5000</span>)</span><br></pre></td></tr></table></figure><p>add_event_detect() 函数<br>该函数对一个引脚进行监听，一旦引脚输入状态发生了改变，调用event_detected()函数会返回true，</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">GPIO.add_event_detect(channel, GPIO.RISING)</span><br><span class="line"><span class="keyword">if</span> GPIO.event_detected(channel):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Button pressed&#x27;</span>)</span><br></pre></td></tr></table></figure><p><strong>RPI.GPIO 模块的脉宽调制（PWM）功能</strong><br>脉宽调制(PWM)是指用微处理器的数字输出来对模拟电路进行控制，是一种对模拟信号电平进行数字编码的方法。在树莓派上，可以通过对GPIO的编程来实现PWM。<br>创建一个 PWM 实例：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">p = GPIO.PWM(channel, frequency)</span><br></pre></td></tr></table></figure><p>启用 PWM：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">p.start(dc)   # dc 代表占空比（范围：0.0 &lt;= dc &lt;= 100.0）</span><br></pre></td></tr></table></figure><p>更改频率：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">p.ChangeFrequency(freq)   <span class="comment"># freq 为设置的新频率，单位为 Hz</span></span><br></pre></td></tr></table></figure><p>更改占空比：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">p.ChangeDutyCycle(dc)  <span class="comment"># 范围：0.0 &lt;= dc &gt;= 100.0</span></span><br></pre></td></tr></table></figure><p>停止 PWM：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">p.stop()</span><br><span class="line"><span class="comment">#注意，如果实例中的变量“p”超出范围，也会导致 PWM 停止。</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">入门介绍</summary>
    
    
    
    <category term="树莓派" scheme="https://values.keys.moe/categories/%E6%A0%91%E8%8E%93%E6%B4%BE/"/>
    
    
    <category term="树莓派" scheme="https://values.keys.moe/tags/%E6%A0%91%E8%8E%93%E6%B4%BE/"/>
    
    <category term="Python" scheme="https://values.keys.moe/tags/Python/"/>
    
  </entry>
  
</feed>
