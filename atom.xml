<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Angelo的代码工坊</title>
  
  <subtitle>I shut my eyes in order to see.</subtitle>
  <link href="https://values.keys.moe/atom.xml" rel="self"/>
  
  <link href="https://values.keys.moe/"/>
  <updated>2022-09-28T07:38:38.409Z</updated>
  <id>https://values.keys.moe/</id>
  
  <author>
    <name>Angelo</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Delving into Data: Effectively Substitute Training for Black-box Attack</title>
    <link href="https://values.keys.moe/2021/12/07/Delving%20into%20Data%20Effectively%20Substitute%20Training%20for%20Black-box%20Attack/"/>
    <id>https://values.keys.moe/2021/12/07/Delving%20into%20Data%20Effectively%20Substitute%20Training%20for%20Black-box%20Attack/</id>
    <published>2021-12-07T10:06:33.000Z</published>
    <updated>2022-09-28T07:38:38.409Z</updated>
    
    <content type="html"><![CDATA[<img src="/2021/12/07/Delving%20into%20Data%20Effectively%20Substitute%20Training%20for%20Black-box%20Attack/封面.png" alt><h1><span id="深入研究数据用于黑盒攻击的有效替代训练">深入研究数据：用于黑盒攻击的有效替代训练</span></h1><h2><span id="摘要">摘要</span></h2><p>​深度模型在处理对抗样本时显示了它们的脆弱性。对于黑盒攻击，在无法访问被攻击模型的架构和权重的情况下，大家广泛聚焦于训练对抗攻击的替代模型的方法。以往的替代训练方法主要是基于真实训练数据或合成数据来窃取目标模型的知识，而没有探索什么样的数据可以进一步提高替代模型和目标模型之间的可转移性。本文中，我们提出了一种新视角的替代训练，聚焦于设计知识窃取过程中使用的数据分布。更具体地说，我们提出了一个多样化数据生成模块来合成具有广泛分布的大规模数据。我们还引入了对抗替代训练策略，以关注分布在决策边界附近的数据。这两个模块的结合可以进一步提高替代模型和目标模型的一致性，从而大大提高了对抗攻击的有效性。大量的实验证明了我们的方法在非定向和定向攻击设置下对最先进的竞争对手的有效性。我们还提供了详细的可视化和分析，以帮助理解我们方法的优势。</p><span id="more"></span><h2><span id="1-介绍">1. 介绍</span></h2><p>​尽管在大多数计算机视觉任务中取得了优异的性能，但深度神经网络（DNN）已被证明容易受到难以察觉的对抗噪声或扰动的影响。对抗样本的存在揭示了将DNN部署到现实世界应用中的重要安全风险。当前社区以白盒与黑盒的设置之分来研究对抗攻击（它们的差异在于是否能完全访问目标攻击模型）。事实上，由于白盒攻击所需的完整目标模型信息在现实世界的部署中是不可用的，本文聚焦于黑盒攻击，其通常只基于目标模型的标签或输出分数来生成对抗样本。通常，黑盒攻击包括基于分数的方法或基于决策的方法。然而，在这些攻击中，需要对目标模型进行雪崩式查询，这仍可能限制它们在现实情况下攻击DNN的可用性。<br>​最近，替代训练的想法在黑盒攻击中得到了广泛的探索。通常情况下，该方法并不是直接生成对抗样本，而是训练一个替代模型，在相同的输入数据的查询下，做出与目标模型类似的预测。在一定数量的查询下，这种方法通常能够根据目标模型学习到替代模型。因此可以对替代模型进行攻击，然后可以转移到目标模型上。<br>​从根本上说，替代模型试图通过给出输入数据和相应的查询标签来从目标模型中获取知识。而关键问题是，输入数据是否来自目标模型的训练数据？假设“是”的话，它确实简化了替代训练。然而，在许多现实世界的视觉任务中，收集真实的输入数据甚至是非平凡的。例如，人物照片和视频的数据受到非常严格的控制，个人数据的隐私在很多国家都受到法律的保护。此外，真实图像是最有效的替代训练数据吗？目标模型的训练数据确实有助于在原始任务上得到一个表现良好的替代模型，但其不能保证攻击从替代模型到目标模型的可转移性。为了提高替代训练中的攻击性能，有必要使替代模型和目标模型之间的决策边界距离最小化，这不仅需要大规模和多样化的训练数据，而且特别需要分布在决策边界附近的数据。</p><p>​为了解决真实数据的局限性并探索替代训练数据的更好分布，我们提出了一种新颖的任务驱动统一框架，该框架仅使用专门设计的生成数据进行替代训练，并实现了高攻击性能。如图所示，与使用目标模型的训练数据进行替代训练相比，多样化的合成数据与对抗样本相结合，将促进替代模型进一步接近目标。更具体地说，在我们的框架中，我们首先提出了一个新颖的多样化数据生成模块（Data Generation module, DDG），该模块将噪声采样与标签嵌入信息相结合来生成多样化的训练数据。这样的分布式生成数据基本可以保证替代模型从目标中学习知识。此外，为进一步促使替代模型具有与目标相似的决策边界，我们提出了对抗替代训练策略（Adversarial Substitute Training strategy, AST），其将对抗样本作为边界数据引入训练过程。总的来说，DDG和AST的联合学习保证了替代模型和目标模型之间的一致性，这大大提高了在没有任何真实数据的情况下进行黑盒攻击的替代训练的成功率。</p><img src="/2021/12/07/Delving%20into%20Data%20Effectively%20Substitute%20Training%20for%20Black-box%20Attack/图1.png" alt><p>图1 将真实数据和合成数据应用于替代训练之间的差异。T&#x2F;S表示目标&#x2F;替代模型，(b)中的蓝色+&#x2F;-表示对抗样本，绿色&#x2F;红色虚线表示决策边界。比较(a)和(b)可以得到，我们的方法生成的合成数据可以训练一个与目标模型具有更相似决策边界的替代模型。</p><p>​本工作的主要贡献总结为：（1）我们首次提出了一种新的基于有效生成的替代训练范式，其通过深入研究输入生成的替代训练数据的本质，来提高无数据黑盒攻击的性能。（2）为了实现这一目标，我们首先提出了一个具有多种不同约束的多样化数据生成模块，以扩大合成数据的分布。然后通过对抗替代训练策略进一步提高替代模型和目标模型之间决策边界的一致性。（3）对四个数据集和一个在线机器学习平台的综合实验和可视化证明了我们的方法相对最先进攻击的有效性。</p><h2><span id="2-方法">2. 方法</span></h2><h3><span id="21-框架概述">2.1. 框架概述</span></h3><p>​我们工作的目标是为黑盒对抗攻击有效地训练替代模型，提出的框架如图所示。它由两个模块组成。多样化数据生成模块（DDG）产生多样化的数据，对抗替代训练策略（AST）进一步模仿目标模型的“行为”。在(a)中，DDG根据随机噪声z(i）和标签索引i的标签嵌入向量e(i)生成数据<img src="/2021/12/07/Delving%20into%20Data%20Effectively%20Substitute%20Training%20for%20Black-box%20Attack/公式1.png" alt>。为了保证合成数据的多样性，生成器G通过三个约束进行训练，即自适应标签归一化生成器、噪声&#x2F;标签重建和类间多样性，这将在后面详细说明。此外，为了确保替代模型S接近目标模型T的决策边界，我们将合成的数据和AST的对抗样本一起输入S以进行(b)中的替代训练。从本质上讲，我们把目标模型T作为分类为M类的黑盒，其中只有标签&#x2F;概率输出可用。师生策略在这里被用于从T学习到S。最后，攻击可以在替代模型上进行，然后转移到目标模型上。</p><img src="/2021/12/07/Delving%20into%20Data%20Effectively%20Substitute%20Training%20for%20Black-box%20Attack/图2.png" alt><p>​图2 整体框架图示。其由多样化数据生成模块（DDG）和对抗替代训练模块（AST）组成。（a）DDG旨在生成具有给定标签的不同数据以训练替代模型。（b）AST利用从当前替代模型生成的对抗样本来推动替代模型模仿目标的边界。</p><h3><span id="22-多样化数据生成">2.2. 多样化数据生成</span></h3><p>​为了合成更好的数据用于替代训练，我们首先提出了一个新颖的多样性数据生成模块（DDG），该模块有三个约束条件来操纵生成的合成图像的多样性。这些约束条件原则上鼓励生成器G为每个不同的类学习相对独立的数据分布，并保持类间差异，从而促进替代模型学习目标模型的知识。</p><p>​<strong>自适应标签归一化生成器</strong>。为了更好地从目标模型中学习，我们需要所有类别的平均分布数据进行替代训练，因此有必要生成标签控制的数据。为了实现这一点，我们充分利用了给定的标签和随机噪声。首先，通过从标准高斯分布和标签i中采样的随机噪声向量<img src="/2021/12/07/Delving%20into%20Data%20Effectively%20Substitute%20Training%20for%20Black-box%20Attack/公式2.png" alt>的输入，我们计算出基于嵌入层的标签嵌入向量<img src="/2021/12/07/Delving%20into%20Data%20Effectively%20Substitute%20Training%20for%20Black-box%20Attack/公式3.png" alt>。这种标签嵌入过程可以将单个离散标签编码为一个连续的可学习向量，它在特征空间中的分布更广，包含更多的表示信息。与GAN不同，我们没有使用真实的图像进行监督，这样的标签嵌入过程对数据的生成至关重要。接下来，我们通过两个全连接层从N维标签嵌入向量e(i)中提取平均值μ(i)和方差σ(i)。之后，将μ(i)和σ(i)加入到所有的反卷积块中，迭代合成具有特定类别条件的图像数据，可以表示为：</p><img src="/2021/12/07/Delving%20into%20Data%20Effectively%20Substitute%20Training%20for%20Black-box%20Attack/公式4.png" alt><p>其中总共有五个反卷积块，t表示反卷积块的数量。在获得最终的ˆx(i)之后，输出生成的数据已经被标签归一化的信息进行了修改。这种自适应标签归一化生成器可以更好地利用输入噪声和标签嵌入向量之间的关系来合成标签控制的数据。</p><p><strong>噪声&#x2F;标签重建</strong>。为了进一步确保生成数据ˆx (i)的多样性，我们引入了一个重建网络R来重建输入噪声和标签嵌入<img src="/2021/12/07/Delving%20into%20Data%20Effectively%20Substitute%20Training%20for%20Black-box%20Attack/公式5.png" alt>。而相应的重建损失可以计算为</p><img src="/2021/12/07/Delving%20into%20Data%20Effectively%20Substitute%20Training%20for%20Black-box%20Attack/公式6.png" alt><p>其中我们使用L1来表示输入z(i)和重建的z(i)r之间的差异。对于标签重建，我们应用函数f(∗)来计算e(i)r和e之间的余弦距离，并由Softmax进一步处理以计算与真实标签i的交叉熵损失。在这种约束下，G可以为每个类别的不同输入噪声向量生成更多样的图像。</p><p><strong>类间多样性</strong>。为了进一步增强不同类别的数据多样性，我们使用余弦相似矩阵来最大化所有合成图像的类间距离。特别地，生成器生成一个MB≪M的不同类别的输入合成数据batch，模型S给出这个batch的输出相似度矩阵<img src="/2021/12/07/Delving%20into%20Data%20Effectively%20Substitute%20Training%20for%20Black-box%20Attack/公式7.png" alt>。请注意，我们有真实的相似性矩阵<img src="/2021/12/07/Delving%20into%20Data%20Effectively%20Substitute%20Training%20for%20Black-box%20Attack/公式8.png" alt>，除了对角线元素被设置为1，其他元素都是0。因此，多样性损失函数Ldiv可以表述为：</p><img src="/2021/12/07/Delving%20into%20Data%20Effectively%20Substitute%20Training%20for%20Black-box%20Attack/公式9.png" alt><p>其中，TRI(∗)被定义为提取相似度矩阵中除对角线元素外的上三角元素的操作。通过这种方式，Ldiv确保合成数据拥有每个类别的独立分布。</p><h3><span id="23-对抗替代训练">2.3. 对抗替代训练</span></h3><p>​在DDG生成多样化的训练数据后，为了获得更好的攻击性能，我们还需要进一步鼓励与目标决策边界更相似的替代模型。众所周知，对抗样本拥有视觉上难以区分的扰动，其会被模型错误的分类。由于扰动相对较小，对抗样本可以看作是决策边界周围的样本。因此，我们提出了一种新颖的对抗替代训练策略（AST），它利用对抗样本进一步推动S的决策边界与T的决策边界相似。更具体地说，对于训练期间的每次迭代，我们的生成器首先通过DDG合成图像。然后我们选择白盒攻击算法来获得基于当前S的合成图像的对抗扰动ε。生成对抗图像的目标函数定义为</p><img src="/2021/12/07/Delving%20into%20Data%20Effectively%20Substitute%20Training%20for%20Black-box%20Attack/公式10.png" alt><p>其中L(·) 表示攻击目标，反映了预测xˆ(i)+ε的标签为iadv的概率或交叉熵。如果考虑非定向攻击，则iadv≠i，否则iadv&#x3D;t，t为目标标签。λ是正则化系数，约束ε∈[0, 1]d将扰动ε限制在有效图像空间内。然后，生成的图像和相应的对抗数据被用来一起更新S。</p><h3><span id="24-损失函数">2.4. 损失函数</span></h3><p>​最后，我们应用基本损失函数来训练替代模型</p><img src="/2021/12/07/Delving%20into%20Data%20Effectively%20Substitute%20Training%20for%20Black-box%20Attack/公式11.png" alt><p>其中Ld测量T和S输出之间的距离，Lc表示生成损失。e-Ld表示Ld的“最小-最大”博弈，CE(·)表示S的预测和输入的真实标签i之间的交叉熵损失。因此，凭借这两个损失函数的交替最小化，替代模型S可以学习模仿目标模型T的输出。在DDG和AST的进一步推动下，通过生成的数据和对抗样本，用于训练S和G的统一替代训练损失LS和生成器损失LG被定义为</p><img src="/2021/12/07/Delving%20into%20Data%20Effectively%20Substitute%20Training%20for%20Black-box%20Attack/公式12.png" alt><p>其中Ladvd的定义与等式中的Ld相同。之前的公式使用对抗样本作为输入测量T和S的输出间的距离，Ladvc被定义为Lc，以约束以对抗样本作为输入的生成，Lrec和Ldiv被用来增强数据多样性。β1、β2和β3是DDG的平衡超参数。总的来说，训练过程如下。</p><img src="/2021/12/07/Delving%20into%20Data%20Effectively%20Substitute%20Training%20for%20Black-box%20Attack/算法1.png" alt><h2><span id="3-实验">3. 实验</span></h2><h3><span id="31-实验设置">3.1. 实验设置</span></h3><p>​<strong>数据集和目标模型</strong>。1) MNIST：攻击模型在AlexNet、VGG-16和ResNet-18上进行预训练。默认的替代模型是具有3个卷积层的网络。2)CIFAR-10：攻击者在AlexNet、VGG-16和 ResNet-18上进行了预训练。默认替代模型是VGG-13。3) CIFAR-100：被攻击者在VGG-19和ResNet50上进行了预训练。默认的替代模型是ResNet-18。4)Tiny Imagenet：被攻击者在ResNet-50上进行了预训练。替代模型是ResNet-34。<br>​<strong>比较对象</strong>。为了验证所提出方法的有效性，我们将我们的攻击结果与无数据黑盒攻击（如DaST）和几种需要真实数据的黑盒攻击（如PBBA和Knockoff）。我们还使用被攻击模型的原始训练数据进行替代训练，并利用ImageNet学习替代模型。<br>​<strong>实现细节</strong>。我们使用Pytorch进行实现。我们利用Adam从头开始训练我们的替代模型、生成器和重建网络，所有权重都使用标准差为0.02的截断正态分布随机初始化。所有网络的初始学习率均设置为0.0001，从第80个epoch开始逐渐降低到0，并在第150个epoch后停止。我们将mini-batch大小设置为500，超参数β1、β2和β3值为1。我们的模型由一个 NVIDIA GeForce GTX 1080Ti GPU训练。我们应用PGD作为在AST和评估期间生成对抗图像的默认方法。我们还使用FGSM、BIM和C&amp;W进行广泛实验的攻击。<br>​<strong>评价指标</strong>。 考虑到 DaST中提出的存在两种不同的场景，即只从目标模型中获取输出标签并很好地访问输出概率，我们将这两种场景命名为基于概率和基于标签。在实验中，我们报告了由替代模型产生的对抗样本攻击目标黑盒模型的攻击成功率（ASRs）。按照DaST的设置，在非目标攻击设置中，我们只在被攻击模型正确分类的图像上生成对抗样本。对于目标攻击，我们只在没有被分类到特定错误标签的图像上生成对抗样本。为了公平比较，在所有的对抗样本过程中，设定扰动ε&#x3D;8。我们对每个测试进行五次，并报告平均结果。</p><h3><span id="32-黑盒攻击结果">3.2. 黑盒攻击结果</span></h3><p>​我们与比较对象手在四个数据集和一个在线机器学习平台上评估我们的方法，包括了定向攻击和非定向攻击设置。如表1、2、3所示，我们在基于概率和基于标签的场景下对每个数据集的多个目标模型进行了广泛的比较。</p><img src="/2021/12/07/Delving%20into%20Data%20Effectively%20Substitute%20Training%20for%20Black-box%20Attack/表1.png" alt>表1 使用概率作为我们的方法和比较对象在多个数据集上的目标模型输出来比较ASRs结果<img src="/2021/12/07/Delving%20into%20Data%20Effectively%20Substitute%20Training%20for%20Black-box%20Attack/表2.png" alt>表2 使用标签作为我们的方法和比较对象在多个数据集上的目标模型输出来比较ASRs结果<img src="/2021/12/07/Delving%20into%20Data%20Effectively%20Substitute%20Training%20for%20Black-box%20Attack/表3.png" alt>表3 比较攻击Microsoft Azure示例模型下我们的方法和比较对象的ASRs结果<p>​<strong>与真实数据进行替代训练进行比较</strong>。这里我们研究了用真实图像进行攻击的替代训练，如表1和表2所示。我们直接使用目标模型或ImageNet的原始训练数据进行替代训练，而不是合成训练。结果显示，真实图像可以让替代模型从目标中学习到部分东西，在分类上可能会有更高的准确率，但与生成的数据相比，攻击强度较弱。我们认为，这个问题是由真实图像的数量和多样性限制造成的，这可能导致替代模型学习和模仿目标模型的失败。因此，我们提出了一种DDG策略来合成大规模和多样化的数据。<br>​<strong>与最新方法的比较</strong>。如表1和表2所示，我们将我们的方法与黑盒攻击进行比较。对于定向攻击和非定向攻击，我们的方法在所有数据集下都取得了比基于概率和基于标签的方案更好的ASRs。此外，与相似的生成法DaST相比，我们的方法很大程度上优于它。这些结果验证了所提出的方法让替代模型更好地接近目标的决策边界和实现无数据黑盒攻击的高ASRs这两个方面的有效性。<br>​<strong>在Microsoft Azure上与比较对象的比较</strong>。为了更好地评估实际应用下的攻击手段能力，我们在Microsoft Azure上进行了在线模型攻击实验。通过以攻击Azure上机器学习教程中的MNIST模型为目标，我们比较了我们的方法和竞争对手之间的结果。表3中显示的结果表明了我们的方法可以在在线模型上获得最好的ASR，这进一步证明了我们的方法在没有攻击先验知识的真实场景下的有效性。</p><h2><span id="4-结论">4. 结论</span></h2><p>​本文重点研究黑盒攻击替代训练的生成数据的分布。它提出了一个统一的替代模型训练框架，包含一个多样化数据生成模块（DDG）和一个对抗替代训练策略（AST）。DDG可以生成标签控制的和多样化的数据来训练替代模型。AST利用对抗样本作为边界数据，使替代模型更好地符合目标的决策边界。大量实验表明该方法可以实现高攻击性能。</p>]]></content>
    
    
    <summary type="html">入门介绍</summary>
    
    
    
    <category term="论文阅读" scheme="https://values.keys.moe/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="论文阅读" scheme="https://values.keys.moe/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
  </entry>
  
  <entry>
    <title>新的故事与旧的回忆 —— 2017~2021 我的本科四年</title>
    <link href="https://values.keys.moe/2021/06/21/%E6%96%B0%E7%9A%84%E6%95%85%E4%BA%8B%E4%B8%8E%E6%97%A7%E7%9A%84%E5%9B%9E%E5%BF%86%20%E2%80%94%E2%80%94%202017~2021%20%E6%88%91%E7%9A%84%E6%9C%AC%E7%A7%91%E5%9B%9B%E5%B9%B4/"/>
    <id>https://values.keys.moe/2021/06/21/%E6%96%B0%E7%9A%84%E6%95%85%E4%BA%8B%E4%B8%8E%E6%97%A7%E7%9A%84%E5%9B%9E%E5%BF%86%20%E2%80%94%E2%80%94%202017~2021%20%E6%88%91%E7%9A%84%E6%9C%AC%E7%A7%91%E5%9B%9B%E5%B9%B4/</id>
    <published>2021-06-20T16:54:45.000Z</published>
    <updated>2022-09-28T06:55:23.539Z</updated>
    
    <content type="html"><![CDATA[<img src="/2021/06/21/%E6%96%B0%E7%9A%84%E6%95%85%E4%BA%8B%E4%B8%8E%E6%97%A7%E7%9A%84%E5%9B%9E%E5%BF%86%20%E2%80%94%E2%80%94%202017~2021%20%E6%88%91%E7%9A%84%E6%9C%AC%E7%A7%91%E5%9B%9B%E5%B9%B4/新的故事与旧的回忆.jpg" alt><span id="more"></span>]]></content>
    
    
    <summary type="html">&lt;img src=&quot;/2021/06/21/%E6%96%B0%E7%9A%84%E6%95%85%E4%BA%8B%E4%B8%8E%E6%97%A7%E7%9A%84%E5%9B%9E%E5%BF%86%20%E2%80%94%E2%80%94%202017~2021%20%E6%88%91%E7%9A%84%E6%9C%AC%E7%A7%91%E5%9B%9B%E5%B9%B4/新的故事与旧的回忆.jpg&quot; alt&gt;</summary>
    
    
    
    <category term="杂谈" scheme="https://values.keys.moe/categories/%E6%9D%82%E8%B0%88/"/>
    
    
    <category term="杂谈" scheme="https://values.keys.moe/tags/%E6%9D%82%E8%B0%88/"/>
    
  </entry>
  
  <entry>
    <title>Prioritize Crowdsourced Test Reports via Deep Screenshot Understanding</title>
    <link href="https://values.keys.moe/2021/04/20/Prioritize%20Crowdsourced%20Test%20Reports%20via%20Deep%20Screenshot%20Understanding/"/>
    <id>https://values.keys.moe/2021/04/20/Prioritize%20Crowdsourced%20Test%20Reports%20via%20Deep%20Screenshot%20Understanding/</id>
    <published>2021-04-20T09:24:05.000Z</published>
    <updated>2022-09-28T08:28:30.927Z</updated>
    
    <content type="html"><![CDATA[<img src="/2021/04/20/Prioritize%20Crowdsourced%20Test%20Reports%20via%20Deep%20Screenshot%20Understanding/title.png" alt><h1><span id="基于深度截图理解的众包测试报告优先级排序">基于深度截图理解的众包测试报告优先级排序</span></h1><h2><span id="摘要">摘要</span></h2><p>​众包测试在移动应用程序测试中日益占据主导地位，但对于应用开发者来说，审查数量过多的测试报告是很大的负担。已有许多学者提出基于文本和简单图片特征的测试报告处理方法。然而，在移动应用测试中，测试报告所包含的文本较为精简且信息不够充分，图片则能够提供更丰富的信息。这一趋势促使我们在深度截图理解的基础上，对众包测试报告的优先级进行排序。<br>​本文中，我们提出了一种新的众包测试报告优先级排序方法，即DEEPPRIOR。我们首先引入一个新的特征来代表众包测试报告，即DEEPFEATURE，它基于对应用程序截图的深度分析，涵盖了所有组件（widget）及它们的文本、坐标、类型甚至是意图。DEEPFEATURE包括直接描述bug的bug特征（Bug Feature），和刻画bug完整上下文的上下文特征（Context Feature）。DEEPFEATURE的相似度用于表示测试报告的相似度，并被用于对众包测试报告进行优先级排序。我们形式上将相似度定义为DEEPSIMILARITY。我们还进行了一个实证实验，以评估所提技术在大型数据中的有效性。结果表明，DEEPPRIOR性能最佳，以不足一半的成本获得优于其它方法的结果。</p><p>索引词 众包测试，移动应用测试，深度截图理解</p><span id="more"></span><h2><span id="i-介绍">I. 介绍</span></h2><p>​众包已成为许多领域的主流技术之一。众包的开放性带来了许多优势。例如，可以在多个不同的实际环境中模拟对众包活动的操作。这样的优势有助于缓解移动应用（app）测试中严重的“碎片化问题”。成千上万种不同品牌、不同操作系统（OS）版本、不同硬件传感器就是安卓测试中众所周知的“碎片化问题”[1]。众包测试是解决这一问题的最佳方案之一。应用开发者可以将它们的应用程序分发给拥有不同移动设备的众包工人，并要求他们提交包含应用截图和文本描述的测试报告。这有助于应用开发者尽可能多的发现问题。<br>​然而，众包测试的报告审查效率低是一个严重的问题。众包的开放性会导致有大量的报告被提交，而几乎82%的提交的报告都是重复的[2]。由于报告的复杂性，自动审查报告是一项艰巨的工作。在文字部分，自然语言的复杂性可能导致歧义，并且众包工人可能使用不同的词来描述相同的对象，或使用相同的词来描述不同的场景。在图像部分，由于许多应用使用相似的UI构建函数，截图的相似度也几乎没有帮助。因此，对于应用程序开发人员来说，及时发现报告中的bug是困难但重要的。<br>​在近期的研究中，测试报告的处理通常分为两部分：应用截图和文本描述。现有的研究分别对这两部分进行分析以提取特征。对于文本描述，现有的方法是提取关键词，并根据预定义的词汇对关键词进行标准化处理。对于应用截图，它们将每个截图作为一个整体，提取用数字向量表示的图像特征。在获得了这两部分的结果之后，目前大多数的研究都以文本为依托，将截图作为补充材料，或者简单地将图像信息和文本信息进行拼接。然而，我们认为这样的处理方式会导致许多有价值的信息丢失。文本描述和应用截图之间的关系会被遗漏，报告的去重与确定优先级效果也可能更差。<br>​在本文中，我们提出了一种新颖的方法，即DEEPPRIOR，通过对截图的深入理解来确定众包测试报告的优先级。DEEPPRIOR详细顾及了对应用截图和文本描述的深入理解。对于一份被提交的测试报告，我们会从截图和文本两者中提取信息。在截图中，我们通过计算机视觉（CV）技术收集所有widget，并根据文本描述定位问题widget（表示为WP）。其余的widget则被视为上下文widget（表示为WC）。本研究以自然语言处理（NLP）技术对文本进行处理，并分为两部分：复现步骤（以R表示）和bug描述（以P表示）。复现步骤被进一步标准化为“操作-对象”序列。bug描述也被进一步处理以提取对问题widget的描述，从而进行WP的定位。<br>​我们不对应用截图和文本描述进行单独处理，而是将它们作为一个整体，并将所有信息收集起来作为报告的DEEPFEATURE。根据bug本身的关联性，DEEPFEATURE包括bug特征（Bug Feature，BFT）和上下文特征（Context Feature，CFT）。bug特征由WP和P组成，它表示报告中揭示的和bug直接相关的信息。上下文特征由WC和R组成，其代表的是上下文信息，包括触发bug的操作轨迹和bug发生时的activity的信息。<br>​将上述特征整合到DEEPFEATURE中后，DEEPPRIOR将计算报告中的DEEPSIMILARITY以进行优先级排序。对于bug特征和上下文特征，我们分别计算DEEPSIMILARITY。<br>​对于bug特征，为了计算报告中的WP的DEEPSIMILARITY，我们利用CV技术对特征点进行提取和匹配。P是一个简短的文本描述，因此我们使用NLP技术，在自建词汇表的基础上提取与bug有关的关键词，比较关键词的频率作为DEEPSIMILARITY。<br>​对于上下文特征，WC被输入到一个预先训练好的深度学习分类器中，以识别每个widget的类型，每个类型的数字向量作为WC DEEPSIMILARITY。R由一系列操作和对应的widget组成，代表了从应用启动到bug发生的序列。因此，我们按照R的顺序，利用NLP技术提取操作和对象。我们将“操作-对象”序列作为行为轨迹，并计算DEEPSIMILARITY。<br>​之后进行优先级排序。我们首先构建一个NULL 报告（如章节III-D中定义），并将其添加到优先级报告池中。之后，我们反复计算每个未确定优先级的报告和报告池中所有报告之间的DEEPSIMILARITY。具有与优先级报告池中相比最低的“最小DEEPSIMILARITY”的报告会被放入优先级报告池中。<br>​我们还设计了一个实证实验，使用一个大型活跃的众包测试平台的大规模数据集组。我们将DEEPPRIOR与其他两种方法进行了比较，结果表明，DEEPPRIOR是有效的。<br>​本文的重要贡献如下：<br>​* 我们提出了一个新颖的方法，通过对截图的深入理解和详细的文本分析，对众包测试报告进行优先级排序。我们从截图中提取所有widget，将文本信息分类到不同的类别，并构建DEEPFEATURE。<br>​* 我们构建了一个用于对截图进行深入理解的集成数据集组，包括大规模的widget图像数据集、大规模的测试报告关键词词汇、大规模的文本分类数据集和大规模的众包测试报告数据集。<br>​* 基于数据集，我们对提出的方法DEEPPRIOR进行了实证评估，结果表明，DEEPPRIOR以不到一半的开销胜过了当前最新的方法。</p><h2><span id="ii-背景与动机">II. 背景与动机</span></h2><p>​众包测试在移动应用测试中得到了广泛的应用，其优点是显而易见的，但其弊端也是不可忽视的。在大多数主流的众测平台上，众测工人都需要提交一份报告来描述自己所遇到的bug。报告的主题是对bug的截图和文本描述。应用的截图和文本描述也同样是对众测报告进行优先级排序的主要依据。<br>​目前考虑截图的众测报告处理方案，如[2][3]主要是分析应用截图特征和文本描述信息来衡量所有报告之间的相似度。虽然他们考虑了应用截图，但只是将图片简单地处理为宽x高xRGB的矩阵。然而，这些方法忽略了丰富而有价值的信息，我们认为应该把应用截图看作是有意义的widget的集合，而非无意义的像素的集合。这是因为在回顾众测报告数据集的时候，我们发现了一些生动的例子，现有的方法难以处理它们，因为这些方法只是做了简单的特征提取，而没有对截图进行深度理解。</p><p>A. 样例1：不同的应用主题<br>        现在的应用都支持不同的主题，用户可以根据自己的喜好定制应用的外观（图1）。此外，“深色模式”使得配色方案更加复杂。图像特征提取算法很难处理这样的复杂问题，会出现错误。从这些样本中我们可以发现，三份报告中的应用截图分别为蓝色、白色和绿色主题。这三份报告都是报告音乐资源文件加载失败的。然而，根据文献[2]，图像颜色特征是报告替代的重要组成部分之一。不同颜色的应用截图将被识别为不同的截图。<br><img src="/2021/04/20/Prioritize%20Crowdsourced%20Test%20Reports%20via%20Deep%20Screenshot%20Understanding/图1.png" alt><br>图1 样例1：不同的应用主题<br><img src="/2021/04/20/Prioritize%20Crowdsourced%20Test%20Reports%20via%20Deep%20Screenshot%20Understanding/图1-1.png" alt></p><p>B. 样例2：相同截图上的不同bug<br>        如图2所示，两份报告使用的是相同的应用activity的截图，图像特征提取算法会在这两张截图之间给出一个较高的相似度。然而，根据bug描述，这两份报告描述的是完全不同的bug。在DEEPPRIOR中，对于报告#1128，我们可以提取出“未找到媒体”的文字；对于报告#1127，除了提示信息之外，我们还可以提取出音量widget，DEEPPRIOR可以识别出不同的问题。<br>        <img src="/2021/04/20/Prioritize%20Crowdsourced%20Test%20Reports%20via%20Deep%20Screenshot%20Understanding/图2.png" alt><br>图2 样例2：相同截图上的不同bug<br><img src="/2021/04/20/Prioritize%20Crowdsourced%20Test%20Reports%20via%20Deep%20Screenshot%20Understanding/图2-2.png" alt></p><p>C. 样例3：不同截图上的相同bug<br>        如图3所示，顶部的ImageView widget的内容不同，且其占据了整个页面的很大比例。并且，由于测试时间不同，评论也不同。因此，现有的方法会认为两张截图的相似度低，即使文本描述的相似度很高，也会降低整体相似度。而通过DEEPPRIOR，我们可以提取底部的弹出信息，为“发表评论失败”，并给两个报告分配一个很高的相似度。这样的弹窗被认为是相当重要的、包含bug的widget。<br><img src="/2021/04/20/Prioritize%20Crowdsourced%20Test%20Reports%20via%20Deep%20Screenshot%20Understanding/图3.png" alt><br>图3 样例3：不同截图上的相同bug<br><img src="/2021/04/20/Prioritize%20Crowdsourced%20Test%20Reports%20via%20Deep%20Screenshot%20Understanding/图3-3.png" alt></p><h2><span id="iii-方法">III. 方法</span></h2><p>​本节介绍DEEPPRIOR的详细内容，即通过深度截图理解，对众测报告进行优先级排序。DEEPPRIOR由4个阶段组成，包括特征提取、特征聚合、DEEPSIMILARITY的计算与报告优先级排序。我们从应用截图和文本描述两者中收集4种不同类型的报告特征。然后，我们将提取的特征汇总成一个DEEPFEATURE，其包括bug特征和上下文特征。基于DEEPFEATURE，我们设计了一个算法来计算每两份测试报告之间的DEEPSIMILARITY。基于预先定义的规则（详见章节III-D），我们根据DEEPSIMILARITY对测试报告进行优先级排序。DEEPPRIOR方法的总体框架可参考图4。<br><img src="/2021/04/20/Prioritize%20Crowdsourced%20Test%20Reports%20via%20Deep%20Screenshot%20Understanding/图4.png" alt></p><p>图4 DeepPrior框架</p><h3><span id="a-特征提取">A. 特征提取</span></h3><p>​第一步也是最重要的一步，为特征提取。我们分别对众测报告的应用截图和文本描述进行分析。<br>​\1) 应用截图中的特征。应用截图在众测报告中至关重要。众测人员需要在bug发生时进行截图，以便更好地阐述bug。如文献[2]所描述，文本描述仅能提供有限的信息，可能不足以清晰地描述bug。因此，除了文本描述之外，还要考虑截图来提供更多的信息。在一张截图中，存在许多不同的widget，有些widget可以提示bug信息。因此，对截图的深度理解主要依赖于widget。在DEEPPRIOR中，我们使用CV技术和深度学习（DL）技术来提取所有的widget并分析其信息。DL技术功能强大，CV技术可以处理更多种类的任务[4]。<br>​问题widget。一个应用activity可以被看作是一个有组织的widget集。一般来说，在众测任务中，众测人员能够发现的bug都会通过widget显露出来。因此，找到引发bug的widget，并将这个widget与其他widget区分开来是很重要的，而该widget就是我们所定义的问题widget（WP）。为了区分问题widget，我们分析文本描述。在众测报告中，众测人员会指出在bug发生前操作了哪个widget。如章节III-A2所示，我们可以从文本描述中提取出问题widget，并且为了定位问题widget，我们针对不同的情况可采取两种不同的策略。<br>​* 如果提取的widget包含文本，我们将widget的截图和文本描述中的文本进行匹配。被匹配的widget被认为是问题widget。<br>​* 如果widget上没有文本或文本匹配失败，我们将提取的widget送入深度神经网络，以识别简单的widget意图。该深度神经网络是在Xiao等人[5]的研究基础上修改而来的。该模型通过卷积神经网络（CNN）将widget截图编码为特征向量。输出是使用循环神经网络（RNN）从特征向量解码而得的短文本片段，该短文本片段描述了widget意图。<br>​上下文widget。除了问题widget以外，应用截图中展现的widget集还包含了更多组成上下文的widget，而它们对深度图像理解也是至关重要的。在早期的调查中，我们发现，即使问题widget、复现步骤（activity启动路径）和bug描述相同，应用的activity也可能全然不同（如章节II-C中的启发性样例）。在这个情况下，上下文widget对于识别差异而言十分重要。因此我们将其余的widget收集为上下文widget（WC）。对于每个上下文widget，我们将widget截图输入卷积神经网络，以识别其类型。每个类型由14维向量构成。<br>​卷积神经网络能够识别14种不同类型的、最为广泛使用的widget，包括Button（BTN）、CheckBox（CHB）、CheckTextView（CTV）、EditText（EDT）、ImageButton（IMB）、ImageView（IMV）、ProgressBarHorizontal（PBH）、ProgressBarVertical（PBV）、RadioButton（RBU）、RatingBar（RBA）、SeekBar（SKB）、Switch（SWC）、Spinner（SPN）、TextView（TXV）。为了训练神经网络，我们收集了36573张均匀分布在这14种类型上的widget截图。训练集、验证集和测试集的比例为7:1:2，这是图像分类任务的常见做法。神经网络由多个卷积层、最大池化层和全连接层组成。采用AdaDelta算法作为优化器，模型采用categorical_crossentropy作为损失函数。<br>​\2) 来自文本描述的特征。除了应用截图，文本描述可以更直观、更直接地提供bug信息。同时，文本描述也可以作为应用截图的正向补充。在DEEPPRIOR中，我们采用NLP技术，特别是DL算法，对测试报告中的文本描述进行处理。<br>​在文本描述中，众测人员需要对截图中的bug进行描述，并提供复现步骤，即从应用启动到bug发生的操作顺序。然而，在大多数众测平台上，bug描述和复现步骤是混在一起的，并且由于专业能力不同，众包工人并不被要求遵循特定的模式[6]。因此，将bug描述与复现步骤区分开是很复杂的。为了解决这个问题，我们采用了TextCNN模型[7]。<br>​TextCNN模型可以通过预先训练的词向量完成句子级分类任务。在将文本输入模型之前，我们对数据进行预处理。将测试报告的文本描述分割成句子，再使用jieba库将句子分割成单词，根据停顿词列表过滤掉停顿词。预处理后，我们将文本送入词向量层。在该层中，利用Word2Vec模型[8]将文本转化为128维的向量。之后，我们采用多个卷积层和最大池化层来提取文本特征。在最后一层，我们使用SoftMax激活函数，得到每个句子是bug描述还是复现步骤的概率。最后，我们将所有分类为bug描述或复现步骤的句子进行合并。为了训练TextCNN模型，我们构建了一个大规模的文本分类数据集，由2252个bug描述和2088个复现步骤组成。我们按照惯例，将训练集、验证集和测试集的比例设置为6:2:2。<br>​bug描述。bug描述总是以短句的形式出现。因此，我们用一个向量来表示句子，其也用Word2Vec模型编码。大多数的bug描述都遵循某种特定的模式，如“对某些widget进行了某些操作，发生了某些非预期的行为”，所以即使具体的词语会有所不同，但提取这种特征仍是有效的。<br>​另一个重要的过程是提取问题widget的描述，以帮助对问题widget进行定位。为了实现这一目标，我们采用基于HMM（Hidden Markov Model，隐马尔科夫模型）模型的文本分割算法[9]，并在文本分割之后分析bug描述的各个部分的词性。然后，我们提取对象部分作为问题widget定位的依据，句子中这样的对象部分就是触发bug的widget。在获取对象之后，我们使用上文所述的策略对问题widget进行定位。<br>​复现步骤。除了bug描述外，文本描述的另一个重要部分是复现步骤。复现步骤是一系列的操作，描述了用户从应用启动到bug发生的操作。对于分类到复现步骤的句子，我们按照报告中的初始顺序进行处理。我们使用相同的NLP算法进行文本分割，并对每个句子的每个文本段进行词性分析。然后，收集操作部分和对象部分，形成“操作-对象”对。然后，我们将“操作-对象”对连接成一个“操作-对象”序列。另外，除了操作词和对象，我们还为一些特定的操作添加一些补充信息。比如，假设有一个操作是键入操作，我们会添加输入内容作为补充信息，因为不同的测试输入可能导致不同的处理结果，使应用定向到不同的activity上。最后，经过形式化处理后，我们就可以从文本描述中获得复现步骤。</p><h3><span id="b-特征聚合">B. 特征聚合</span></h3><p>​在从应用截图和文本描述中获取所有特征后，我们将其汇总为两个特征类别。bug特征（Bug Feature, BFT）和上下文特征（Context Feature, CFT）。bug特征指的是众测报告中直接反映或描述bug的特征，而上下文特征是由bug出现时提供环境描述的特征聚合而成。<br>​\1) bug特征（BFT）：bug特征可以直接提供bug的信息。由于众测报告是由应用截图和文本描述组成，这两部分都包含了发生bug的关键信息。在应用截图中，我们提取了问题widget，是widget截图。DEEPPRIOR可以自动提取这样的信息。在文本描述中，bug描述部分直接描述了bug。因此，在平衡考虑应用截图和文本描述的情况下，我们将问题widget和bug描述汇总为bug特征。<br>​\2) 上下文特征（CFT）：上下文特征包括了为bug的发生构建完整上下文的特征。在应用截图中，上下文widget由问题widget以外的所有widget组成。在文本描述中，之所以考虑到复现步骤信息是因为其提供了从应用程序启动到bug发生时的完整操作路径，可以帮助识别两个测试报告的bug是否在同一个应用activity上。因此，将上下文widget和复现步骤汇总在一起，构成上下文特征。<br>​\3) 特征聚合：借助bug特征和上下文特征，我们可以将众测报告中的应用截图和文本描述中所获得的所有特征聚合到最终的DEEPFEATURE中。我们并非直接将应用截图转化为简单的特征向量，而是对应用截图进行深度理解。同时，我们对应用截图和文本描述之间的结合也更加紧密。此外，我们将应用截图和文本描述作为一个整体，根据它们在bug反馈中的作用进行划分。bug特征非常重要，我们认为上下文特征在众包测试报告的优先级确定中也应起到至关重要的作用，bug相似度的计算很大程度上依赖于整个上下文。</p><h3><span id="c-deepsimilarity的计算">C. DEEPSIMILARITY的计算</span></h3><p>​要对众测报告进行优先级排序，一个主要的步骤就是计算所有报告间的相似度。由于我们是第一个将深度截图理解引入到报告的优先级排序中的,我们将这个相似度命名为DEEPSIMILARITY。之前的研究[2][3]普遍采用合并不同特征的做法，我们分别计算不同特征的DEEPSIMILARITY，并为不同特征的结果分配不同的权重。形式化表达如下：<br>​<img src="/2021/04/20/Prioritize%20Crowdsourced%20Test%20Reports%20via%20Deep%20Screenshot%20Understanding/公式1.png" alt><br>​\1) bug特征：我们分别计算问题widget和bug描述的DEEPSIMILARITY，并使用参数α来整合他们。<br>​问题widget。问题widget是根据章节III-A1中介绍的策略，从应用截图中提取的widget截图。为了计算问题widget的DEEPSIMILARITY，我们提取了widget截图的图像特征。为了提取图像特征，我们采用了最先进的SIFT（Scale-Invariant Feature Transform）算法[10]。因此，每个widget由一个特征点集来表示。SIFT算法的优点是可以处理不同尺寸、位置和旋转角度的图像，而这种图像在这样一个移动设备有成千上万种不同型号的时代是相当普遍的。为了对不同众测报告中的问题widget进行对比和匹配，我们使用FLANN库[11]。经过计算，我们可以得到一个0到1间的分数，0表示完全不同，1表示完全相同。这个分数可以看作是问题widget的DEEPSIMILARITY。<br>​bug描述。bug描述是简短的语句，简要描述了众测报告中的bug。因此，我们使用NLP技术对bug描述进行编码。遵循以往研究中的方法，我们使用Word2Vec模型作为编码器。为了提高Word2Vec模型的性能，我们构建了一个测试报告关键词数据库。测试报告关键词 数据库包含了8647个与软件测试、移动应用、测试报告相关的关键词，包括了标注的同义词、反义词和多义词。编码后的bug描述是一个100维的向量。之后，仍参考前人的研究，如[2][3]，我们采用广泛使用的欧氏度量算法来成对地计算不同测试报告中的bug描述的DEEPSIMILARITY。为了统一不同尺度的数值，我们使用函数 (x-min)&#x2F;(max-min)，以将每个结果x归一化到[0,1]区间，其中max是所有结果的最大值，min是所有结果的最小值。<br>​\2) 上下文特征：我们还分别计算上下文widget和复现步骤的DEEPSIMILARITY，并使用参数β来整合他们。<br>​上下文widget。上下文widget也是bug发生时整个上下文的重要组成部分。为了对应用截图有深度理解，特别是应用截图上的widget，我们使用卷积神经网络来识别每个提取的widget截图的widget类型，并形成一个包含14种widget类型数量的向量。之后，我们使用欧氏度量算法来计算获取的14维向量的距离。我们考虑了每个类型的widget的绝对数量和所有widget的分布情况。欧氏度量算法的结果（从0到1）即上下文widget的DEEPSIMILARITY。<br>​复现步骤。在特征提取过程中，复现步骤被转化为“操作-对象”序列。为了计算“操作-对象”序列的DEEPSIMILARITY，我们采用动态时间规整（Dynamic Time Warping, DTW）算法处理待比较的“操作-对象”序列。DTW算法在自动语音识别方面表现出色。在本文中，我们调整了DTW算法来处理相应的众测报告中触发bug的操作路径。DTW算法可以测量时空序列的相似度，尤其是可能存在“速度”变化的时空序列。具体而言，我们任务中的“速度”是指不同的用户操作可以通过不同的路径到达同一个应用activity的情况。与其他轨迹相似度算法相比，DTW由于可以处理不同长度的序列，所以匹配的效果更好，适合处理“操作-对象”序列。</p><h3><span id="d-报告的优先次序">D. 报告的优先次序</span></h3><p>​在聚合了DEEPFEATURE，并定义了DEEPSIMILARITY的计算规则后，我们开始对众测报告进行优先级排序。首先，我们构建两个空报告池：非优先级报告池和优先级报告池。所有的众测报告最初都会被放入非优先级报告池。<br>​与[3]中采用的随机选择一个报告作为初始报告的策略不同，我们认为应该平等的对待所有报告，随机选择报告可能影响最终的优先级。因此，为了使优先级算法形式化、统一化，我们引入了NULL报告的概念，它也包含了四个特点。<br>​* 问题widget：问题widget的截图本质上是一个三维矩阵，分别代表宽度、高度和三个颜色通道。因此，我们将问题widget构造为一个零矩阵。零矩阵的宽度和高度设置为所有实际众测报告的平均大小。直观地说，它是一个全黑的图像。<br>​* bug描述：NULL报告的bug描述直接设置为空字符串，由于字符串长度为0，显然不包含任何单词，经过Word2Vec处理后，特征向量将是一个100维的全部为“0”的向量。<br>​* 上下文widget：对于NULL报告的上下文widget，我们直接构造出代表数量为14种的不同类型widget的向量，并且所有元素均为0。这表示众包的应用截图上“没有”widget。<br>​* 复现步骤：NULL报告的复现步骤也设置为空字符串，“操作-对象”序列的长度也为0。<br>​优先级划分的主要共识是，在某些报告会重复描述bug的情况下，尽早发现所有的bug[3][13][14]。<br>​因此，要尽早为开发者提供尽可能多的描述不同bug的报告。基于这一思想，我们设计了如下的优先级策略，形式化表达式见算法1。<br>​<img src="/2021/04/20/Prioritize%20Crowdsourced%20Test%20Reports%20via%20Deep%20Screenshot%20Understanding/算法1.png" alt><br>​首先，我们根据上述规则构建NULL报告，并将NULL报告追加到空的优先级报告池中。接下来进行一个迭代的过程。我们计算每个未确定优先级的报告与整个优先级报告池的DEEPSIMILARITY，此DEEPSIMILARITY定义为未确定优先级的报告与优先级报告池中的所有报告的最小DEEPSIMILARITY。与优先级报告池中DEEPSIMILARITY最低的报告将被移至优先报告池中。</p><h2><span id="iv-评估">IV. 评估</span></h2><h3><span id="a-实验设置">A. 实验设置</span></h3><p>​为了评估我们提出的DEEPPRIOR，我们设计了一个实证实验。为了完成实验，我们收集了10个不同移动应用的536份众包测试报告（详见表I）。A1到A10代表10个应用，不同应用的测试报告数量在10到152之间。我们还邀请软件测试专家根据测试报告所描述的bug进行人工分类，平均一个bug类别的报告数量为8.06份。</p><p>表I 实验应用<br><img src="/2021/04/20/Prioritize%20Crowdsourced%20Test%20Reports%20via%20Deep%20Screenshot%20Understanding/表1.png" alt></p><p>​在众包测试报告数据集的基础上，我们建立了3个具体的数据集来更好地支持评估，包括1) 大规模widget图像数据集，2) 大规模测试报告关键词集，3) 大规模文本分类数据集。这4个数据集构成了综合数据集。<br>​我们共设计了三个研究问题（RQ）来评估所提出的测试报告优先级确定方法DEEPPRIOR。</p><p>​* RQ1：DEEPPRIOR如何有效识别从应用截图中提取的widget类型？<br>​* RQ2：DEEPPRIOR对众测报告中的文本描述的分类效果如何？<br>​* RQ3：DEEPPRIOR能多有效地确定众测报告的优先级？</p><h3><span id="b-rq1widget类型分类">B. RQ1：widget类型分类</span></h3><p>​第一个研究问题的设定是评估我们对应用截图的处理效果。在应用截图处理中，最重要的部分是widget的提取和分类。因此，我们评估了widget类型分类的CNN的准确性。我们共收集了36573张不同的widget图像，这些图像在14个类别中均匀分布。<br>​CNN的具体介绍详见章节III-A1。数据集按惯例，按照7:2:1的比例划分为训练集、验证集和测试集。在CNN模型训练完成后，我们对测试集的准确性进行评估。widget类型分类的总体准确率达到89.98%。具体而言，我们用精确率（precision）、召回率（recall）和F值（F-Measure）评估网络。计算公式如下，其中TP表示真正例样本，FP表示假正例样本，TN表示真负例样本，FN表示假负例样本。</p><img src="/2021/04/20/Prioritize%20Crowdsourced%20Test%20Reports%20via%20Deep%20Screenshot%20Understanding/公式23.png" alt><p>​评估结果如表II所示，精确率平均值达90.05%，最低精确率为74.36%，最高为99.81%。对于衡量实际检索到的实例总量的召回率方面，平均值为89.98%，召回率值在70.83%至100%之间。F值是精确率和召回率的调和平均数，其平均值达到89.92%。以上结果反映了所提出的分类器的突出能力。</p><p>表II widget类型分类<br><img src="/2021/04/20/Prioritize%20Crowdsourced%20Test%20Reports%20via%20Deep%20Screenshot%20Understanding/表2.png" alt></p><p>​我们也对结果进行了深入的观察。我们发现，有两组widget容易被混淆。第一组包括ImageButton和ImageView。不难理解，从视觉上看，这两种类型几乎是无法识别的。这两种类型之间的唯一区别是，ImageButton可以触发一个操作，而ImageView只是一个简单的图像。不过，有一点很重要的是，在应用设计中，开发者可以在ImageView widget上添加一个超链接来实现等效的效果。第二组包括Button、EditText和TextView。这三个widget都是一个固定的区域，里面包含一个文本片段，从视觉上看也很相似，即使是人类也难以辨别。此外，一些特殊的渲染方式也让这些widget更加难以识别。根据我们的调查，我们发现这两组易混淆的widget不会有太大影响，无论是从视觉角度还是从功能角度，都可以把这些widget当作为等价的widget。<br>​RQ1的结论：CNN对widget类型进行分类的总体准确率达到89.98%，对于每个具体类型，平均精确率达90.05%，最低精确率为74.36%，F值为89.92%。另外，根据我们对实测报告的调查，即使一些精确率较低的模型，其视觉和功能特征也不会对DEEPPRIOR造成负面影响。</p><h3><span id="c-rq2文本描述分类">C. RQ2：文本描述分类</span></h3><p>​在文本描述的处理中，我们将其分为两类：bug描述和复现步骤。不同的文本描述被认为是不同的报告特征。为了对文本描述进行分类，我们将文本描述分割成句子。然后，我们将这些句子输入到TextCNN模型中来完成任务，详细介绍参见章节III-A2。同时，为了更好地训练和评估网络，我们建立了一个大规模的文本分类数据集。该数据集包含了4340个标记的文本片段，其中包括2252个bug描述和2088个复现步骤。数据集按照7:2:1的比例分为训练集、验证集和测试集。</p><p>表III 文本分类<br><img src="/2021/04/20/Prioritize%20Crowdsourced%20Test%20Reports%20via%20Deep%20Screenshot%20Understanding/表3.png" alt></p><p>​RQ2的结论：文本分类的整体准确率达到96.65%。精确率、召回率和F值都在98%以上。这样的结果显示了DEEPPRIOR对文本描述的优秀分析能力，这也为众测报告的优先级的排序打下了坚实的基础。</p><h3><span id="d-rq3众包测试报告的优先级排序">D. RQ3：众包测试报告的优先级排序</span></h3><p>​在本研究问题中，我们评估DEEPPRIOR的测试报告优先级排序效果。我们使用的指标是APFD（Average Percentage of Fault Detected）指标[15]，Feng等人也使用该指标对众测报告进行优先级排序的评估[3]。在公式中，表示最先发现bugi的报告的索引，<img src="file:///C:/Users/Angelo/AppData/Local/Temp/msohtmlclip1/01/clip_image006.png" alt="img">n是报告总数，M是暴露的bug的总数。</p><img src="/2021/04/20/Prioritize%20Crowdsourced%20Test%20Reports%20via%20Deep%20Screenshot%20Understanding/公式4.png" alt><p>​为了更好地说明DEEPPRIOR的优势，我们将DEEPPRIOR与以下优先级策略进行比较。<br>​* IDEAL：这种策略理论上是具有最好的优先级排序，意味着开发人员可以在最短的时间内审查所有报告中的bug。<br>​* IMAGE：这种策略只使用DEEPPRIOR的深度图像理解结果来对测试报告进行排序，因为深度图像理解是我们研究的重要组成部分。<br>​* BDDIV：这种策略参考了Feng等的工作[3]的算法，这也是众包测试报告优先级排序的最先进的方法。<br>​* RANDOM：RANDOM策略指的是没有任何优先级策略的情况。<br>​对于DEEPPRIOR和IMAGE策略，因为我们的方法很稳定，所以我们运行一次，训练后的模型不会因为不同的尝试而产生不同的结果；对于IDEAL策略，我们手动计算APFD，因为对于固定的报告簇而言，它是一个定值；对于BDDIV策略，我们运行30次，然后像原文[3]一样计算平均值；对于RANDOM策略，我们运行100次，以消除偶然情况的影响。<br>​首先，我们将DEEPPRIOR与RANDOM策略进行比较。如表IV所示，我们发现DEEPPRIOR策略比RANDOM策略效果好了许多，从15.15%至38.93%之间不等，并且平均提升幅度达27.04%。由此可见DEEPPRIOR的优越性。</p><p>表IV DEEPPRIOR报告优先级排序结果与比较<br><img src="/2021/04/20/Prioritize%20Crowdsourced%20Test%20Reports%20via%20Deep%20Screenshot%20Understanding/表4.png" alt></p><p>​然后，我们将DEEPPRIOR与单一IMAGE策略的结果进行对比。DEEPPRIOR的平均提升幅度为4.54%，在2个应用（A3和A4）中，DEEPPRIOR的表现远超过了IMAGE策略。对于A8来说，DEEPPRIOR的策略弱于IMAGE策略。我们对A8的报告进行检查，发现是因为文本描述写的不好，不能对报告的优先级有积极的帮助。总的而言，结果证明了文本分析和深度图像理解两者结合的必要性，单一策略可以弥补彼此的缺点，提高确定优先级的准确性。<br>​同时，我们还对DEEPPRIOR和BDDIV策略（即最先进的方法）进行了比较。根据实验结果，DEEPPRIOR的表现优于BDDIV，平均提升幅度为3.81%。在一些应用中的提升尤其明显。此外，我们还记录了从读取报告簇到输出优先级排序好的报告的总时间开销。结果显示，DEEPPRIOR所使用的时间不到BDDIV的一半，表现出极大的性能优势。<br>​DEEPPRIOR相对于BDDIV的另一个优势是，DEEPPRIOR可以输出稳定的结果，而BDDIV的结果会浮动。根据BDDIV策略的详细结果（于在线包中），我们发现BDDIV的波动性很大。<br>​在不同的应用上与基线策略相比的提升情况不同，可以用某些原因解释这一点。首先，“报告-种类”的比率是不同的，所以在一个应用的有限的activity集中，同样的activity重复出现的频率会变得很高。其次，不同的应用有不同的内容。比如A1是一款儿童教育类应用，它由大量的图片、视频、变体文字组成。在这种情况下，想要提取有用的文本信息，并对应用截图有一个全面的理解，就会变得更为复杂。因此，预估值会降低。<br>​RQ3的结论：DEEPPRIOR对众测报告进行优先级排序的能力是非常优秀的，它的性能优于最先进方法BDDIV，但开销不到其一半。同时，IMAGE策略的具体实验也表明了我们深度截图理解算法的有效性。与最先进的方法相比，DEEPPRIOR的表现更加稳定。</p><h3><span id="e-有效性风险">E. 有效性风险</span></h3><p>​本次实验中应用的类别是有限的。我们的15个实验应用涵盖了8个不同的类别（根据应用商店分类法），类别覆盖有限。但是，我们要强调的是，由于我们对应用截图的深度理解涉及到对应用活动的布局特征，因此DEEPPRIOR仅适用于分析具有网格布局或列表布局的应用程序。我们所表述的也仅限于此类布局的应用中。<br>​众包工人的注册不受限制。众包工人的能力不受限制，可能会出现低质量的报告。然而，即使一些报告的质量很低，如果它真的包含一个bug，DEEPPRIOR也可以识别它所描述的bug。如果没有，DEEPPRIOR会将该报告归类为单一类别，不会影响到其他报告的优先级。<br>​我们构建的数据集是中文的。数据集的语言可能是一个威胁，但NLP和OCR技术相当强大。如果我们把文本处理引擎换成其它语言的引擎，文本处理也会很好地完成，不会对DEEPPRIOR产生负面影响。此外，机器翻译的成熟[16]也使其具备了处理跨语言文本信息的强大能力。</p><h2><span id="v-相关工作">V. 相关工作</span></h2><h3><span id="a-众包测试">A. 众包测试</span></h3><p>​众包测试已是一种主流的测试方法。它与传统测试有很大的不同。测试任务被分配给大量来自不同地点、具备不同专业能力的众包工人。众包测试最显著的优势是能够模拟不同的使用条件，经济成本相对较低[17][18]。然而，众包测试的开放性导致了大量的冗余报告。关键问题是如何提高开发人员审核测试报告的效率。一些研究从选择有技能的众包工人来完成任务[19][20][21]。这样的策略是有效的，但同时它仍然难以控制，因为即使是熟练的众包工人也会在任务中偷懒。因此，我们认为在众包测试中，更重要的是处理测试报告，而非其他因素。Liu等[22]和Yu[6]分别提出了由测试报告的截图自动生成描述的方法，这些方法都基于这样的一个共识，即对所有众包工人而言，应用程序的截图容易获得，但文本描述很难编写。这一想法启发了我们对截图的深度理解，以帮助更好地确定测试报告的优先级。</p><h3><span id="b-众包测试报告处理">B. 众包测试报告处理</span></h3><p>​为了更好地帮助开发人员审查报告和修复bug，当前已有许多研究来处理众包测试报告。基本策略包括报告分类、重复检测和报告优先级确定。在本节中，我们将介绍基于不同策略的相关工作。<br>​Banerjee等人提出了FactorLCS[23]，其使用常见的序列匹配，该方法在开放的bug跟踪库上是有效的。他们还提出了一种多标签分类器的方法[24]，以找到报告集群中具有高度相似性的“主要”报告。同样，Jiang等人提出了TERFUR[14]，这是一个使用NLP技术对测试报告进行聚类的工具，他们还过滤掉了低质量的报告。Wang等[25]将众包工人的特征作为测试报告的特征考虑进去，然后进行聚类。Wang等提出了LOAF[26]，这是第一个将操作步骤和结果描述分开处理的报告特征提取方法。<br>​更多的研究专注于检测重复测试报告。Sun等[27]采用信息检索模型，比较准确地检测重复的bug报告。Sureka等[28]采用基于字符n-gram的模型来完成重复检测任务。Prifti等[29]对大规模的开源项目测试报告进行了调查，提出了一种可以将重复报告的搜索集中在整个存储库的特定部分的方法。Sun等人提出了一个衡量相似度的检索函数REP[30]，并且该函数包括部件、版本等非文本字段的相似度。Nguyen等提出了DBTM [31]，该工具同时利用了基于IR的特征和基于主题的特征，并根据技术问题检测重复的bug报告。Alipour等[32]对测试报告上下文进行了较为全面的分析，提高了检测准确率。Hindle[33]通过结合上下文质量属性、架构术语和系统开发主题进行改进，提高了bug重复检测的能力。<br>​上述方法，包括报告分类和重复检测，都是选择部分测试报告来代表所有的测试报告。但是，我们认为，即使存在重复的报告，所有的报告都包含有价值的信息。而且，在检测到重复的报告后，开发人员仍需要对报告进行审查，以推进bug的处理。因此，我们认为报告优先级是一个更好的选择。<br>​当前也已有许多关于报告优先级的研究。Zhou等提出了BugSim[34]，其考虑了文本和统计特征以对测试报告进行排序。Tian等[35]提出的DRONE是一种基于机器学习的方法，其考虑测试报告的不同因素来预测测试报告的优先级。Feng等人提出了一系列方法，DivRisk[36]和BDDiv[3]，以对测试报告进行优先级排序，他们首先考虑了测试报告的截图。随后，Wang等[2]进一步研究并探索出了一种更加完善的测试报告优先级排序方法，并更提高了对截图的关注度。<br>​在以上的所有研究中，只有少数研究，如[2]和[3]，考虑了应用程序的截图，我们认为这是一个在提取特征来处理测试报告方面相当有价值的因素。但这些研究仅将截图作为简单的矩阵，而非有意义的内容。</p><h3><span id="c-深度图像理解">C. 深度图像理解</span></h3><p>​图像理解是计算机视觉（CV）领域的一个热点问题。本节主要介绍在软件测试中利用图像理解的研究。<br>​Lowe[10]提出了SIFT算法，其利用一系列新的图像局部特征，这些特征对于图像本身而言是恒定的，包括平移、缩放和旋转，以匹配目标图像上的特征点，并计算相似度。光学字符识别（Optical Character Recognition，OCR）是一种应用广泛的文字识别工具，它有助于根据图像上丰富的文字信息更好地理解图像。Nguyen等[37]提出了REMAUI，其使用CV技术来识别应用截图中的widget、文本、图像甚至容器。Moran等[38]在REMAUI的基础上提出了REDRAW，更精确地识别widget，并能自动生成应用UI的代码。同样，Chen等[39]也提出了一种结合CV技术和机器学习的工具，以根据应用截图生成GUI骨架。Yu等[1]提出了一种名为LIRAT的工具，可以在透彻了解应用截图的情况下跨平台记录和重新运行移动应用测试脚本。</p><h2><span id="vi-结论">VI. 结论</span></h2><p>​本文通过深度截图理解，提出了一种众包测试报告优先级排序方法DEEPPRIOR。DEEPPRIOR将应用截图和文本描述转化为四个不同的特征，包括问题widget、上下文widget、bug描述和复现步骤。然后，将特征汇总到DEEPFEATURE中，这些特征根据与bug的相关性，包括bug特征和上下文特征。之后，我们根据特征计算DEEPSIMILARITY。最后，根据DEEPSIMILARITY，按照预先设定的规则对报告进行优先级排序。我们还进行了一个实验来评估所提出的方法，结果显示，DEEPPRIOR的表现优于目前的最优方法，且开销不到它的一半。</p><h2><span id="感谢">感谢</span></h2><p>​本工作得到国家重点研发计划（2018AAA0102302）、国家自然科学基金（61802171、61772014、61690201）、中央高校基本科研基金（14380021）、国家大学生创新创业训练计划（202010284073Z）的部分支持。</p><h2><span id="参考文献">参考文献</span></h2><p>[1] S. Yu, C. Fang, Y. Feng, W. Zhao, and Z. Chen, “Lirat: Layout and image recognition driving automated mobile testing of cross-platform,” in 2019 34th IEEE&#x2F;ACM International Conference on Automated Software Engineering (ASE). IEEE, 2019, pp. 1066–1069. </p><p>[2] J. Wang, M. Li, S. Wang, T. Menzies, and Q. Wang, “Images don’t lie: Duplicate crowdtesting reports detection with screenshot information,” Information and Software Technology, vol. 110, pp. 139–155, 2019. </p><p>[3] Y. Feng, J. A. Jones, Z. Chen, and C. Fang, “Multi-objective test report prioritization using image understanding,” in 2016 31st IEEE&#x2F;ACM International Conference on Automated Software Engineering (ASE). IEEE, 2016, pp. 202–213. </p><p>[4] N. O’Mahony, S. Campbell, A. Carvalho, S. Harapanahalli, G. V. Hernandez, L. Krpalkova, D. Riordan, and J. Walsh, “Deep learning vs. traditional computer vision,” in Science and Information Conference. Springer, 2019, pp. 128–144. </p><p>[5] X. Xiao, X. Wang, Z. Cao, H. Wang, and P. Gao, “Iconintent: automatic identification of sensitive ui widgets based on icon classification for android apps,” in 2019 IEEE&#x2F;ACM 41st International Conference on Software Engineering (ICSE). IEEE, 2019, pp. 257–268. </p><p>[6] S. Yu, “Crowdsourced report generation via bug screenshot understanding,” in 2019 34th IEEE&#x2F;ACM International Conference on Automated Software Engineering (ASE). IEEE, 2019, pp. 1277–1279. </p><p>[7] Y. Kim, “Convolutional neural networks for sentence classification,” arXiv preprint arXiv:1408.5882, 2014. </p><p>[8] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, “Distributed representations of words and phrases and their compositionality,” in Advances in neural information processing systems, 2013, pp. 3111–3119. </p><p>[9] L. R. Rabiner, “A tutorial on hidden markov models and selected applications in speech recognition,” Proceedings of the IEEE, vol. 77, no. 2, pp. 257–286, 1989. </p><p>[10] D. G. Lowe et al., “Object recognition from local scale-invariant features.” in iccv, vol. 99, no. 2, 1999, pp. 1150–1157. </p><p>[11] M. Muja and D. G. Lowe, “Fast approximate nearest neighbors with automatic algorithm configuration.” VISAPP (1), vol. 2, no. 331-340, p. 2, 2009. </p><p>[12] D. F. Silva and G. E. Batista, “Speeding up all-pairwise dynamic time warping matrix calculation,” in Proceedings of the 2016 SIAM International Conference on Data Mining. SIAM, 2016, pp. 837–845. </p><p>[13] T. Y. Chen, F.-C. Kuo, R. G. Merkel, and T. Tse, “Adaptive random testing: The art of test case diversity,” Journal of Systems and Software, vol. 83, no. 1, pp. 60–66, 2010. </p><p>[14] B. Jiang, Z. Zhang, W. K. Chan, and T. Tse, “Adaptive random test case prioritization,” in 2009 IEEE&#x2F;ACM International Conference on Automated Software Engineering. IEEE, 2009, pp. 233–244. </p><p>[15] G. Rothermel, R. H. Untch, C. Chu, and M. J. Harrold, “Prioritizing test cases for regression testing,” IEEE Transactions on software engineering, vol. 27, no. 10, pp. 929–948, 2001. </p><p>[16] S. Karimi, F. Scholer, and A. Turpin, “Machine transliteration survey,” ACM Computing Surveys (CSUR), vol. 43, no. 3, pp. 1–46, 2011. </p><p>[17] R. Gao, Y. Wang, Y. Feng, Z. Chen, and W. E. Wong, “Successes, challenges, and rethinking–an industrial investigation on crowdsourced mobile application testing,” Empirical Software Engineering, vol. 24, no. 2, pp. 537–561, 2019. </p><p>[18] K. Mao, L. Capra, M. Harman, and Y. Jia, “A survey of the use of crowdsourcing in software engineering,” Journal of Systems and Software, vol. 126, pp. 57–84, 2017. </p><p>[19] Q. Cui, S. Wang, J. Wang, Y. Hu, Q. Wang, and M. Li, “Multi-objective crowd worker selection in crowdsourced testing.” </p><p>[20] Q. Cui, J. Wang, G. Yang, M. Xie, Q. Wang, and M. Li, “Who should be selected to perform a task in crowdsourced testing?” in 2017 IEEE 41st Annual Computer Software and Applications Conference (COMPSAC), vol. 1. IEEE, 2017, pp. 75–84. </p><p>[21] M. Xie, Q. Wang, G. Yang, and M. Li, “Cocoon: Crowdsourced testing quality maximization under context coverage constraint,” in 2017 IEEE 28th International Symposium on Software Reliability Engineering (ISSRE). IEEE, 2017, pp. 316–327. </p><p>[22] D. Liu, X. Zhang, Y. Feng, and J. A. Jones, “Generating descriptions for screenshots to assist crowdsourced testing,” in 2018 IEEE 25th International Conference on Software Analysis, Evolution and Reengineering (SANER). IEEE, 2018, pp. 492–496. </p><p>[23] S. Banerjee, B. Cukic, and D. Adjeroh, “Automated duplicate bug report classification using subsequence matching,” in 2012 IEEE 14th International Symposium on High-Assurance Systems Engineering. IEEE, 2012, pp. 74–81. </p><p>[24] S. Banerjee, Z. Syed, J. Helmick, and B. Cukic, “A fusion approach for classifying duplicate problem reports,” in 2013 IEEE 24th International Symposium on Software Reliability Engineering (ISSRE). IEEE, 2013, pp. 208–217. </p><p>[25] J. Wang, Q. Cui, Q. Wang, and S. Wang, “Towards effectively test report classification to assist crowdsourced testing,” in Proceedings of the 10th ACM&#x2F;IEEE International Symposium on Empirical Software Engineering and Measurement, 2016, pp. 1–10. </p><p>[26] J. Wang, S. Wang, Q. Cui, and Q. Wang, “Local-based active classification of test report to assist crowdsourced testing,” in Proceedings of the 31st IEEE&#x2F;ACM International Conference on Automated Software Engineering, 2016, pp. 190–201. </p><p>[27] C. Sun, D. Lo, X. Wang, J. Jiang, and S.-C. Khoo, “A discriminative model approach for accurate duplicate bug report retrieval,” in Proceedings of the 32nd ACM&#x2F;IEEE International Conference on Software Engineering-Volume 1, 2010, pp. 45–54. </p><p>[28] A. Sureka and P. Jalote, “Detecting duplicate bug report using character n-gram-based features,” in 2010 Asia Pacific Software Engineering Conference. IEEE, 2010, pp. 366–374. </p><p>[29] T. Prifti, S. Banerjee, and B. Cukic, “Detecting bug duplicate reports through local references,” in Proceedings of the 7th International Conference on Predictive Models in Software Engineering, 2011. </p><p>[30] C. Sun, D. Lo, S.-C. Khoo, and J. Jiang, “Towards more accurate retrieval of duplicate bug reports,” in 2011 26th IEEE&#x2F;ACM International Conference on Automated Software Engineering (ASE 2011). IEEE, 2011, pp. 253–262. </p><p>[31] A. T. Nguyen, T. T. Nguyen, T. N. Nguyen, D. Lo, and C. Sun, “Duplicate bug report detection with a combination of information retrieval and topic modeling,” in 2012 Proceedings of the 27th IEEE&#x2F;ACM International Conference on Automated Software Engineering. IEEE, 2012, pp. 70–79. </p><p>[32] A. Alipour, A. Hindle, and E. Stroulia, “A contextual approach towards more accurate duplicate bug report detection,” in 2013 10th Working Conference on Mining Software Repositories (MSR). IEEE, 2013, pp. 183–192. </p><p>[33] A. Hindle, A. Alipour, and E. Stroulia, “A contextual approach towards more accurate duplicate bug report detection and ranking,” Empirical Software Engineering, vol. 21, no. 2, pp. 368–410, 2016. </p><p>[34] J. Zhou and H. Zhang, “Learning to rank duplicate bug reports,” in Proceedings of the 21st ACM international conference on Information and knowledge management, 2012, pp. 852–861. </p><p>[35] Y. Tian, D. Lo, and C. Sun, “Drone: Predicting priority of reported bugs by multi-factor analysis,” in 2013 IEEE International Conference on Software Maintenance. IEEE, 2013, pp. 200–209. </p><p>[36] Y. Feng, Z. Chen, J. A. Jones, C. Fang, and B. Xu, “Test report prioritization to assist crowdsourced testing,” in Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering, 2015, pp. 225–236. </p><p>[37] T. A. Nguyen and C. Csallner, “Reverse engineering mobile application user interfaces with remaui (t),” in IEEE&#x2F;ACM International Conference on Automated Software Engineering, 2016. </p><p>[38] K. Moran, C. Bernal-Cardenas, M. Curcio, R. Bonett, and D. Poshy- ´ vanyk, “Machine learning-based prototyping of graphical user interfaces for mobile apps,” arXiv preprint arXiv:1802.02312, 2018. </p><p>[39] C. Chen, T. Su, G. Meng, Z. Xing, and Y. Liu, “From ui design image to gui skeleton: a neural machine translator to bootstrap mobile gui implementation,” in Proceedings of the 40th International Conference on Software Engineering. ACM, 2018, pp. 665–676.</p>]]></content>
    
    
    <summary type="html">&lt;img src=&quot;/2021/04/20/Prioritize%20Crowdsourced%20Test%20Reports%20via%20Deep%20Screenshot%20Understanding/title.png&quot; alt&gt;

&lt;h1 id=&quot;基于深度截图理解的众包测试报告优先级排序&quot;&gt;&lt;a href=&quot;#基于深度截图理解的众包测试报告优先级排序&quot; class=&quot;headerlink&quot; title=&quot;基于深度截图理解的众包测试报告优先级排序&quot;&gt;&lt;/a&gt;基于深度截图理解的众包测试报告优先级排序&lt;/h1&gt;&lt;h2 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要&quot;&gt;&lt;/a&gt;摘要&lt;/h2&gt;&lt;p&gt;​		众包测试在移动应用程序测试中日益占据主导地位，但对于应用开发者来说，审查数量过多的测试报告是很大的负担。已有许多学者提出基于文本和简单图片特征的测试报告处理方法。然而，在移动应用测试中，测试报告所包含的文本较为精简且信息不够充分，图片则能够提供更丰富的信息。这一趋势促使我们在深度截图理解的基础上，对众包测试报告的优先级进行排序。&lt;br&gt;​		本文中，我们提出了一种新的众包测试报告优先级排序方法，即DEEPPRIOR。我们首先引入一个新的特征来代表众包测试报告，即DEEPFEATURE，它基于对应用程序截图的深度分析，涵盖了所有组件（widget）及它们的文本、坐标、类型甚至是意图。DEEPFEATURE包括直接描述bug的bug特征（Bug Feature），和刻画bug完整上下文的上下文特征（Context Feature）。DEEPFEATURE的相似度用于表示测试报告的相似度，并被用于对众包测试报告进行优先级排序。我们形式上将相似度定义为DEEPSIMILARITY。我们还进行了一个实证实验，以评估所提技术在大型数据中的有效性。结果表明，DEEPPRIOR性能最佳，以不足一半的成本获得优于其它方法的结果。&lt;/p&gt;
&lt;p&gt;索引词 众包测试，移动应用测试，深度截图理解&lt;/p&gt;</summary>
    
    
    
    <category term="论文阅读" scheme="https://values.keys.moe/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="论文阅读" scheme="https://values.keys.moe/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
  </entry>
  
  <entry>
    <title>Syntactic Data Augmentation Increases Robustness to Inference Heuristics</title>
    <link href="https://values.keys.moe/2020/09/16/Syntactic%20Data%20Augmentation%20Increases%20Robustness%20to%20Inference%20Heuristics/"/>
    <id>https://values.keys.moe/2020/09/16/Syntactic%20Data%20Augmentation%20Increases%20Robustness%20to%20Inference%20Heuristics/</id>
    <published>2020-09-16T08:52:33.000Z</published>
    <updated>2022-10-05T09:40:33.660Z</updated>
    
    <content type="html"><![CDATA[<img src="/2020/09/16/Syntactic%20Data%20Augmentation%20Increases%20Robustness%20to%20Inference%20Heuristics/title.png" alt><h1><span id="通过语法数据扩增提升推理启发法的鲁棒性">通过语法数据扩增提升推理启发法的鲁棒性</span></h1><h2><span id="摘要">摘要</span></h2><p>​诸如BERT的预训练的神经模型在微调以执行自然语言推理（NLI）时，常常在标准数据集上展现出了高度准确性，但在受控的挑战集上，它们却表现出对语序敏感度的出奇缺乏。我们假设此问题并不主要因为预训练模型的局限性引起，而是由于缺乏众包的NLI样例引起的，而这些样例可能在微调阶段传递了语法结构的重要性。我们探索了几种方法来扩增标准训练集中语法丰富的实例，这些实例是通过对MNLI语料库的句子应用语法转换而生成的。而表现最好的扩增方法，主语&#x2F;宾语倒置法，可以在不影响BERT对MNLI测试集性能的情况下，将BERT对受控实例的词序敏感度诊断从0.28提升至0.73。这种改进全面超过了用于数据扩增的特定结构，这表明了扩增可以使BERT学习到抽象语法的表现形式。</p><span id="more"></span><h2><span id="1-介绍">1. 介绍</span></h2><p>​在NLP里常见的监督学习范例中，特定分类任务的大量标记实例被随机地分为训练集和测试集。系统在训练集上进行训练，然后在测试集上进行评估。神经网络，尤其是对单词预测对象的进行预训练的系统，如ELMo(Peters et al.,2018)或BERT(Devlin et al.,2019)——在这种范例中表现出色：在具有足够大的预训练语料库的情况下，这些模型在许多测试集上所表现出的准确性达到甚至超过了未经训练的人类标注者(Raffel et al.,2019)。</p><p>​同时，越来越多的证据表明，从与训练集相同的范围中提取的测试集上的高精度并不表示模型已经掌握了该任务。当模型应用于表示相同任务的不同数据集中，这种差异可能表现为准确性的急剧下降(Talmor and Berant, 2019;Yogatama et al., 2019)，或者表现为对输入语言无关扰动的过度敏感(Jia and Liang, 2017; Wallace et al., 2019)。</p><p>​在自然语言推理（NLI）任务中，McCoy等人(2019b)记录了这样的一种差异，即模型在标准测试集上的出色性能并不对应表明它能像人类定义的那样精通于此任务。在这个任务中，系统将获得两个句子，其被期望确定一个句子（前提）是否蕴含另一个句子（假设）。即使不是所有人，大多数人也都会同意NLI需要对语法结构敏感。例如，以下句子即使包含了相同的单词，但它们并不相互蕴含：</p><p>​(1)  演员看到了律师 (The lawyer saw the actor.)</p><p>​(2)  律师看到了演员 (The actor saw the lawyer.)</p><p>​McCoy等人构造了HANS挑战集，其包含了一系列此类构造的例子，并且其被用来表明，当BERT在MNLI语料库进行微调时，该微调模型在从该语料库提取的测试集上取得了较高的准确率，但其对语法几乎没有敏感性；该模型会错误地得出结论，如（1）蕴含（2）。</p><p>​我们考虑用两种解释来说明为什么在MNLI上微调的BERT会在HANS上失败。在代表性不足假设下，BERT在HANS上失败，是因为它的预训练表现形式缺失了一些必要的语法信息。而在缺失连接的假设下，BERT从输入中提取相关语法信息(参见 Goldberg 2019;Tenney et al. 2019)，但是它无法在HANS上使用这个信息，因为很少有MNLI训练实例可以表明语法应该如何支持NLI的(McCoy et al., 2019b)。这两种假设都有可能是正确的：部分语法方面BERT可能根本未学习到，还有部分方面已经学过了，但并没有应用被用于进行推理。</p><p>​缺失连接假设预测，从一个语法结构中使用少量的实例进行训练集的扩增将使BERT知道任务需要它使用它的语法表现形式。这不仅将使得用于数据扩增的结构的改进，并且也可推广到其他结构上。相反，代表性不足假设预测，模型想要在HANS上具有更好的表现，BERT必须从头开始学习每种语法结构是如何影响NLI的。这预计需要有更大的数据扩增集来获得足够的性能，并且整个结构几乎不能泛化。</p><p>​本文旨在验证这些假设。我们通过对MNLI的少量实例进行语法转换以构造扩增集。尽管在MNLI上只扩增了约400个主语与宾语互换的实例（大约是MNLI训练集大小的0.1%），但模型对语法上具有挑战性的案例的准确率得到了显著的提高。更关键的是，即使在扩增中仅使用了一个单一的变换，但在一系列的结构中，准确度都得到了提高。例如，BERT在涉及关系从句的实例（例如，演员给游客看到的银行家打电话(The actors called the banker who the tourists saw) 无法推导出银行家打电话给游客(The banker called the tourists)）在未扩增的实例中准确性为0.33，而扩增后为0.83。这表明我们的方法并不会过度适应于一个结构，而是利用了BERT现有的语法表示形式，从而为缺失连接假设提供了支持。同时，我们还观察到了泛化的局限性，在这些情况下，它们支持了代表性不足的假设。</p><h2><span id="2-背景">2. 背景</span></h2><p>​HANS是一个模板生成的挑战集，旨在测试NLI模型是否采用了三种语法启发法。首先是词汇重叠启发法，其假定所有时间内所有在假设中的单词也都在前提中，且标签需是蕴含标签。在MNLI训练集中，这种启发法通常会做出正确的预测，几乎从不做出错误的预测。这可能是由于MNLI生成的过程所致：众包工作者被给予一个前提，并被要求生成与该前提相矛盾，或是蕴含这个前提的句子。为了最大程度地减少工作量，工作人员可能过度地使用了词汇重叠，将其作为一种生成含义假设的捷径。当然，词汇重叠启发法并不是一种普遍有效的推理策略，并且在许多HANS实例中都是失败的。例如上文所述，律师看到了演员(the lawyer saw the actor)，并不意味着演员看到了律师(the actor saw the lawyer)。</p><p>​HANS还包括诊断子序列启发法（假定前提蕴含任何与其相邻的子序列的假设）和成分启发法（假设前提蕴含其自身所有构成要素）的实例情形。当我们专注于对抗词汇重叠启发法时，我们还将测试其他启发法的泛化情况，这可以看作是词汇重叠中特别具有挑战性的案例。表A.5，A.6，A.7给出了用于诊断这三种启发法的所有结构的实例。</p><p>​数据扩增通常用于增强视觉的鲁棒性(Perez and Wang, 2017)与语言的鲁棒性(Belinkov and Bisk, 2018; Wei and Zou, 2019)，包括了NLI (Minervini and Riedel, 2018; Yanaka et al., 2019)。在许多情况下，使用一种实例进行扩增可以提高特定情况下的准确性，但不能泛化到其他情况，这表明模型过拟合于扩增集(Jia and Liang, 2017; Ribeiro et al., 2018; Iyyer et al., 2018; Liu et al., 2019)。特别的，McCoy等人(2019b)发现，HANS的实例的扩增可以泛化推广到不同的单词重叠挑战集(Dasgupta et al., 2018)，但这仅适用于长度与HANS实例相似的实例。我们通过生成各种基于语料库的实例来减轻对表面属性的过度拟合，这些实例与挑战集上的词法与语法均不同。最后，Kim等人(2018)使用了与我们相似的数据扩增方法，但没有研究对不在扩增集中的实例类型的泛化。</p><h2><span id="3-生成扩增数据">3. 生成扩增数据</span></h2><p>​我们使用两种语法转换从MNLI生成扩增实例：倒置INVERSION（互换原句的主语与宾语）和被动化PASSIVIZATION。对于每个转换，我们都有两个系列的扩增集。原始前提(ORIGINAL PREMISE)策略保留了原有的MNLI前提，并对假设进行了转换；转换假设(TRANSFORMED HYPOTHESIS)使用原始MNLI假设作为新前提，转换后的假设作为新假设（实例见表1，具体请参见§A.2）。我们尝试了三种扩增集的大小：小型（101个实例），中型（405个实例），大型（1215个实例）。所有的扩增集都比MNLI训练集（297k）小得多。</p><img src="/2020/09/16/Syntactic%20Data%20Augmentation%20Increases%20Robustness%20to%20Inference%20Heuristics/表1.png" alt><p>​我们没有试图确保生成实例的自然性；例如，在倒置转换中，车厢造成了大量噪音(The carriage made a lot of noise)被转换成大量噪音造成了车厢(A lot of noise made the carriage)。此外，扩增数据集的标签存在一些噪音；例如，我们假设倒置将正确的标签从蕴含改为中性，但是也并非必然如此（如果买方遇到卖方(The buyer met the seller)，那么卖方遇到买方(The seller met the buyer)是有可能的）。如下所示，这种噪声不会损害MNLI的准确性。</p><p>​最后，我们包括一个随机的打乱条件，其中MNLI前提及其假设都被随机打乱。我们使用这个情况来测试语法上不知情的方法是否能教会这个模型：当忽略单词顺序时，就无法做出可靠的推论。</p><h2><span id="4-试验设置">4. 试验设置</span></h2><p>​我们将每个扩增集分别添加到MNLI的训练集中，并对每个生成的训练集进行微调BERT的训练。微调的更多细节在附录A.1中。我们为扩增策略与扩增集大小的每种组合重复了五个随机种子的过程，但最成功的策略（倒置+转换假设（INVERSION+TRANSFORMED HYPOTHESIS））除外。且对于每个扩增的范围，均进行了15次运行。参照McCoy等人(2019b)，在对HANS进行评估时，我们将模型产生的中性与矛盾标签合并为一个单一的非蕴含(non-entailment)标签。</p><p>​对于原始前提(ORIGINAL PREMISE)与转换假设(TRANSFORMED HYPOTHESIS)，我们尝试了分别使用每一种转换，并使用了包含倒置与被动化的数据集进行了实验。我们还分别对仅使用带有蕴含标签的被动化例子和仅使用带有非蕴含标签的被动化例子进行了单独的实验。作为基线，我们使用了100次在未进行数据扩增的MNLI上训练出的微调的BERT模型(McCoy et al., 2019a)。</p><p>​我们会报告模型在HANS上的准确性和在MNLI的开发集上的准确性（MNLI测试集的标签不公开）。我们没有调整这个开发集的任何参数。我们下面讨论的所有比较都在p&lt;0.01的水平上，比较结果都是十分显著的（基于双向t检验）。</p><h2><span id="5-结果">5. 结果</span></h2><p>​MNLI的准确性在不同的扩增策略中都非常相似，并且与未经扩增的基线(0.84)相匹配，这表明最多有1215个实例的语法扩增不会损害数据集的整体表现。相比之下，HANS的准确度差异很大，大多数模型在非蕴含的实例中的表现得比置信准确度差（在HANS上为0.5），这表明了它们采用了启发法（图1）。很大程度上，最有效的扩增策略是倒置结合转换假设。HANS在单词重叠案例（其中正确的标签都是非蕴含的，例如：医生看了律师(the doctor saw the lawyer)↛律师看了医生(the lawyer saw the doctor)）的准确度在没有数据扩增的情况下为0.28，在大型扩增集上为0.73。同时，在启发法做出正确预测的情况下（如演员旁的游客们给作家们打电话(The tourists by the actor called the authors) → 游客们给作家们打电话(The tourists called the authors)），这种策略降低了BERT的准确性；实际上，在词汇重叠做出正确与不正确的预测情况下，最佳模型的准确度都是相似的，这表明了这种干预阻止了模型采用启发法。</p><img src="/2020/09/16/Syntactic%20Data%20Augmentation%20Increases%20Robustness%20to%20Inference%20Heuristics/图1.png" alt><p>​随机打乱的方法并未使模型在未经扩增的基线上得到了改善，表明关注语法的转换是必要的(表A.2)。被动化比倒置的收益要小得多，这可能是由于存在显式的标记引起的（如单词by），这可能导致模型仅在这些单词出现时才考虑词序。有趣的是，即使是在HANS的被动实例中，倒置也仍比被动化更有效（大型倒置扩增：0.13；大型被动化扩增：0.01）。最后，自身倒置比倒置与被动化的结合更有效。</p><p>​现在，我们来更详细地分析最有效的策略，即倒置结合转换假设。首先，该策略在抽象层面上与HANS的主语&#x2F;宾语交换类别相似，但是两者的词汇与语法特性均有不同。尽管存在这些差异，但模型在HANS类别上的表现在中型与大型扩增条件下都是完美的（1.00），这表明BERT能从转换的高级语法结构中受益。对于小的扩增集，此类别的准确性为0.53，表明有101个实例不足以使BERT知道不能随意的交换主语与宾语的对象。相反，将扩增大小从中型变至大型，能在HANS的子案例中产生适度且易变的效果（见附录A.3了解具体个案的结果）；为了更清楚的了解扩增大小的影响，可能还需要对该参数进行更密集的采样。</p><img src="/2020/09/16/Syntactic%20Data%20Augmentation%20Increases%20Robustness%20to%20Inference%20Heuristics/图2.png" alt><p>​尽管倒置是该扩充集中的唯一转换，但是除了主语&#x2F;宾语交换之外，其他结构的性能也得到了显著的提高（图2）；例如，模型能够更好地处理包含介词短语的实例，例如，经理背后的法官看医生(The judge behind the manager saw the doctors)并不蕴含医生看经理(The doctors saw the manager)，（未扩增：0.41；大型扩增：0.89）。在以子序列启发法方法为目标的案例中，有一个更平缓，但仍然十分明显的提升；这种较小程度的提升表明，对连续子序列从词汇重叠中分离处理更具泛化性。一个例外是对“NP&#x2F;S”推论的准确性，例如经理们听说秘书辞职了(the managers heard the secretary resigned) ↛经理们听见了秘书说话(The managers heard the secretary)，这一准确度从0.02（未扩增）大幅提升至0.5（大型扩增）。因此，对子序列案例的进一步改进可能需要涉及子序列的数据扩增。</p><p>​在过去的一年中，人们提出了一系列技术来提高HANS的性能。这些模型包括语法感知模型(Moradshahi et al., 2019; Pang et al., 2019)，旨在捕获预定义浅层启发法的辅助模型，以使主模型可以专注于稳健策略(Clark et al., 2019; He et al., 2019; Mahabadi and Henderson, 2019)以及提高难度训练实例权重的方法(Yaghoobzadeh et al., 2019)。尽管其中的一些方法在HANS上比我们的方法具有更高的准确性，包括更好的泛化了成分和子序列的情况（参见表A.4），但它们并不具有直接的可比性：我们的目标是在不修改模型或训练程序的情况下，评估训练集中的具有语法挑战性的实例是如何影响BERT的NLI的表现的。</p><h2><span id="6-讨论">6. 讨论</span></h2><p>​我们最佳效果的策略涉及通过对MNLI实例的主语&#x2F;宾语倒置转换而生成的少量MNLI实例来扩增MNLI训练集。这产生了可观的泛化能力：既是对另一个域而言的（HANS挑战集），更重要的是，其也适用于其他结构，如关系从句和介词短语。这支持了缺失连接假设：对一个结构进行少量扩增会引起抽象的语法敏感性，而不是仅仅通过为模型建立来自同一分布的案例样本来“接种(inoculating)”模型，以防止在挑战集上失败(Liu et al., 2019)。</p><p>​同时，倒置转换并未完全抵消启发法，特别是，这些模型在被动句上的表现较差。因此，对于这些结构，BERT的预训练可能在通过一个较小的扩增后，也仍无法产生强有力的语法表现形式；换句话说，这可能是我们的代表性不足假设成立的情况。该假设预测，作为单词预测模型的预训练BERT对于被动词处理困难，并且可能需要专门针对NLI任务学习其对应结构的特性；这可能需要大量的扩增实例。</p><p>​表现最佳的扩增策略是从一个单独原句生成前提&#x2F;假设对，这意味着该策略不依赖于NLI语料库。我们可以从任何语料库生成扩增实例，这使得我们有可能测试非常大的扩增集是否有效（当然，请注意，来自不同领域的扩增语句可能会影响在MNLI上本身的表现）。</p><p>​最终，我们希望有一个能在跨语言理解任务中也能在使用语法方面具有强归纳偏差的模型，即使在重叠启发法导致其在训练集上的高精度时也是如此。事实上，很难想象人们在理解一个句子时会完全忽略语法。另一种可选的方法是创建足以代表各种语言现象的训练集；众包工作者（理性的人）偏爱使用尽可能简单的生成策略，这可以通过对抗性过滤等方法来抵消(Nie et al., 2019)。然而，在此期间，我们得出结论，数据扩增是一个简单而有效的策略，其可以缓解BERT等模型中已知的推理启发法。</p><h2><span id="a-附录">A. 附录</span></h2><h3><span id="a1-微调细节">A.1 微调细节</span></h3><p>​我们在所有实验中都使用了bert-base-uncased版本的模型。按照标准，我们通过在MNLI上训练线性分类器从CLS标记的最终层嵌入预测标签，同时继续更新BERT参数，来对该预训练模型进行微调(Devlin et al., 2019)。每个模型的训练实例顺序都进行了打乱。所有模型都经过了三个epoch的训练。</p><h3><span id="a2-生成扩增实例">A.2 生成扩增实例</span></h3><p>​以下列表描述了我们使用的扩增策略。表A.1说明了应用于特定原句的所有策略。注意，倒置通常会改变句子的含义（侦探跟随着嫌疑犯(the detective followed the suspect)与嫌疑犯跟随着侦探(the suspect followed the detective)描述的并不是一个场景），但是被动语句并不会（侦探跟随着嫌疑犯(the detective followed the suspect)与嫌疑犯被侦探跟随(the suspect was followed by the detective)描述的是同一个场景）。</p><ul><li><p>倒置（原始前提）：对于原实例(p,h, →)，生成(p,INV(h), ↛)，其中INV返回主语与宾语调换了的原句，忽略原实例中标签为↛的实例。</p></li><li><p>倒置（转换假设）：对于原实例(p,h) （带有任意标签），丢弃前提p并且生成(h,INV(h), ↛)</p></li><li><p>被动（原始前提）：对于原实例(p,h) （带有任意标签），生成(p,PASS(h))，保持标签不变，其中PASS返回原句的被动语态版本（在不改变原意的情况下）</p></li><li><p>被动（转换假设）：对于原实例(p,h)，丢弃前提p，并且生成两个实例，一个携带蕴含标签——(h,PASS(h), →)，一个携带非蕴含标签——(h,PASS(h), ↛)</p></li></ul><p>​我们使用MNLI提供的选区分析，识别出MNLI中可以作为原句的及物句，但噪声较大的TELEPHONE类型除外。为此，我们搜索了恰好带有VP的一个NP子节点的矩阵S节点，其中主宾都是完整的名词短语（即，都不是像me这样的人称代词），而动词词缀不是be或have。我们保留了动词的原始时态，并在必要时修改了他们的一致性特征。（如：电影是Matt Dillon和Gary Sinise 出演(the movie stars Matt Dillon and Gary Sinise)改为Matt Dillon和Gary Sinise 出演了这部电影(Matt Dillon and Gary Sinise star the movie)）。</p><p>​所有策略中，最大的扩增集大小为1215。这个大小是我们可以从MNLI生成的最大扩增数据集确定的，该数据集是使用了上述的倒置结合原始前提方法。为了公平比较，即使对于可以生成更大数据集的策略，我们仍保持相同的大小。我们还通过随机抽样405个使用上述过程识别的案例创造了中型数据集，以及包含了101个实例的小数据集。我们对于每种策略仅执行一次这个过程：因此，运行仅在分类器的权重初始化和实例顺序方面有所不同，而在训练中包含的扩增实例没有变化。</p><p>​为了创建组合的扩增数据集，我们将倒置和被动化的数据集进行串联，然后随即丢掉一半实例（以使组合数据集的大小与其他数据集相匹配）。与其他数据集一样，我们只执行了一次：合并的扩增集在每次运行中相同。该过程的一个结果是，倒置和被动化的实例数量不完全相同。</p><h3><span id="a3-详细结果">A.3 详细结果</span></h3><p>​下列图表提供了我们试验的详细结果。</p><p>​表A.2显示了每种策略在MNLI上的平均准确率，以及在诊断三种启发法（词汇重叠启发法，子序列启发法和成分启发法）中的每一种的HANS案例的平均准确率，正确的标签是非蕴含（↛）。表A.3探究了最佳扩增策略—具有转换假设的主宾倒置，包括了在正确的标签是蕴含（→）和非蕴含（↛）两种情况。</p><p>​最后，末尾三个表格详细说明了通过倒置结合转换假设来对30个HANS子案例进行扩增的效果，并按设计按照诊断启发法来细分成表：词汇重叠启发法（表A.5）；子序列启发法（表A.6）和成分启发法（表A.7）。</p><img src="/2020/09/16/Syntactic%20Data%20Augmentation%20Increases%20Robustness%20to%20Inference%20Heuristics/表A.1.png" alt><p>表A.1：语法扩充策略（完整表格）</p><img src="/2020/09/16/Syntactic%20Data%20Augmentation%20Increases%20Robustness%20to%20Inference%20Heuristics/表A.2.png" alt><img src="/2020/09/16/Syntactic%20Data%20Augmentation%20Increases%20Robustness%20to%20Inference%20Heuristics/表A.3.png" alt><p>表A.3：使用主语&#x2F;宾语倒置结合转换假设的数据扩增对HANS准确性的影响。图表展示了在使用三种扩增集大小（101，405，1215个实例）来扩增的MNLI训练集后BERT的微调结果，也展示了在未经扩增的MNLI训练集上BERT的微调结果。</p><img src="/2020/09/16/Syntactic%20Data%20Augmentation%20Increases%20Robustness%20to%20Inference%20Heuristics/表A.4.png" alt><p>表A.4：不同的架构与训练方式所得到的HANS准确率，其被按照实例诊断的启发法与实例明确的标签拆分开来。除了MT-DNN+LF外，其余均采用了BERT作为基本模型。L，S与C分别代表词汇重叠、子序列和成分启发法。扩增集的大小为n&#x3D;101（小型），n&#x3D;405（中型），n&#x3D;1215（大型）。</p><img src="/2020/09/16/Syntactic%20Data%20Augmentation%20Increases%20Robustness%20to%20Inference%20Heuristics/表A.5.png" alt><p>表A.5：主语&#x2F;宾语倒置结合转换假设：图表展示了HANS子案例在诊断为词法重叠启发法时的结果，这些案例包括四种训练方案-未经扩增（只在MNLI上进行训练），和小型（n&#x3D;101），中型（n&#x3D;405），大型（n&#x3D;1215）扩增集的情况。置信准确度为0.5。图表上半部分：标签是非蕴含的案例情况。图表下半部分：标签是蕴含的案例情况。</p><img src="/2020/09/16/Syntactic%20Data%20Augmentation%20Increases%20Robustness%20to%20Inference%20Heuristics/表A.6.png" alt><p>表A.6：主语&#x2F;宾语倒置结合转换假设：图表展示了HANS子案例在诊断为子序列启发法时的结果，这些案例包括四种训练方案-未经扩增（只在MNLI上进行训练），和小型（n&#x3D;101），中型（n&#x3D;405），大型（n&#x3D;1215）扩增集的情况。图表上半部分：标签是非蕴含的案例情况。图表下半部分：标签是蕴含的案例情况。</p><img src="/2020/09/16/Syntactic%20Data%20Augmentation%20Increases%20Robustness%20to%20Inference%20Heuristics/表A.7.png" alt><p>表A.7：主语&#x2F;宾语倒置结合转换假设：图表展示了HANS子案例在诊断为成分启发法时的结果，这些案例包括四种训练方案-未经扩增（只在MNLI上进行训练），和小型（n&#x3D;101），中型（n&#x3D;405），大型（n&#x3D;1215）扩增集的情况。置信准确度为0.5。图表上半部分：标签是非蕴含的案例情况。图表下半部分：标签是蕴含的案例情况。</p>]]></content>
    
    
    <summary type="html">&lt;img src=&quot;/2020/09/16/Syntactic%20Data%20Augmentation%20Increases%20Robustness%20to%20Inference%20Heuristics/title.png&quot; alt&gt;

&lt;h1 id=&quot;通过语法数据扩增提升推理启发法的鲁棒性&quot;&gt;&lt;a href=&quot;#通过语法数据扩增提升推理启发法的鲁棒性&quot; class=&quot;headerlink&quot; title=&quot;通过语法数据扩增提升推理启发法的鲁棒性&quot;&gt;&lt;/a&gt;通过语法数据扩增提升推理启发法的鲁棒性&lt;/h1&gt;&lt;h2 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要&quot;&gt;&lt;/a&gt;摘要&lt;/h2&gt;&lt;p&gt;​	诸如BERT的预训练的神经模型在微调以执行自然语言推理（NLI）时，常常在标准数据集上展现出了高度准确性，但在受控的挑战集上，它们却表现出对语序敏感度的出奇缺乏。我们假设此问题并不主要因为预训练模型的局限性引起，而是由于缺乏众包的NLI样例引起的，而这些样例可能在微调阶段传递了语法结构的重要性。我们探索了几种方法来扩增标准训练集中语法丰富的实例，这些实例是通过对MNLI语料库的句子应用语法转换而生成的。而表现最好的扩增方法，主语&amp;#x2F;宾语倒置法，可以在不影响BERT对MNLI测试集性能的情况下，将BERT对受控实例的词序敏感度诊断从0.28提升至0.73。这种改进全面超过了用于数据扩增的特定结构，这表明了扩增可以使BERT学习到抽象语法的表现形式。&lt;/p&gt;</summary>
    
    
    
    <category term="论文阅读" scheme="https://values.keys.moe/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="论文阅读" scheme="https://values.keys.moe/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
  </entry>
  
  <entry>
    <title>通过Xrdp实现Windows远程访问Ubuntu 16.04</title>
    <link href="https://values.keys.moe/2019/03/05/%E9%80%9A%E8%BF%87Xrdp%E5%AE%9E%E7%8E%B0Windows%E8%BF%9C%E7%A8%8B%E8%AE%BF%E9%97%AEUbuntu-16-04/"/>
    <id>https://values.keys.moe/2019/03/05/%E9%80%9A%E8%BF%87Xrdp%E5%AE%9E%E7%8E%B0Windows%E8%BF%9C%E7%A8%8B%E8%AE%BF%E9%97%AEUbuntu-16-04/</id>
    <published>2019-03-05T03:25:45.000Z</published>
    <updated>2022-09-28T06:24:41.208Z</updated>
    
    <content type="html"><![CDATA[<p>​目前网上的大量教程都是需要安装xfac4或者xubuntu桌面系统才能实现远程连接。因为xrdp支持在13.10之后版本就已经不支持的Gnome了和原生Unity桌面，所以网上很多方法都是安装能够被xdrp支持的第三方xfac4或者xubuntu桌面系统间接达到远程控制Ubuntu。<br>​本文提供如何使用Xrdp访问原生Ubuntu桌面。</p><span id="more"></span><h2><span id="step1-下载tigervnc-server软件包">Step.1 下载TigerVNC Server软件包</span></h2><p>下载地址：<br><a href="http://www.c-nergy.be/downloads/tigervncserver_1.6.80-4_amd64.zip">http://www.c-nergy.be/downloads/tigervncserver_1.6.80-4_amd64.zip</a></p><h2><span id="step2-安装tigervnc-server">Step.2 安装TigerVNC Server</span></h2><p>1.打开终端，进入到刚刚你你下载TigerVNC Server的存放目录</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">cd</span> 下载</span><br></pre></td></tr></table></figure><p>2.执行安装指令</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo dpkg -i tigervncserver_1.<span class="number">6.80</span>-<span class="number">4</span>_amd64.<span class="keyword">deb</span></span><br><span class="line">或者</span><br><span class="line">sudo apt-<span class="built_in">get</span> install tightvncserver</span><br></pre></td></tr></table></figure><p>过程中如果出现警告信息和错误信息，原因是没有相对应的依赖包。<br>执行</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-<span class="built_in">get</span> install -<span class="keyword">f</span></span><br></pre></td></tr></table></figure><p>然后在执行之前的安装命令。</p><h2><span id="step3-安装xrdp">Step.3 安装Xrdp</span></h2><p>终端输入安装命令</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-<span class="built_in">get</span> install xrdp -<span class="keyword">y</span></span><br></pre></td></tr></table></figure><h2><span id="step4-配置xrdp">Step.4 配置Xrdp</span></h2><p>需要通过xrdp连接到桌面，<br>需要正确配置相关信息并填充到.xsession文件（针对每个用户）<br>或&#x2F;etc&#x2F;startwm.sh（针对所有用户）<br>命令如下：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">echo</span> unity&gt;~/.xsession</span><br><span class="line">或者</span><br><span class="line">sudo sed -i.bak <span class="string">&#x27;/fi/a #xrdp multi-users \n unity \n&#x27;</span> /etc/xrdp/startwm.<span class="keyword">sh</span></span><br></pre></td></tr></table></figure><h2><span id="step5-重启xrdp服务">Step.5 重启Xrdp服务</span></h2><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo service xrdp restart</span><br></pre></td></tr></table></figure><h2><span id="step6-开启桌面共享功能">Step.6 开启桌面共享功能</span></h2><img src="/2019/03/05/%E9%80%9A%E8%BF%87Xrdp%E5%AE%9E%E7%8E%B0Windows%E8%BF%9C%E7%A8%8B%E8%AE%BF%E9%97%AEUbuntu-16-04/桌面共享.jpg" alt><p>系统-&gt;首选项-&gt;桌面共享，或者直接搜索桌面共享功能<br>进入后<br>将【允许其他人查看您的桌面】勾上，<br>【自动配置UPnP路由器开放和转发接口】勾上，如图所示</p><img src="/2019/03/05/%E9%80%9A%E8%BF%87Xrdp%E5%AE%9E%E7%8E%B0Windows%E8%BF%9C%E7%A8%8B%E8%AE%BF%E9%97%AEUbuntu-16-04/桌面共享2.jpg" alt><p>之后配置基本结束。Windows可以通过mstc直接通过IP访问Ubuntu。</p><hr><p>注：Ubuntu18尝试后似乎没有桌面共享功能的选项。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;​		目前网上的大量教程都是需要安装xfac4或者xubuntu桌面系统才能实现远程连接。因为xrdp支持在13.10之后版本就已经不支持的Gnome了和原生Unity桌面，所以网上很多方法都是安装能够被xdrp支持的第三方xfac4或者xubuntu桌面系统间接达到远程控制Ubuntu。&lt;br&gt;​		本文提供如何使用Xrdp访问原生Ubuntu桌面。&lt;/p&gt;</summary>
    
    
    
    <category term="Ubuntu" scheme="https://values.keys.moe/categories/Ubuntu/"/>
    
    
    <category term="Ubuntu" scheme="https://values.keys.moe/tags/Ubuntu/"/>
    
  </entry>
  
  <entry>
    <title>树莓派人脸识别-face-recognition的安装与应用</title>
    <link href="https://values.keys.moe/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB-face-recognition%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E5%BA%94%E7%94%A8/"/>
    <id>https://values.keys.moe/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB-face-recognition%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E5%BA%94%E7%94%A8/</id>
    <published>2019-03-02T03:06:57.000Z</published>
    <updated>2022-09-27T17:08:54.800Z</updated>
    
    <content type="html"><![CDATA[<img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB-face-recognition%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E5%BA%94%E7%94%A8/人脸识别.gif" alt><span id="more"></span><img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB-face-recognition%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E5%BA%94%E7%94%A8/face_recongnition.png" alt>该库可以通过python或者命令行即可实现人脸识别的功能。使用dlib深度学习人脸识别技术构建，在户外脸部检测数据库基准（Labeled Faces in the Wild）上的准确率为99.38%。<h2><span id="安装过程"><strong>安装过程</strong></span></h2><p>先在终端下安装一大堆需要的库：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-<span class="built_in">get</span> <span class="keyword">update</span> sudo apt-<span class="built_in">get</span> install build-essential \</span><br><span class="line">cmake \</span><br><span class="line">gfortran \</span><br><span class="line">git \</span><br><span class="line">wget \</span><br><span class="line">curl \</span><br><span class="line">graphicsmagick \</span><br><span class="line">libgraphicsmagick1-dev \</span><br><span class="line">libatlas-dev \</span><br><span class="line">libavcodec-dev \</span><br><span class="line">libavformat-dev \</span><br><span class="line">libboost-<span class="keyword">all</span>-dev \</span><br><span class="line">libgtk2.<span class="number">0</span>-dev \</span><br><span class="line">libjpeg-dev \</span><br><span class="line">liblapack-dev \</span><br><span class="line">libswscale-dev \</span><br><span class="line">pkg-config \</span><br><span class="line"><span class="keyword">python3</span>-dev \</span><br><span class="line"><span class="keyword">python3</span>-numpy \</span><br><span class="line"><span class="keyword">python3</span>-pip \</span><br><span class="line">zip</span><br></pre></td></tr></table></figure><p>如果使用树莓派的摄像头（CSI接口），执行下面的命令：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-<span class="built_in">get</span> install <span class="keyword">python3</span>-picamera</span><br><span class="line">sudo pip3 install --upgrade picamera[array]</span><br></pre></td></tr></table></figure><p>下载安装dlib：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mkdir</span> -<span class="keyword">p</span> dlib </span><br><span class="line">git clone -<span class="keyword">b</span> <span class="string">&#x27;v19.6&#x27;</span> --single-branch https://github.<span class="keyword">com</span>/davisking/dlib.git dlib/ </span><br><span class="line"><span class="keyword">cd</span> ./dlib</span><br><span class="line">sudo <span class="keyword">python3</span> setup.<span class="keyword">py</span> install --<span class="keyword">compiler</span>-flags <span class="string">&quot;-mfpu=neon&quot;</span></span><br></pre></td></tr></table></figure><p>安装face_recognition：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo pip3 install face_recognition</span><br></pre></td></tr></table></figure><p>下载示例代码并尝试运行（可选）：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git clone --single-branch https://github.<span class="keyword">com</span>/ageitgey/face_recognition.git</span><br><span class="line"><span class="keyword">cd</span> ./face_recognition/examples</span><br><span class="line"><span class="keyword">python3</span> facerec_on_raspberry_pi.<span class="keyword">py</span></span><br></pre></td></tr></table></figure><p><strong>注</strong>：过程缺少库时，使用</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">apt-cache <span class="built_in">search</span> 库名</span><br></pre></td></tr></table></figure><p>来搜索到那个库的安装包，然后用</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-<span class="built_in">get</span> install 包名</span><br></pre></td></tr></table></figure><p>来安装。</p><p><strong>例如</strong>：缺少了libatlas.so.3，那我们就用<strong>apt-cache search libatlas</strong>来搜索，发现它的包名叫libatlas3-base，所以我们用<strong>sudo apt-get install libatlas3-base</strong>来安装。<br>后面测试摄像头的时候也会遇到这样的问题，解决办法是一样的。 </p><p>待把CSI接口树莓派摄像头装上后，在raspi-config中启用摄像头，然后重启。<br>（详见博客内关于CSI摄像头的另一篇文章）</p><p>运行一下实时人脸识别的代码（可选）：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">python3</span> facerec_from_webcam_faster.<span class="keyword">py</span></span><br></pre></td></tr></table></figure><p>过程可能报错，import cv2的时候缺少库，然后根据提示用之前安装方法安装就好了。装完一个库再运行的时候，发现又提示缺少别的库，然后再安装缺少的库。反复个多次，把缺少的库都装好即可。</p><p>再次运行的时候，会报别的错误，出错的代码是</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">small_frame = cv2.<span class="keyword">resize</span>(frame, (<span class="number">0</span>, <span class="number">0</span>), fx=<span class="number">0.25</span>, fy=<span class="number">0.25</span>)</span><br></pre></td></tr></table></figure><p>这是因为video_capture.read()没有读到图片。<br>树莓派中的camera module是放在&#x2F;boot&#x2F;目录中以固件形式加载的，不是一个标准的V4L2的摄像头驱动，所以加载起来之后会找不到&#x2F;dev&#x2F;video0的设备节点。<br><strong>解决方法</strong>：转载<a href="https://blog.csdn.net/deiki/article/details/71123947">https://blog.csdn.net/deiki/article/details/71123947</a></p><p>之后可以使用下面的命令来加载驱动模块：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo modprobe bcm2835-v4l2</span><br></pre></td></tr></table></figure><p>如果想开机自动加载，我们可以修改&#x2F;etc&#x2F;modules文件，添加一行：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bcm2835-v4l2</span><br></pre></td></tr></table></figure><p>如下图所示：<br><img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB-face-recognition%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E5%BA%94%E7%94%A8/bcm2835-v4l2.png" alt></p><p>安装过程基本完毕。</p><h2><span id="个人提供的样例代码"><strong>个人提供的样例代码：</strong></span></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env </span></span><br><span class="line"><span class="comment">#coding=utf-8</span></span><br><span class="line"><span class="comment">#从目录中读取一堆文件</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> face_recognition</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> RPi.GPIO</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">RPi.GPIO.setmode(RPi.GPIO.BCM)</span><br><span class="line"></span><br><span class="line">RPi.GPIO.setwarnings(<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">buzzer=<span class="number">4</span></span><br><span class="line">RPi.GPIO.setup(buzzer,RPi.GPIO.OUT)</span><br><span class="line"></span><br><span class="line">RPi.GPIO.output(buzzer,<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">strangerAppear=<span class="literal">False</span></span><br><span class="line">strangerNum=<span class="number">0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">demo_filelist=[]</span><br><span class="line">demo_face_encodings=[]</span><br><span class="line">demo_face_names=[]</span><br><span class="line"><span class="keyword">for</span> f <span class="keyword">in</span> os.listdir(<span class="string">&#x27;/home/pi/demo/face_recognition/jpg//&#x27;</span>):</span><br><span class="line">    demo_filelist.append(f)</span><br><span class="line">    demo_face_names.append(f[:-<span class="number">4</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> filename <span class="keyword">in</span> demo_filelist:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">u&#x27;正在加载....&#x27;</span>+filename)</span><br><span class="line">    demo_image = face_recognition.load_image_file(<span class="string">&#x27;/home/pi/demo/face_recognition/jpg//&#x27;</span>+filename)</span><br><span class="line">    face_encoding = face_recognition.face_encodings(demo_image)[<span class="number">0</span>]</span><br><span class="line">    demo_face_encodings.append(face_encoding)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize some variables</span></span><br><span class="line">face_locations = []</span><br><span class="line">face_encodings = []</span><br><span class="line">face_names = []</span><br><span class="line">process_this_frame = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Get a reference to webcam #0 (the default one)</span></span><br><span class="line">video_capture = cv2.VideoCapture(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    <span class="comment"># Grab a single frame of video</span></span><br><span class="line">    ret, frame = video_capture.read()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Resize frame of video to 1/4 size for faster face recognition processing</span></span><br><span class="line">    small_frame = cv2.resize(frame, (<span class="number">0</span>, <span class="number">0</span>), fx=<span class="number">0.3</span>, fy=<span class="number">0.3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Only process every other frame of video to save time</span></span><br><span class="line">    <span class="keyword">if</span> process_this_frame==<span class="number">1</span>:</span><br><span class="line">        <span class="comment"># Find all the faces and face encodings in the current frame of video</span></span><br><span class="line">        face_locations = face_recognition.face_locations(small_frame)</span><br><span class="line">        face_encodings = face_recognition.face_encodings(small_frame, face_locations)</span><br><span class="line">   <span class="comment">#     print(u&quot;我检测到了&#123;&#125;张脸。&quot;.format(len(face_locations)))</span></span><br><span class="line">        face_names = []</span><br><span class="line">        <span class="keyword">for</span> face_encoding <span class="keyword">in</span> face_encodings:</span><br><span class="line">        <span class="comment"># See if the face is a match for the known face(s)</span></span><br><span class="line">            <span class="keyword">match</span> = face_recognition.compare_faces(demo_face_encodings, face_encoding,tolerance=<span class="number">0.6</span>)<span class="comment">#</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="keyword">match</span>)</span><br><span class="line">            name = <span class="string">&quot;Unknown&quot;</span></span><br><span class="line">            i=-<span class="number">1</span></span><br><span class="line">            <span class="keyword">for</span> m <span class="keyword">in</span> <span class="keyword">match</span>:</span><br><span class="line">                i+=<span class="number">1</span></span><br><span class="line">                <span class="keyword">if</span> m:</span><br><span class="line">                    name= demo_face_names[i]</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span>(name==<span class="string">&quot;Unknown&quot;</span>):</span><br><span class="line">                RPi.GPIO.output(buzzer,<span class="literal">False</span>)</span><br><span class="line">                frame_1=cv2.flip(frame,<span class="number">1</span>)</span><br><span class="line">                path=<span class="string">&quot;/home/pi/demo/face_recognition/&quot;</span>+<span class="built_in">str</span>(strangerNum)+<span class="string">&quot;.png&quot;</span></span><br><span class="line">                cv2.imwrite(path,frame_1)</span><br><span class="line">                strangerNum=strangerNum+<span class="number">1</span></span><br><span class="line">                strangerAppear=<span class="literal">True</span></span><br><span class="line">            face_names.append(name)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Display the results</span></span><br><span class="line">    <span class="keyword">for</span> (top, right, bottom, left), name <span class="keyword">in</span> <span class="built_in">zip</span>(face_locations, face_names):</span><br><span class="line">        <span class="comment"># Draw a box around the face</span></span><br><span class="line">        <span class="comment">#cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Draw a label with a name below the face</span></span><br><span class="line"><span class="comment">#        cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 0, 255), 5)</span></span><br><span class="line">        font = cv2.FONT_HERSHEY_DUPLEX<span class="comment">#FONT_HERSHEY_DUPLEX</span></span><br><span class="line"><span class="comment">#        print(name)</span></span><br><span class="line"><span class="comment">#        boxsize, _ = cv2.getTextSize(fs.string, fs.face, fs.fsize, fs.thick)</span></span><br><span class="line"> <span class="comment">#       locx = int((right+left)/2-25 - 14*len(name)/2)</span></span><br><span class="line">        cv2.putText(frame, name, (<span class="number">20</span>,<span class="number">20</span>), font, <span class="number">1.0</span>, (<span class="number">0</span>, <span class="number">0</span>, <span class="number">255</span>), <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    process_this_frame += <span class="number">1</span><span class="comment"># not process_this_frame</span></span><br><span class="line">    <span class="keyword">if</span> process_this_frame&gt;<span class="number">1</span>:</span><br><span class="line">        process_this_frame=<span class="number">1</span></span><br><span class="line">    <span class="comment"># Display the resulting image</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">############</span></span><br><span class="line">    cv2.imshow(<span class="string">&#x27;Video&#x27;</span>, frame)</span><br><span class="line">    cv2.moveWindow(<span class="string">&#x27;Video&#x27;</span>,<span class="number">600</span>,<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Hit &#x27;q&#x27; on the keyboard to quit!</span></span><br><span class="line">    <span class="keyword">if</span> cv2.waitKey(<span class="number">1</span>) &amp; <span class="number">0xFF</span> == <span class="built_in">ord</span>(<span class="string">&#x27;q&#x27;</span>):</span><br><span class="line">        RPi.GPIO.output(buzzer,<span class="literal">True</span>)</span><br><span class="line">        strangerAppear=<span class="literal">False</span></span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Release handle to the webcam</span></span><br><span class="line">video_capture.release()</span><br><span class="line">cv2.destroyAllWindows()</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">入门介绍</summary>
    
    
    
    <category term="树莓派" scheme="https://values.keys.moe/categories/%E6%A0%91%E8%8E%93%E6%B4%BE/"/>
    
    
    <category term="树莓派" scheme="https://values.keys.moe/tags/%E6%A0%91%E8%8E%93%E6%B4%BE/"/>
    
    <category term="Python" scheme="https://values.keys.moe/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>树莓派CSI摄像头的连接与常用指令</title>
    <link href="https://values.keys.moe/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BECSI%E6%91%84%E5%83%8F%E5%A4%B4%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%B8%8E%E5%B8%B8%E7%94%A8%E6%8C%87%E4%BB%A4/"/>
    <id>https://values.keys.moe/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BECSI%E6%91%84%E5%83%8F%E5%A4%B4%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%B8%8E%E5%B8%B8%E7%94%A8%E6%8C%87%E4%BB%A4/</id>
    <published>2019-03-02T01:28:19.000Z</published>
    <updated>2022-09-28T06:03:44.256Z</updated>
    
    <content type="html"><![CDATA[<img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BECSI%E6%91%84%E5%83%8F%E5%A4%B4%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%B8%8E%E5%B8%B8%E7%94%A8%E6%8C%87%E4%BB%A4/CSI摄像头.jpg" alt><span id="more"></span><h1><span id="安装树莓派摄像头模块">安装树莓派摄像头模块</span></h1><p>​1、找到 CSI 接口(CSI接口在以太网接口旁边)，掀起深色胶带。<br>​2、拉起 CSI 接口挡板。<br>​3、拿起你的摄像头模块，将贴在镜头上的塑料保护膜撕掉。确保黄色部分的PCB(有字的一面)是安装完美的（可以轻轻按一下黄色的部分来保证安装完美）。<br>​4、将排线插入CSI接口。记住，有蓝色胶带的一面应该面向以太网接口方向。同样，这时也确认一下排线安装好了之后，将挡板拉下。</p><img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BECSI%E6%91%84%E5%83%8F%E5%A4%B4%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%B8%8E%E5%B8%B8%E7%94%A8%E6%8C%87%E4%BB%A4/安装图.jpg" alt><h1><span id="在树莓派上启用摄像头模块">在树莓派上启用摄像头模块</span></h1><p>在安装完摄像头模块之后，首先要确认你已经升级了树莓派系统并应用了最新的固件。<br>终端下可以输入以下命令来操作：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-<span class="built_in">get</span> <span class="keyword">update</span>   </span><br><span class="line">sudo apt-<span class="built_in">get</span> upgrade </span><br></pre></td></tr></table></figure><p>运行树莓派配置工具来激活摄像头模块：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo raspi-config</span><br></pre></td></tr></table></figure><p>移动光标至菜单中的 “Enable Camera（启用摄像头）”，将其设为Enable（启用状态）。完成之后重启树莓派。</p><h1><span id="安装驱动使能树莓派的相关模块">安装驱动使能树莓派的相关模块</span></h1><h2><span id="1-添加驱动程序文件进来">1、添加驱动程序文件进来：</span></h2><p>终端输入指令</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo <span class="keyword">vim</span> /etc/modules</span><br></pre></td></tr></table></figure><p>在最后添加如下的代码：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bcm2835-v4l2</span><br></pre></td></tr></table></figure><p>这样就完成了在启动过程中加载camera驱动的前提。</p><h2><span id="2-修改raspberry的启动配置使能项">2、修改Raspberry的启动配置使能项：</span></h2><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo raspi-config</span><br></pre></td></tr></table></figure> <img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BECSI%E6%91%84%E5%83%8F%E5%A4%B4%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%B8%8E%E5%B8%B8%E7%94%A8%E6%8C%87%E4%BB%A4/配置1.jpg" alt><p>(由于系统版本，树莓派版本不同，显示设置可能不同，但基本大同小异。)<br>选择Interfacing Option，选中Select然后Enter进入，如下图所示： </p><img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BECSI%E6%91%84%E5%83%8F%E5%A4%B4%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%B8%8E%E5%B8%B8%E7%94%A8%E6%8C%87%E4%BB%A4/配置2.jpg" alt><h2><span id="3-检查x2fdev下面是否存在摄像头设备">3、检查&#x2F;dev下面是否存在摄像头设备</span></h2><p>重启完之后，我们的基本的操作就完成了，下来来看看&#x2F;dev下面是否存在摄像头设备的问题：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">ls</span> -al /dev/ | grep video</span><br></pre></td></tr></table></figure><img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BECSI%E6%91%84%E5%83%8F%E5%A4%B4%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%B8%8E%E5%B8%B8%E7%94%A8%E6%8C%87%E4%BB%A4/检查.jpg" alt><h1><span id="使用操作树莓派的摄像头">使用操作树莓派的摄像头</span></h1><p>下面简单的使用操作树莓派的摄像头：<br>1、我们使用rapistill指令来截图</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">raspistill -<span class="keyword">o</span> image.jpg</span><br></pre></td></tr></table></figure><p>raspistill命令的相关参数和实验的具体效果：<br>-v：调试信息查看<br>-w：图像宽度<br>-h：图像高度<br>-rot：图像旋转角度，只支持 0、90、180、270 度（这里说明一下，测试发现其他角度的输入都会被转换到这四个角度之上）<br>-o：图像输出地址，例如image.jpg，如果文件名为“-”，将输出发送至标准输出设备<br>-t：获取图像前等待时间，默认为5000，即5秒<br>-tl：多久执行一次图像抓取<br>使用raspivid指令来生成.h246的文件</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">raspivid -<span class="keyword">o</span> mykeychain.h264 -t <span class="number">10000</span> -<span class="keyword">w</span> <span class="number">1280</span> -h <span class="number">720</span> </span><br></pre></td></tr></table></figure><p>如果你想改变拍摄时长，只要通过 “-t” 选项来设置你想要的长度就行了（单位是毫秒）。<br>如果你想改变图像的分辨率，使用 “-w” 和 “-h” 选项将分辨率降为 1280x720等等。</p>]]></content>
    
    
    <summary type="html">入门介绍</summary>
    
    
    
    <category term="树莓派" scheme="https://values.keys.moe/categories/%E6%A0%91%E8%8E%93%E6%B4%BE/"/>
    
    
    <category term="树莓派" scheme="https://values.keys.moe/tags/%E6%A0%91%E8%8E%93%E6%B4%BE/"/>
    
    <category term="Python" scheme="https://values.keys.moe/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>树莓派常见问题与解决方案</title>
    <link href="https://values.keys.moe/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%E4%B8%8E%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/"/>
    <id>https://values.keys.moe/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%E4%B8%8E%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/</id>
    <published>2019-03-02T01:26:05.000Z</published>
    <updated>2022-09-28T06:28:27.634Z</updated>
    
    <content type="html"><![CDATA[<p>又是老问题？（笑</p><span id="more"></span><h1><span id="树莓派有密码的网络联网失败的问题与解决">树莓派有密码的网络联网失败的问题与解决</span></h1><p><strong>在保证系统无任何修改的情况下：</strong><br>树莓派出现无法连接有密码的网络，而可以连接无密码网络的情况。<br>问题出现原因：<strong>电池电源电流供应不足</strong>。<br>解决方案：给树莓派micrioUSB处通以<strong>正常电源</strong>，问题得以解决。</p><h1><span id="修改树莓派热点的名称和密码">修改树莓派热点的名称和密码</span></h1><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo nano /etc/hostapd/hostapd.<span class="keyword">conf</span> </span><br></pre></td></tr></table></figure><p>其中ssid为热点的名称、wpa_passphrase为热点的密码<br><img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%E4%B8%8E%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/修改树莓派热点的名称和密码.jpg" alt></p><h1><span id="树莓派更换国内可用镜像源">树莓派更换国内可用镜像源</span></h1><p>***最新系统不用折腾换源了，亲测官方源可以使用。</p><p><strong>查看树莓派的镜像列表</strong><br><a href="http://www.raspbian.org/RaspbianMirrors">http://www.raspbian.org/RaspbianMirrors</a><br>操作</p><h2><span id="1-编辑sourceslist">1. 编辑sources.list</span></h2><p>打开终端 输入</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo nano /etc/apt/sources.<span class="keyword">list</span></span><br></pre></td></tr></table></figure><p>用#注释或直接删除原有的内容，新增两条：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">deb</span> http://mirrors.tuna.tsinghua.edu.<span class="keyword">cn</span>/raspbian/raspbian/ stretch main contrib non-free rpi</span><br><span class="line">#deb-src http://mirrors.tuna.tsinghua.edu.<span class="keyword">cn</span>/raspbian/raspbian/ stretch main contrib non-free rpi</span><br></pre></td></tr></table></figure><p>ctrl+x 保存并退出。</p><h2><span id="2-编辑raspilist">2. 编辑raspi.list</span></h2><p>sudo nano &#x2F;etc&#x2F;apt&#x2F;sources.list.d&#x2F;raspi.list<br>用#注释或直接删除原有的内容，新增两条：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">deb</span> http://mirror.tuna.tsinghua.edu.<span class="keyword">cn</span>/raspberrypi/ stretch main ui</span><br><span class="line">#deb-src http://mirror.tuna.tsinghua.edu.<span class="keyword">cn</span>/raspberrypi/ stretch main ui</span><br></pre></td></tr></table></figure><p>ctrl+x 保存并退出。</p><h2><span id="3-更新软件源列表">3. 更新软件源列表</span></h2><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-<span class="built_in">get</span> <span class="keyword">update</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;又是老问题？（笑&lt;/p&gt;</summary>
    
    
    
    <category term="树莓派" scheme="https://values.keys.moe/categories/%E6%A0%91%E8%8E%93%E6%B4%BE/"/>
    
    
    <category term="树莓派" scheme="https://values.keys.moe/tags/%E6%A0%91%E8%8E%93%E6%B4%BE/"/>
    
  </entry>
  
  <entry>
    <title>树莓派空气质量检测仪-攀藤G5003ST的连接与使用</title>
    <link href="https://values.keys.moe/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%E6%A3%80%E6%B5%8B%E4%BB%AA-%E6%94%80%E8%97%A4G5003ST%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%B8%8E%E4%BD%BF%E7%94%A8/"/>
    <id>https://values.keys.moe/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%E6%A3%80%E6%B5%8B%E4%BB%AA-%E6%94%80%E8%97%A4G5003ST%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%B8%8E%E4%BD%BF%E7%94%A8/</id>
    <published>2019-03-02T01:23:34.000Z</published>
    <updated>2022-09-28T05:44:45.667Z</updated>
    
    <content type="html"><![CDATA[<img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%E6%A3%80%E6%B5%8B%E4%BB%AA-%E6%94%80%E8%97%A4G5003ST%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%B8%8E%E4%BD%BF%E7%94%A8/5003ST正.jpg" alt><span id="more"></span><img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%E6%A3%80%E6%B5%8B%E4%BB%AA-%E6%94%80%E8%97%A4G5003ST%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%B8%8E%E4%BD%BF%E7%94%A8/5003ST反.jpg" alt><h1><span id="攀藤g5003st数字接口定义">攀藤G5003ST数字接口定义</span></h1><p><strong>pin1(VCC)</strong>:连接电压为3.3V&#x2F;5V电压<br><strong>pin2(GND)</strong> 电源-<br>**pin3(SET待机位置)**，设置管脚?&#x2F;TTL电平@3.3V，高电平或悬空为正常工作状态，低电平为休眠状态，该引脚可悬空。<br><strong>pin4(RXD串口接收管脚)</strong> 传感器接收来自树莓派的信号数据，如果不需要可以悬空<br><strong>pin5(TXD串口发送管脚)</strong> 传感器将信号发送给树莓派<br><strong>pin6(RESET)</strong> 模块复位信号&#x2F;TTL电平@3.3V，低复位，如果不需要使用可以悬空<br><strong>pin7(NC)</strong> : No internal connection. 无内部连接，不需连接。<br><strong>Pin8(NC&#x2F;PWM)</strong> : PWM周期为1s，其中低电平对应大气环境下的PM2.5质量浓度数据，每1ms低电平代表1ug&#x2F;m?。例如低电平时间长度为210ms，则代表此时PM2.5质量浓度值（大气环境）为210ug&#x2F;m?<br><strong>（pin7和pin8为程序内部调试使用，应用电路中应该使其悬空）</strong></p><p>树莓派Pi3的UART（ttyAMA0）是被蓝牙默认占用的，更改起来十分困难，在实体机上尝试多次无果后决定使用USB TO TTL转接口，直接将PMS5003ST接至树莓派的USB接口上，这样可以直接在&#x2F;dev&#x2F;tty中直接检索到USB0，即为传感器。</p><img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%E6%A3%80%E6%B5%8B%E4%BB%AA-%E6%94%80%E8%97%A4G5003ST%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%B8%8E%E4%BD%BF%E7%94%A8/接线.jpg" alt><img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%E6%A3%80%E6%B5%8B%E4%BB%AA-%E6%94%80%E8%97%A4G5003ST%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%B8%8E%E4%BD%BF%E7%94%A8/接线2.jpg" alt><img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%E6%A3%80%E6%B5%8B%E4%BB%AA-%E6%94%80%E8%97%A4G5003ST%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%B8%8E%E4%BD%BF%E7%94%A8/pin口.png" alt><h1><span id="攀藤g5003st技术指标">攀藤G5003ST技术指标</span></h1><img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%E6%A3%80%E6%B5%8B%E4%BB%AA-%E6%94%80%E8%97%A4G5003ST%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%B8%8E%E4%BD%BF%E7%94%A8/技术指标1.png" alt><img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%E6%A3%80%E6%B5%8B%E4%BB%AA-%E6%94%80%E8%97%A4G5003ST%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%B8%8E%E4%BD%BF%E7%94%A8/技术指标2.png" alt><img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%E6%A3%80%E6%B5%8B%E4%BB%AA-%E6%94%80%E8%97%A4G5003ST%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%B8%8E%E4%BD%BF%E7%94%A8/技术指标3.png" alt><img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%E6%A3%80%E6%B5%8B%E4%BB%AA-%E6%94%80%E8%97%A4G5003ST%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%B8%8E%E4%BD%BF%E7%94%A8/技术指标4.png" alt><h1><span id="攀藤g5003st输出结果分析">攀藤G5003ST输出结果分析</span></h1><p>1.颗粒物浓度：主要输出为单位体积内各浓度颗粒物质量以及个数，其中颗粒物个数的单位体积为0.1升，质量浓度单位为：微克&#x2F;立方米（μg&#x2F;m3）。<br>此外传感器输出分为主动输出和被动输出两种状态。<br>传感器上电后默认状态为主动输出，即传感器主动向主机发送串行数据，时间间隔为200 ~ 800ms，空气中颗粒物浓度越高，时间间隔越短。<br>主动输出又分为两种模式：平稳模式和快速模式。<br>在空气中颗粒物浓度变化较小时，传感器输出为平稳模式，即每三次输出同样的一组数值，实际数据更新周期约为2s。<br>当空气中颗粒物浓度变化较大时，传感器输出自动切换为快速模式，每次输出都是新的数值，实际数据更新周期为200~800ms。<br>PWM输出：PMS3XXXP系列产品带有PWM输出，PWM周期为1秒，低电平时间长度代表PM2.5浓度（大气环境下），每1ms低电平代表1ug&#x2F;m3。<br>例如：低电平时间长度为210ms，则代表此时PM2.5质量浓度值（大气环境）为210ug&#x2F;m3<br>2.甲醛浓度输出：单位体积内甲醛质量浓度，单位为毫克&#x2F;立方米<br>3.温湿度输出：输出吸入传感器内部的采样空气温度及湿度。</p><h1><span id="攀藤g5003st对外输出格式">攀藤G5003ST对外输出格式</span></h1><img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%E6%A3%80%E6%B5%8B%E4%BB%AA-%E6%94%80%E8%97%A4G5003ST%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%B8%8E%E4%BD%BF%E7%94%A8/输出格式0.png" alt><img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%E6%A3%80%E6%B5%8B%E4%BB%AA-%E6%94%80%E8%97%A4G5003ST%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%B8%8E%E4%BD%BF%E7%94%A8/输出格式1.png" alt><img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%E6%A3%80%E6%B5%8B%E4%BB%AA-%E6%94%80%E8%97%A4G5003ST%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%B8%8E%E4%BD%BF%E7%94%A8/输出格式2.png" alt><img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%E6%A3%80%E6%B5%8B%E4%BB%AA-%E6%94%80%E8%97%A4G5003ST%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%B8%8E%E4%BD%BF%E7%94%A8/输出格式3.png" alt><h1><span id="python程序实现">python程序实现</span></h1><p>读取数据：<br><img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%E6%A3%80%E6%B5%8B%E4%BB%AA-%E6%94%80%E8%97%A4G5003ST%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%B8%8E%E4%BD%BF%E7%94%A8/读取数据.png" alt></p><p>分析数据：<br><img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%E6%A3%80%E6%B5%8B%E4%BB%AA-%E6%94%80%E8%97%A4G5003ST%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%B8%8E%E4%BD%BF%E7%94%A8/分析数据.png" alt></p><p>得出数据结论：<br><img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%E6%A3%80%E6%B5%8B%E4%BB%AA-%E6%94%80%E8%97%A4G5003ST%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%B8%8E%E4%BD%BF%E7%94%A8/数据结论.png" alt></p>]]></content>
    
    
    <summary type="html">入门介绍</summary>
    
    
    
    <category term="树莓派" scheme="https://values.keys.moe/categories/%E6%A0%91%E8%8E%93%E6%B4%BE/"/>
    
    
    <category term="树莓派" scheme="https://values.keys.moe/tags/%E6%A0%91%E8%8E%93%E6%B4%BE/"/>
    
    <category term="Python" scheme="https://values.keys.moe/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>使用lirc红外控制树莓派</title>
    <link href="https://values.keys.moe/2019/03/02/%E4%BD%BF%E7%94%A8lirc%E7%BA%A2%E5%A4%96%E6%8E%A7%E5%88%B6%E6%A0%91%E8%8E%93%E6%B4%BE/"/>
    <id>https://values.keys.moe/2019/03/02/%E4%BD%BF%E7%94%A8lirc%E7%BA%A2%E5%A4%96%E6%8E%A7%E5%88%B6%E6%A0%91%E8%8E%93%E6%B4%BE/</id>
    <published>2019-03-02T01:20:33.000Z</published>
    <updated>2022-09-28T06:09:54.964Z</updated>
    
    <content type="html"><![CDATA[<img src="/2019/03/02/%E4%BD%BF%E7%94%A8lirc%E7%BA%A2%E5%A4%96%E6%8E%A7%E5%88%B6%E6%A0%91%E8%8E%93%E6%B4%BE/红外传感器.png" alt><span id="more"></span><p>红外传感器<br>引脚从上到下分别为<br>IO GND VCC</p><h1><span id="lirc的安装与使用">lirc的安装与使用</span></h1><p>使用红外，首先需要安装树莓派的lirc模块<br>LIRC (Linux Infrared remote control)是一个linux系统下开源的软件包。这个软件可以让Linux系统接收及发送红外线信号。<br>注意事项：<br><strong>安装：</strong></p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-<span class="built_in">get</span> install lirc</span><br></pre></td></tr></table></figure><p>修改以下几处：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo leafpad /etc/lirc/hardware.<span class="keyword">conf</span></span><br><span class="line">LIRCD_ATGS=<span class="string">&quot;&quot;</span></span><br><span class="line">DRIVER=<span class="string">&quot;default&quot;</span></span><br><span class="line">DEVICE=<span class="string">&quot;/dev/lirc0&quot;</span></span><br><span class="line">MODULES=<span class="comment">&quot;lirc-rpi</span></span><br></pre></td></tr></table></figure><p>终端执行</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo leafpad /etc/modules</span><br></pre></td></tr></table></figure><p>添加下面两行到模块配置文件：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">lirc-dev</span><br><span class="line"><span class="comment">#红外接收模块的OUT接口接到了树莓派的GPIO18</span></span><br><span class="line"><span class="comment">#因为本例中未用到红外发射模块，所以后面的gpio_out_pin可以不写</span></span><br><span class="line">lirc-rpi gpio_in_pin=<span class="number">18</span> gpio_out_pin=<span class="number">17</span></span><br></pre></td></tr></table></figure><p><strong>如测试时报错-ERROR: could not insert ‘lirc_rpi’: No&amp;nbs</strong><br>解决办法：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo <span class="keyword">vi</span> /boot/config.txt</span><br></pre></td></tr></table></figure><p>找到：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">#dtoverlay=lirc-rpi</span><br></pre></td></tr></table></figure><p>把前面的“#”号去掉， 然后重启系统即可</p><p><strong>测试红外线接收功能</strong><br>首先关闭lirc软件，然后执行如下命令：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo /etc/init.d/lirc <span class="keyword">stop</span></span><br><span class="line">mode2 -d /dev/lirc0</span><br></pre></td></tr></table></figure><p>这时候提示</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">no</span> such <span class="keyword">file</span> <span class="built_in">or</span> directory</span><br></pre></td></tr></table></figure><p>经过查看发现 &#x2F;dev 下面没有 lirc0 这个module，发现在&#x2F;boot&#x2F;config.txt里面dtoverlay&#x3D;lirc-rpi<br>取消注释，然后reboot，问题解决。</p><p>再次执行</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mode2 -d /dev/lirc0</span><br></pre></td></tr></table></figure><p>如果弹出</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Partial <span class="keyword">read</span> <span class="number">8</span> bytes <span class="keyword">on</span> /dev/lirc0pi@raspberrypi:~ $</span><br></pre></td></tr></table></figure><p>发生错误，解决方案：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">changed the following two lines in </span><br><span class="line">    /etc/lirc/lirc_options.<span class="keyword">conf</span></span><br><span class="line">driver = default</span><br></pre></td></tr></table></figure><p>（尝试过程中第一次仍然无效，但是第二次重装系统后正常，目前未知原理）<br>如果显示下面内容</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">pulse <span class="number">629</span></span><br><span class="line">space <span class="number">518</span></span><br><span class="line">pulse <span class="number">627</span></span><br><span class="line">space <span class="number">523</span></span><br><span class="line">pulse <span class="number">628</span></span><br><span class="line">space <span class="number">523</span></span><br><span class="line">pulse <span class="number">631</span></span><br><span class="line">space <span class="number">517</span></span><br><span class="line">pulse <span class="number">629</span></span><br></pre></td></tr></table></figure><p>则说明接收正常.</p><p>#协议<br>采用脉宽调制的串行码，以脉宽为0.565ms、间隔0.56ms、周期为1.125ms的组合表示二进制的”0”；以脉宽为0.565ms、间隔1.685ms、周期为2.25ms的组合表示二进制的”1<br><img src="/2019/03/02/%E4%BD%BF%E7%94%A8lirc%E7%BA%A2%E5%A4%96%E6%8E%A7%E5%88%B6%E6%A0%91%E8%8E%93%E6%B4%BE/协议.png" alt><br>协议：<br>上述“0”和“1”组成的32位二进制码经38kHz的载频进行二次调制以提高发射效率，达到降低电源功耗的目的。然后再通过红外发射二极管产生红外线向空间发射，如下图。<br><img src="/2019/03/02/%E4%BD%BF%E7%94%A8lirc%E7%BA%A2%E5%A4%96%E6%8E%A7%E5%88%B6%E6%A0%91%E8%8E%93%E6%B4%BE/协议2.png" alt><br>|    引导码    |  用户识别码   |用户识别码反码 |   操作码    |  操作码反码   |<br>一个命令只发送一次，即使遥控器上的按键一直按着。但是会每110mS发送一次代码，直到遥控器按键释放。</p><p>重复码比较简单：一个9mS的AGC脉冲、2.25mS间隔、560uS脉冲。<br><img src="/2019/03/02/%E4%BD%BF%E7%94%A8lirc%E7%BA%A2%E5%A4%96%E6%8E%A7%E5%88%B6%E6%A0%91%E8%8E%93%E6%B4%BE/协议3.png" alt><br><img src="/2019/03/02/%E4%BD%BF%E7%94%A8lirc%E7%BA%A2%E5%A4%96%E6%8E%A7%E5%88%B6%E6%A0%91%E8%8E%93%E6%B4%BE/协议4.png" alt></p><h1><span id="读取并校验接收-对应到的红外信号">读取并校验接收、对应到的红外信号</span></h1><img src="/2019/03/02/%E4%BD%BF%E7%94%A8lirc%E7%BA%A2%E5%A4%96%E6%8E%A7%E5%88%B6%E6%A0%91%E8%8E%93%E6%B4%BE/代码1.png" alt>]]></content>
    
    
    <summary type="html">入门介绍</summary>
    
    
    
    <category term="树莓派" scheme="https://values.keys.moe/categories/%E6%A0%91%E8%8E%93%E6%B4%BE/"/>
    
    
    <category term="树莓派" scheme="https://values.keys.moe/tags/%E6%A0%91%E8%8E%93%E6%B4%BE/"/>
    
    <category term="Python" scheme="https://values.keys.moe/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>树莓派GPIO控制的初级应用-多色二极管的亮度调节与颜色变化</title>
    <link href="https://values.keys.moe/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BEGPIO%E6%8E%A7%E5%88%B6%E7%9A%84%E5%88%9D%E7%BA%A7%E5%BA%94%E7%94%A8-%E5%A4%9A%E8%89%B2%E4%BA%8C%E6%9E%81%E7%AE%A1%E7%9A%84%E4%BA%AE%E5%BA%A6%E8%B0%83%E8%8A%82%E4%B8%8E%E9%A2%9C%E8%89%B2%E5%8F%98%E5%8C%96/"/>
    <id>https://values.keys.moe/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BEGPIO%E6%8E%A7%E5%88%B6%E7%9A%84%E5%88%9D%E7%BA%A7%E5%BA%94%E7%94%A8-%E5%A4%9A%E8%89%B2%E4%BA%8C%E6%9E%81%E7%AE%A1%E7%9A%84%E4%BA%AE%E5%BA%A6%E8%B0%83%E8%8A%82%E4%B8%8E%E9%A2%9C%E8%89%B2%E5%8F%98%E5%8C%96/</id>
    <published>2019-03-02T01:16:06.000Z</published>
    <updated>2022-09-28T05:55:12.340Z</updated>
    
    <content type="html"><![CDATA[<img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BEGPIO%E6%8E%A7%E5%88%B6%E7%9A%84%E5%88%9D%E7%BA%A7%E5%BA%94%E7%94%A8-%E5%A4%9A%E8%89%B2%E4%BA%8C%E6%9E%81%E7%AE%A1%E7%9A%84%E4%BA%AE%E5%BA%A6%E8%B0%83%E8%8A%82%E4%B8%8E%E9%A2%9C%E8%89%B2%E5%8F%98%E5%8C%96/多色二极管.png" alt><span id="more"></span><p>引脚R：控制红色二极管亮&#x2F;灭<br>引脚G：控制绿色二极管亮&#x2F;灭<br>引脚B：控制蓝色二极管亮&#x2F;灭<br>GND：接地</p><p>在这里我们令各个引脚：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">R=<span class="number">13</span> </span><br><span class="line">G=<span class="number">26</span> </span><br><span class="line">B=<span class="number">16</span></span><br></pre></td></tr></table></figure><p>初始化各个引脚</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">RPi.GPIO.setup(R,RPi.GPIO.OUT)</span><br><span class="line">RPi.GPIO.setup(G,RPi.GPIO.OUT)</span><br><span class="line">RPi.GPIO.setup(B,RPi.GPIO.OUT)</span><br></pre></td></tr></table></figure><p>初始化脉宽调制为最大并启动</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">pwmR = RPi.GPIO.PWM(R,<span class="number">100</span>)</span><br><span class="line">pwmG = RPi.GPIO.PWM(G,<span class="number">100</span>)</span><br><span class="line">pwmB = RPi.GPIO.PWM(B,<span class="number">100</span>)</span><br><span class="line">pwmR.start(<span class="number">0</span>)</span><br><span class="line">pwmG.start(<span class="number">0</span>)</span><br><span class="line">pwmB.start(<span class="number">0</span>)</span><br></pre></td></tr></table></figure><p>以红色灯为例</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">红灯亮：RPi.GPIO.output(R,<span class="literal">True</span>)</span><br><span class="line">红灯灭：RPi.GPIO.output(R,<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><p>亮度调节：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">红灯<span class="number">1</span>级亮度：pwmR.ChangeDutyCycle(blightness[count1])  <span class="comment"># blightness[count1]=1</span></span><br><span class="line">红灯<span class="number">2</span>级亮度：pwmR.ChangeDutyCycle(blightness[count1])  <span class="comment"># blightness[count1]=20</span></span><br><span class="line">红灯<span class="number">3</span>级亮度：pwmR.ChangeDutyCycle(blightness[count1])  <span class="comment"># blightness[count1]=50</span></span><br><span class="line">红灯<span class="number">4</span>级亮度：pwmR.ChangeDutyCycle(blightness[count1])  <span class="comment"># blightness[count1]=100</span></span><br></pre></td></tr></table></figure><p>颜色调节：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">红灯：红色二极管亮</span><br><span class="line">蓝灯：蓝色二极管亮</span><br><span class="line">绿灯：绿色二极管亮</span><br><span class="line">黄色灯：红色、绿色二极管同时亮</span><br><span class="line">白色灯：红色、蓝色、绿色灯同时亮</span><br></pre></td></tr></table></figure><p>同时控制亮度和颜色方法：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">当接收到红外信号时，内部计数器+<span class="number">1</span> 并对<span class="number">25</span>求余</span><br><span class="line">当计数器为<span class="number">4</span> <span class="number">9</span> <span class="number">14</span> <span class="number">19</span> <span class="number">24</span>时，表示灯熄灭</span><br><span class="line"><span class="number">0</span>~<span class="number">3</span>时表示红灯，具体计数器数值表示亮度对应的字典下标</span><br><span class="line"><span class="number">5</span>~<span class="number">8</span>时表示绿灯，具体计数器数值表示亮度对应的字典下标</span><br><span class="line"><span class="number">10</span>~<span class="number">13</span>时表示蓝灯，具体计数器数值表示亮度对应的字典下标</span><br><span class="line"><span class="number">15</span>~<span class="number">18</span>时表示黄灯，具体计数器数值表示红灯和绿灯亮度对应的字典下标</span><br><span class="line"><span class="number">20</span>~<span class="number">23</span>时表示白灯，具体计数器数值表示红灯、绿灯和蓝灯亮度对应的字典下标</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">基于树莓派GPIO控制的介绍</summary>
    
    
    
    <category term="树莓派" scheme="https://values.keys.moe/categories/%E6%A0%91%E8%8E%93%E6%B4%BE/"/>
    
    
    <category term="树莓派" scheme="https://values.keys.moe/tags/%E6%A0%91%E8%8E%93%E6%B4%BE/"/>
    
    <category term="Python" scheme="https://values.keys.moe/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>树莓派引脚介绍与GPIO的初步认识与应用</title>
    <link href="https://values.keys.moe/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E5%BC%95%E8%84%9A%E4%BB%8B%E7%BB%8D%E4%B8%8EGPIO%E7%9A%84%E5%88%9D%E6%AD%A5%E8%AE%A4%E8%AF%86%E4%B8%8E%E5%BA%94%E7%94%A8/"/>
    <id>https://values.keys.moe/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E5%BC%95%E8%84%9A%E4%BB%8B%E7%BB%8D%E4%B8%8EGPIO%E7%9A%84%E5%88%9D%E6%AD%A5%E8%AE%A4%E8%AF%86%E4%B8%8E%E5%BA%94%E7%94%A8/</id>
    <published>2019-03-02T01:03:20.000Z</published>
    <updated>2022-09-27T16:58:33.957Z</updated>
    
    <content type="html"><![CDATA[<p>​下图所示为树莓派3b+开发板。上方引脚处左下角引脚为1号引脚。</p><img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E5%BC%95%E8%84%9A%E4%BB%8B%E7%BB%8D%E4%B8%8EGPIO%E7%9A%84%E5%88%9D%E6%AD%A5%E8%AE%A4%E8%AF%86%E4%B8%8E%E5%BA%94%E7%94%A8/树莓派.png" alt style="zoom:10%;"><span id="more"></span><h1><span id="一认识gpio">一．认识GPIO</span></h1><p>所谓GPIO，就是“通用输入&#x2F;输出”接口，树莓派系统中已经编译自带了GPIO的驱动。</p><p><strong>树莓派GPIO的编号方式：</strong></p><img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E5%BC%95%E8%84%9A%E4%BB%8B%E7%BB%8D%E4%B8%8EGPIO%E7%9A%84%E5%88%9D%E6%AD%A5%E8%AE%A4%E8%AF%86%E4%B8%8E%E5%BA%94%E7%94%A8/引脚.png" alt><p><strong>1.功能物理引脚（physical）：</strong><br>从左到右，从上到下。左边为奇数，右边为偶数。共计40个引脚，计数为1-40。<br><strong>2.BCM：</strong><br>编号侧重于CPU寄存器，根据BCM2835的GPIO寄存器编号。具体编号参照上图中BCM一栏。<br><strong>3.wiringPi</strong><br>编号侧重实现逻辑，把GPIO端口从0开始编号，这种编号方便编程，参考上图wPi一栏。</p><p>三种编号的方式均指代的对象相同，只是编码方式不同。</p><p><strong>其中</strong><br>GPIO.**: 通用输入输出接口，GPIO端口，可通过软件分别配置成输入或输出。<br>3.3V&#x2F;5.0V(VCC):提供3.3V&#x2F;5.0V的固定电压<br>0V（GND）：接地<br>SDA<em>：SDA 是I2C 数据传输口。<br>SCL：I2C时钟信号。<br>RXD：接收数据的引脚。<br>TXD：发送数据的引脚。<br>MOSI：为主输出从输入。<br>MISO：为主输入从输出。<br>SCLK: 系统时钟,指晶振频率。<br>CE</em>：片选（芯片有效）-表示低电平有效</p><h1><span id="二python-gpio">二．Python GPIO</span></h1><p>默认的python GPIO均已集成入raspbian系统，不需要另外安装。<br>如果需要安装，请按以下顺序：<br>1、先安装python-dev，输入以下指令。<br>      sudo apt-get install python-dev<br>2、安装RPi.GPIO，依次输入以下指令。<br>1)下载：wget <a href="http://raspberry-gpio-python.googlecode.com/files/RPi.GPIO-0.5.3a.tar.gz">http://raspberry-gpio-python.googlecode.com/files/RPi.GPIO-0.5.3a.tar.gz</a><br>2)解压缩：tar xvzf RPi.GPIO-0.5.3a.tar.gz<br>3)进入解压之后的目录： cd RPi.GPIO-0.5.3a<br>4)启动安装 ： sudo python setup.py install</p><h1><span id="三应用">三．应用</span></h1><p><strong>导入模块</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">import RPi.GPIO as GPIO</span><br></pre></td></tr></table></figure><p><strong>设置引脚引用模式：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">GPIO.setmode(GPIO.BOARD)</span><br><span class="line">#or</span><br><span class="line">GPIO.setmode(GPIO.BCM)</span><br></pre></td></tr></table></figure><p><strong>检测使用的哪种模式可以使用：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mode=GPIO.getmode()</span><br><span class="line">#mode的取值有GPIO.BOARD, GPIO.BCM, None</span><br></pre></td></tr></table></figure><p>以下代码如无特殊说明，均使用GPIO.BOARD引脚映射模式。</p><p><strong>设置引脚方向（输入，输出）:</strong></p><p>如 设置40号引脚为输入方向：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pin = <span class="number">40</span></span><br><span class="line">GPIO.setup(pin,GPIO.IN)</span><br></pre></td></tr></table></figure><p>输出同理：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">GPIO.setup(pin,GPIO.OUT)</span><br><span class="line"><span class="comment">#输出还可以加初始电平：</span></span><br><span class="line">GPIO.setup(pin,GPIO.OUT,initial=GPIO.HIGH)</span><br><span class="line">如果要同时设置多个引脚：</span><br><span class="line"><span class="built_in">list</span>=[<span class="number">11</span>,<span class="number">12</span>]</span><br><span class="line">GPIO.setup(<span class="built_in">list</span>,GPIO.OUT)</span><br></pre></td></tr></table></figure><p>如果要同时设置多个引脚：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">list</span>=[<span class="number">11</span>,<span class="number">12</span>]</span><br><span class="line">GPIO.setup(<span class="built_in">list</span>,GPIO.OUT)</span><br></pre></td></tr></table></figure><p><strong>释放</strong><br>一般来说，程序到达最后都需要释放资源，这个好习惯可以避免偶然损坏树莓派。释放脚本中的使用的引脚：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GPIO.cleanup()</span><br></pre></td></tr></table></figure><p><strong>警告</strong><br>如果RPi.GRIO检测到一个引脚已经被设置成了非默认值，那么你将看到一个警告信息。你可以通过下列代码禁用警告：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GPIO.setwarnings(<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><p>注意，GPIO.cleanup()只会释放掉脚本中使用的GPIO引脚，并会清除设置的引脚编号规则。</p><p><strong>读取</strong><br>我们也常常需要读取引脚的输入状态，获取引脚输入状态如下代码：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">GPIO.input(channel)</span><br><span class="line">#低电平返回0 / GPIO.LOW / False，高电平返回1 / GPIO.HIGH / True。</span><br></pre></td></tr></table></figure><p>如果输入引脚处于悬空状态，引脚的值将是漂动的。<br>换句话说，读取到的值是未知的，因为它并没有被连接到任何的信号上，直到按下一个按钮或开关。<br>由于干扰的影响，输入的值可能会反复的变化。<br>使用如下代码可以解决问题：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">GPIO.setup(channel, GPIO.IN, pull_up_down=GPIO.PUD_UP)  </span><br><span class="line"><span class="comment"># or</span></span><br><span class="line">GPIO.setup(channel, GPIO.IN, pull_up_down=GPIO.PUD_DOWN)</span><br><span class="line"><span class="comment">#需要注意的是，上面的读取代码只是获取当前一瞬间的引脚输入信号。</span></span><br></pre></td></tr></table></figure><p>如果需要实时监控引脚的状态变化，可以有两种办法。<br>最简单原始的方式是每隔一段时间检查输入的信号值，这种方式被称为轮询。<br>如果你的程序读取的时机错误，则很可能会丢失输入信号。<br>轮询是在循环中执行的，这种方式比较占用处理器资源。<br><strong>轮询方式</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">while GPIO.input(channel) == GPIO.LOW:</span><br><span class="line">    time.sleep(0.01)  # wait 10 ms to give CPU chance to do other things</span><br></pre></td></tr></table></figure><p>另一种响应GPIO输入的方式是使用中断（边缘检测），这里的边缘是指信号从高到低的变换（下降沿）或从低到高的变换（上升沿）。<br><strong>边缘检测</strong><br>边缘是指信号状态的改变，从低到高（上升沿）或从高到低（下降沿）。通常情况下，我们更关心于输入状态的该边而不是输入信号的值。这种状态的该边被称为事件。</p><p>wait_for_edge() 函数<br>wait_for_edge()被用于阻止程序的继续执行，直到检测到一个边缘。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">channel = GPIO.wait_for_edge(channel, GPIO_RISING, timeout=<span class="number">5000</span>)</span><br></pre></td></tr></table></figure><p>add_event_detect() 函数<br>该函数对一个引脚进行监听，一旦引脚输入状态发生了改变，调用event_detected()函数会返回true，</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">GPIO.add_event_detect(channel, GPIO.RISING)</span><br><span class="line"><span class="keyword">if</span> GPIO.event_detected(channel):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Button pressed&#x27;</span>)</span><br></pre></td></tr></table></figure><p><strong>RPI.GPIO 模块的脉宽调制（PWM）功能</strong><br>脉宽调制(PWM)是指用微处理器的数字输出来对模拟电路进行控制，是一种对模拟信号电平进行数字编码的方法。在树莓派上，可以通过对GPIO的编程来实现PWM。<br>创建一个 PWM 实例：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">p = GPIO.PWM(channel, frequency)</span><br></pre></td></tr></table></figure><p>启用 PWM：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">p.start(dc)   # dc 代表占空比（范围：0.0 &lt;= dc &lt;= 100.0）</span><br></pre></td></tr></table></figure><p>更改频率：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">p.ChangeFrequency(freq)   <span class="comment"># freq 为设置的新频率，单位为 Hz</span></span><br></pre></td></tr></table></figure><p>更改占空比：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">p.ChangeDutyCycle(dc)  <span class="comment"># 范围：0.0 &lt;= dc &gt;= 100.0</span></span><br></pre></td></tr></table></figure><p>停止 PWM：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">p.stop()</span><br><span class="line"><span class="comment">#注意，如果实例中的变量“p”超出范围，也会导致 PWM 停止。</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">入门介绍</summary>
    
    
    
    <category term="树莓派" scheme="https://values.keys.moe/categories/%E6%A0%91%E8%8E%93%E6%B4%BE/"/>
    
    
    <category term="树莓派" scheme="https://values.keys.moe/tags/%E6%A0%91%E8%8E%93%E6%B4%BE/"/>
    
    <category term="Python" scheme="https://values.keys.moe/tags/Python/"/>
    
  </entry>
  
</feed>
