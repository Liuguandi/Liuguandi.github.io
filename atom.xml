<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Angelo的代码工坊</title>
  
  <subtitle>I shut my eyes in order to see.</subtitle>
  <link href="https://values.keys.moe/atom.xml" rel="self"/>
  
  <link href="https://values.keys.moe/"/>
  <updated>2024-08-24T16:32:58.482Z</updated>
  <id>https://values.keys.moe/</id>
  
  <author>
    <name>Angelo</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>阻止一个一心求死的人自杀 是不是一种伪善？</title>
    <link href="https://values.keys.moe/2024/08/24/%E9%98%BB%E6%AD%A2%E4%B8%80%E4%B8%AA%E4%B8%80%E5%BF%83%E6%B1%82%E6%AD%BB%E7%9A%84%E4%BA%BA%E8%87%AA%E6%9D%80%20%E6%98%AF%E4%B8%8D%E6%98%AF%E4%B8%80%E7%A7%8D%E4%BC%AA%E5%96%84%EF%BC%9F/"/>
    <id>https://values.keys.moe/2024/08/24/%E9%98%BB%E6%AD%A2%E4%B8%80%E4%B8%AA%E4%B8%80%E5%BF%83%E6%B1%82%E6%AD%BB%E7%9A%84%E4%BA%BA%E8%87%AA%E6%9D%80%20%E6%98%AF%E4%B8%8D%E6%98%AF%E4%B8%80%E7%A7%8D%E4%BC%AA%E5%96%84%EF%BC%9F/</id>
    <published>2024-08-24T09:31:56.000Z</published>
    <updated>2024-08-24T16:32:58.482Z</updated>
    
    <content type="html"><![CDATA[<img src="/2024/08/24/%E9%98%BB%E6%AD%A2%E4%B8%80%E4%B8%AA%E4%B8%80%E5%BF%83%E6%B1%82%E6%AD%BB%E7%9A%84%E4%BA%BA%E8%87%AA%E6%9D%80%20%E6%98%AF%E4%B8%8D%E6%98%AF%E4%B8%80%E7%A7%8D%E4%BC%AA%E5%96%84%EF%BC%9F/title.jpg" alt><p>”阻止一个一心求死的人自杀，是不是一种伪善？“</p><p>此类涉及伦理、哲学的话题在网上总有持续的热度。之前不久，我也在公司内网内看到过与之类似的帖子。帖子内容大致是，公司与警方通力合作，找到并挽救了在平台上发送寻死信息的青年。而帖子评论下，部分同事持续地在争论，到底应不应该拯救寻死者。那时我的答案很明确，”当然，而且必须救“。那时我想，如果对他人的生死都熟视无睹，未免有点过于冷血了。</p><p>而当我真正亲历这样的事后，却反而看不透这个问题的“标准答案”了。</p><p>让我说说我遇到的故事吧。</p><span id="more"></span><h1><span id="一条朋友圈">一条朋友圈</span></h1><p>某日下午，Coding到有点疲惫的我打开手机微信朋友圈，在搬砖之余看看大家令人羡慕的生活。手指按住钢化膜往下滑的一瞬，却看到了这样的一条朋友圈：</p><p><strong>“北京、河北、安徽月，全套XX，包夜XX”。</strong></p><p>“我的朋友圈怎么会有这种人？”，我的第一反应如上。卖淫嫖娼行为是我一贯坚决抵制的，我认为卖淫本质上就是性层面上的剥削。因此，我自认为根本不应该会有一个“小姐”好友。</p><p>这位是被盗号了？还是有什么难言之隐？我定睛看了下发文者的头像和昵称备注。很奇怪，从头像上看，她是个女生，但我并没有给她备注。看聊天历史，也和她也完全没有消息往来。脑子里的记忆碎片翻来覆去，终于想起，她是我一位朋友的朋友，在多年前打游戏时一起联机，为后续还方便开黑，便加了好友，但后来似乎也没再怎么开黑过。印象中，她是一个很乐观开朗的女孩子，打游戏开麦交流的时候，她总是能鼓励大家，重振感觉已经白给了的士气。</p><p>我不太相信这样的女孩子会做出这种事。于是私聊问了问，“你这朋友圈发了个啥？”。没隔几秒。她很快就回复了我，但是回复我的内容是：</p><p><strong>一张黄图。</strong></p><h1><span id="困惑与劝阻">困惑与劝阻</span></h1><p>她是怎么了？</p><p>我很担心的问她，是不是家里出什么事了，是不是生病需要钱治，是不是有什么很急用钱的地方。</p><p>她说，“没事呀”，还回了个可爱的表情包。</p><p>她的言语间透露着一种表面的“无所谓”，这让我更困惑了。到底发生了什么，让她能想得出要做这种傻事？</p><p>她还是回复，“没什么，就是要卖，一次XX，包夜XX”。之后，又给我发了一张黄图。</p><p>在工位的我，看到这些消息，赶紧把屏幕亮度调到最低，担心被同事误会。</p><p>“到底怎么了？”，我又发了条消息问她。</p><p>“就是缺钱呀。”</p><p>因为缺钱就选择卖身？我实在有点难以理解，更没法把这样的人和之前的她联系起来。我劝她千万别干这种傻事，能挣钱的正道有很多，卖身赚钱是最最愚蠢的行为。</p><p>“没事的，我已经决定了。”</p><p>我仅存的良知告诉我，绝对不能劝到这就结束了，出卖自己身体，踏出一步就是深渊。</p><p>“你能接电话吗？我电话和你聊，你打字就行，不用和我说话”。我拿上耳机，以上厕所之名，躲到公司里的电话间，拨通了她的微信电话。</p><p>我在电话间呆了两个多小时，劝导了她两个多小时。劝导她嫖客没有好人，不会把你真的当人看；卖淫嫖娼的疾病传播非常广、社会影响非常差；缺钱我们可以一起想办法，我也可以帮助她；出卖身体是一条不归路，躺下了就永远站不起来了；生活只要靠自己的双手，总是能一点点慢慢变好的…如此这般。</p><p>她也终算是逐步打开了话匣子。</p><p>她和我说，我是唯一一个劝她的人。</p><p>她说她的原生家庭很破碎，父母很早就离婚了。她跟随了她的父亲，她的姐姐们跟随了她的母亲。原以为一个人跟随父亲，能享受父亲全部的父爱，没曾想父亲根本不爱他，只爱母亲那边的姐姐们。既然没人爱她，为什么要生下她？</p><p>她说她被爱人背叛了、被朋友背叛了。异地的男友突然不回她的消息一个多月，后来才知道，男友和她的闺蜜出轨了，他们都没告诉她这件事。</p><p>她说她的工作不是人干的。她不会读书，只有初中毕业。最后一次从事的是医美行业，在北京，每天早上八点干到晚上十一点，从来没好好吃过一顿晚饭，工作强度难以承受。顾客都是手脚不干净的男色狼，喜欢对她的裙子做不干不净之事，领导和同事只让她忍耐。</p><p>她说她很穷。最穷的时候，一个馒头都买不起，饿的只能喝冷水。</p><p>亲情、友情、爱情，她什么都没有。她觉得只有钱拿到手才是最好的了。</p><p>她说，她生活太苦了，赚钱太难了，她这周受不了，辞职之后，就选择卖身赚钱，她卖一次，能拿到她努力工作好几天都挣不来的钱。</p><p>我告诉她一定要爱惜自己的身体，千万不要做这种傻事。生活是能靠自己的努力变好的。</p><p>她和我说，她看不到任何希望了。</p><p>她说，她有自己的计划，就是卖几年，赚几年钱，这几年吃够自己喜欢吃的，穿够自己喜欢穿的小裙子，就可以心满意足的去死了。</p><p>我一时语塞，不知道该说什么。</p><p>但我还是努力劝她，自己的身体是绝对的底线，工作可以慢慢找，可以找轻松点的。千万别做卖身这种傻事。哪怕没钱，我都可以给她，都不需要她还，吃住能花多少钱，慢慢找工作稳定下来就好。</p><p>她和我说，她唯独不想要我的钱。</p><p>“那就当是我花钱买你，只是买的次数先攒着，好吗？”我以近乎恳求的哭腔，希望她能转变想法，走回正轨。</p><p>她和我说，如果我想和她做，她不收我的钱，路费都不要我掏。她预计九月半来，如果我着急她可以调整时间。</p><p>我再次语塞。</p><p>那种无奈、难受和愤懑感，已经挤满了我的脑袋。</p><p>感觉像是，用尽全力想把一个跳崖的人拉上岸，但她已经彻底疯狂，自我毁灭的挣脱了。</p><p>但是我还不想放弃。只要电话没挂，我就还有阻止她自毁的机会。</p><p>我和她聊了很多很多，讲到最后，可能我自己都不知道自己在说什么胡话了。</p><p>奇迹似乎发生了。微信屏幕上跳出了一行字：</p><p><strong>“好，我答应你。”</strong></p><p>她告诉我，她不会再卖了。她已经把那群嫖客的联系方式都删光了，已经付款了的嫖客的钱，也全部退还了。</p><p>我紧绷的神经终于感受到了一丝放松。太好了，把一个快走入歧途的人拉了回来。</p><p>我挂了电话，把耳机塞回充电仓，感觉自己解脱了。打开没有空调的电话间的门，外面空调的冷风直直地灌入，吹得我神清气爽。只是回到工位的时候，同事调侃我怎么上了两个小时厕所，我也笑笑糊弄了过去。</p><p>解锁工位上的电脑，准备继续干活。但这之前，我又习惯性的解锁手机，看看朋友圈。</p><p><strong>我又看到了她，发了一条新的卖身朋友圈。</strong></p><p>她忘记屏蔽了我。</p><p>我赶紧微信再给她发消息，她和我说，她只是在搞抽象。</p><p>怎么可能，我又不是傻子。</p><p>我又给她打电话。</p><p>这次她没接。</p><p>她和我发消息，说她活着感觉真没有意思。只有初中学历的她，什么岗位都干过了，从小苦到大，实在受不了了，十九岁的她就想尝点甜的。她的计划已经定了，是不可能再改变了。</p><p>又一次，我不知道该说什么，感觉自己已经把能说的都说了，还是没劝动她。</p><p>她和我说，她马上要出去一下，要再找她聊，得晚点了。</p><p>我问她是不是要出去卖，跟她说千万别去，我出更多的钱都行，求她了。</p><p><strong>“对不起，我要去。</strong>”</p><h1><span id="寻死与报警">寻死与报警</span></h1><p>懊悔、无奈。</p><p>一个下午，我也没劝住她去卖身。无论再怎么给她发消息劝她别去，也收不到任何回复了。</p><p>烦闷地吃完晚饭，同事都问我是不是玉玉了，吃饭都不说话了。</p><p>饭后我难得的跑回自己的小出租屋，想摆烂一会儿再回工位上班。</p><p>点开微信，朋友圈又亮起了她更新了的头像提示。</p><p>点开朋友圈，她只发了几个字：</p><p><strong>“再见了，世界。”</strong></p><p>我顿感不妙。</p><p>我赶紧给她再打电话，不接。</p><p>发微信，不回。</p><p>过了几分钟，我看到这条朋友圈，她自己又回复了一条评论：</p><p><strong>“我先走一步。”</strong></p><p>我坐不住了。我太担心出事了。如果她真的自杀，而我没有阻止的话，我绝对会后悔一辈子。</p><p>我拨通了110。这是我这辈子第一次报警。</p><img src="/2024/08/24/%E9%98%BB%E6%AD%A2%E4%B8%80%E4%B8%AA%E4%B8%80%E5%BF%83%E6%B1%82%E6%AD%BB%E7%9A%84%E4%BA%BA%E8%87%AA%E6%9D%80%20%E6%98%AF%E4%B8%8D%E6%98%AF%E4%B8%80%E7%A7%8D%E4%BC%AA%E5%96%84%EF%BC%9F/110.jpg" alt><p>第一次接警的是杭州警方。警察同志问我自杀者在哪，我告诉他在北京。他和我说，这种情况下得报北京110处理。让我加区号报警。</p><p>赶紧百度搜了下，北京区号是010。着急到手抖的我拨打了01011，最后一个0还没按出来就拨号了。赶紧挂机重新拨打。</p><p>北京警方接警后，问我和她是什么关系，有没有她的身份信息。我说只是网友，根据她的微信号和朋友圈，能推断出她的手机号和姓名。</p><p>而后接警警员进一步进线联系我，让我提供更详细的信息。可我实际上都不认识这个女生，未曾蒙面，今天以前甚至都没有什么密切的交流。她和我说，这个电话没有实名，找人比较困难。但是他们会尽力。</p><p>在这之后，北京地方派出所的警察同志通过个人电话打给了我，和我说他们正在通过电话基站定位人，但是目前看定位的坐标不是很准，让我尽可能再尝试和她取得联系。他们也在持续的拨打电话，希望她能接听。</p><p>我持续地给她发微信消息，劝她冷静。一刻不停地给她打电话。</p><p><strong>她终于回复了。</strong></p><p>她和我说，警察已经联系她了，她现在已经不想死了。</p><p>她请求我别给她再添麻烦了，如果卖身的事情再曝光，她还得被拘留。</p><p>只能先答应她，会和警方说你已经不打算自杀了。</p><p>我联系了刚刚给我打电话的警察同志，和他说人已经回复了，不想死了。</p><p>北京的警察同志们很负责，还进一步帮我寻找联系了户籍所在的省市的警察，进一步协作排查下这个女生的信息。只是联系的省市似乎不对，没有搜到具体的信息。十分感谢他们。</p><p>在我告知已经和警方说清楚了之后，她和我说。</p><p><strong>活着还是死了，对她来说，已经无所谓了。</strong></p><p><strong>即使活着，她也只会继续卖，她的计划是不会变的。</strong></p><p><strong>她看不到任何希望。</strong></p><p>再一次的再一次，我不知道该说什么了。</p><h1><span id="那之后">那之后</span></h1><p>她删除了她想要自杀的朋友圈。</p><p>她和我说，她感觉很痛。</p><p>她说，她已经发疯了。</p><p>她给我发了张截图，她把微信里的所有好友都删了，只留了她的小号，和我。</p><p>她问我有没有空下班后和她说说话，我答应了她。</p><p>下班后，我再找她，发出的消息已经提示了红点。</p><p>思虑再三，我还是担心出事，又申请了她的好友，想问问她最后删之前，是想和我说什么话。</p><p>她和我说她特别后悔把我删了，她不是故意的，和我道歉，说再也不会删了我。</p><p>我问她，是想和我聊什么。她说他已经想不起来了。</p><p>我看她已经清空了她的朋友圈。</p><p>我和她说，就当之前所有的事情都没发生过，删除了所有的嫖客，所有的坏人，务必让生活重新回归正轨，重新上路，好好生活。</p><p>今天对我贸然的报警十分抱歉，给你添麻烦了。</p><p>之后，我把她删除了。</p><p>次日上午，我又看到了她的好友申请，让我不要删掉她。</p><p>点进她的个人资料，她的个性签名从原来的正常签名，变成了：</p><p><strong>一次XX，包夜XX。</strong></p><p>她清空的朋友圈，又填满了各种擦边照片，以及各种挑逗式的文案。</p><p>她仍然在自毁。</p><p>我，没有再同意她的好友请求。</p><h1><span id="阻止一个一心求死的人自杀是不是一种伪善">阻止一个一心求死的人自杀，是不是一种伪善？</span></h1><p>此事之后，我感觉到了一种无比特殊的自责与愧怍感。</p><p>和朋友们谈及此事，朋友们都说我“过于亚萨西了”，“亚萨西到有点傻傻的”。</p><p>我所做的一切，最终还是什么都没有改变，她仍然还在践行着自己的“计划”。</p><p>我所做的一切，似乎都只是在满足自己的伪善。因为我见不得这种情况，所以我得阻止它的发生。</p><p>到底，活着是为了什么呢？</p><p>阻止一心求死的人自杀，真的应该吗？</p><p>我已经想不明白了。</p><p>当我和她电话过程中，她反问我，现在她这样活着，到底有什么意思的时候，我不知道该怎么回答她。</p><p>我只是想起，在许多年前的一个下午，和朋友们一起打游戏的时候，这个女生明明很乐观、很开朗、很会鼓励队友。</p><p>为什么她现在变成了一个要抑郁自杀，对混蛋的嫖客们摇尾乞怜的人？</p><p>我越想越难受与压抑。</p><p>但是，如果我看到那条自杀朋友圈，没有报警的话。</p><p>如果真的出事了，我一定会后悔一辈子，就像是我亲自下手的一样。</p><p>但是，即使救了下来。</p><p>她也依然在痛苦的生活，依然在自愿地成为那群畜生嫖客的玩物。</p><p>这真的对吗？</p><p>我已经想不明白了。</p><p>在我报警的时候，我在想，</p><p>哪怕，有一丝一毫的，想活下去、想变好的可能性，我都要尽可能的挽救。哪怕我帮她出钱，也要尽可能改变现在这种自毁般的生活。</p><p>但现在回头看，</p><p>这是不是我的一种傲慢？</p><p>我只是个普通人，怎么可能背负其他人的人生？</p><p>我和她说，让她好好生活，生活一定会慢慢变好。</p><p>但我自己都觉得，这种话是不是太苍白无力了？</p><p>她说她想自杀是因为她卖完，觉得好痛，我的报警只是在给她徒增麻烦。</p><p>这是不是我的傲慢与伪善？只想做出不让自己后悔的选择，却没有考虑当事人的想法？</p><p>我已经想不明白了。</p><p>最后我把她删掉，也没有再同意她的好友请求。她求我别删她，我也没有同意。</p><p>是不是证明我还是担心摊上更麻烦的事，彻底坐实了我的伪善？</p><p>阻止一个一心求死的人自杀，</p><p>是不是一种伪善？</p><p>是不是一种傲慢？</p><p>是不是一种自我满足？</p><p>我真的救了她吗？</p><p>我仍没能改变她的想法，她的计划，她仍然坚持着卖身几年，然后自杀。</p><p>直到最后的最后，我把脑子里能想到的词都言尽了，但她仍然是固执地坚持她的计划。</p><p>我是不是只是延长了她的痛苦？</p><p>这是不是就是我的傲慢？觉得我能拯救她，我能改变她？</p><p>“靠着卖淫挣来的钱，花的肯定也不会开心”，我的这种观点强加于她人之上，是不是错误的？</p><p>我想不明白了。</p><p>只是，</p><p>我看到她朋友圈里的拉嫖文案，</p><p>看的无比绝望。</p><p>只是，</p><p>我和她说我可以帮他，让她找一个正常工作，在这之前开销我都可以承担。</p><p>她不接受，说她太累了，不想这样下去了。</p><p>她本能过上正常人的人生，只是我没能拉住她。</p><p>只是，</p><p>我没有救下一条人命，</p><p>她还是想死。</p><hr><img src="/2024/08/24/%E9%98%BB%E6%AD%A2%E4%B8%80%E4%B8%AA%E4%B8%80%E5%BF%83%E6%B1%82%E6%AD%BB%E7%9A%84%E4%BA%BA%E8%87%AA%E6%9D%80%20%E6%98%AF%E4%B8%8D%E6%98%AF%E4%B8%80%E7%A7%8D%E4%BC%AA%E5%96%84%EF%BC%9F/mygo.jpg" alt><blockquote><p>你是抱着多大的觉悟说出这种话的？</p><p>区区一介学生，有办法背负他人的人生吗？</p><p>“什么都愿意做”就是这么沉重的话，</p><p>做不到的事情就不要说出口。</p><p>你这个人，满脑子都只想着自己呢。</p><p>还真是高高在上啊。</p><p>——《BanG Dream! It’s MyGO!!!!!》</p></blockquote>]]></content>
    
    
    <summary type="html">&lt;img src=&quot;/2024/08/24/%E9%98%BB%E6%AD%A2%E4%B8%80%E4%B8%AA%E4%B8%80%E5%BF%83%E6%B1%82%E6%AD%BB%E7%9A%84%E4%BA%BA%E8%87%AA%E6%9D%80%20%E6%98%AF%E4%B8%8D%E6%98%AF%E4%B8%80%E7%A7%8D%E4%BC%AA%E5%96%84%EF%BC%9F/title.jpg&quot; alt&gt;



&lt;p&gt;”阻止一个一心求死的人自杀，是不是一种伪善？“&lt;/p&gt;
&lt;p&gt;此类涉及伦理、哲学的话题在网上总有持续的热度。之前不久，我也在公司内网内看到过与之类似的帖子。帖子内容大致是，公司与警方通力合作，找到并挽救了在平台上发送寻死信息的青年。而帖子评论下，部分同事持续地在争论，到底应不应该拯救寻死者。那时我的答案很明确，”当然，而且必须救“。那时我想，如果对他人的生死都熟视无睹，未免有点过于冷血了。&lt;/p&gt;
&lt;p&gt;而当我真正亲历这样的事后，却反而看不透这个问题的“标准答案”了。&lt;/p&gt;
&lt;p&gt;让我说说我遇到的故事吧。&lt;/p&gt;</summary>
    
    
    
    <category term="杂谈" scheme="https://values.keys.moe/categories/%E6%9D%82%E8%B0%88/"/>
    
    
    <category term="杂谈" scheme="https://values.keys.moe/tags/%E6%9D%82%E8%B0%88/"/>
    
    <category term="人间观察" scheme="https://values.keys.moe/tags/%E4%BA%BA%E9%97%B4%E8%A7%82%E5%AF%9F/"/>
    
  </entry>
  
  <entry>
    <title>Practical Accuracy Estimation for Efficient Deep Neural</title>
    <link href="https://values.keys.moe/2021/12/26/Practical%20Accuracy%20Estimation%20for%20Efficient%20Deep%20Neural/"/>
    <id>https://values.keys.moe/2021/12/26/Practical%20Accuracy%20Estimation%20for%20Efficient%20Deep%20Neural/</id>
    <published>2021-12-25T23:04:01.000Z</published>
    <updated>2023-10-25T14:52:55.951Z</updated>
    
    <content type="html"><![CDATA[<img src="/2021/12/26/Practical%20Accuracy%20Estimation%20for%20Efficient%20Deep%20Neural/pic1.jpg" alt><h1><span id="高效深度神经网络测试的实用准确度估计">高效深度神经网络测试的实用准确度估计</span></h1><h2><span id="摘要">摘要</span></h2><p>​深度神经网络 (Deep neural network, DNN) 愈发流行，DNN的测试对DNN的正确性（即DNN在指定工作中的准确度）至关重要。然而，DNN 测试存在严重的效率问题，即，标注每个测试输入以了解DNN在测试集上的准确度的成本很高，因为需要大量的人（甚至他们还需要特定领域知识）来对测试数据进行手工标注，而测试集本身又是大规模的。为缓解此问题，本文提出了一种叫PACE新颖手法（Practical ACcuracy Estimation的缩写），其选择一小部分测试输入，可以精确地估计整个测试集的准确性。这样一来，只需对这小部分选定样本进行标注即可，这很大程度上降低了标注成本。除了实现精确的准确性估计外，为使PACE更加实用，其还被要求应该是可解释的、确定的，并尽可能地高效的。因此，PACE首先结合聚类，将具有不同测试能力（即测试DNN模型不同功能）的测试输入划分为不同的组。然后，PACE利用MMD-critic算法（一种最先进的基于样例的解释算法，根据组的大小，从每组中选择原型，即选择最具代表性的测试输入）来可以减少聚类带来的噪音影响。同时，PACE还使用了自适应随机测试的思想，从少数空间（即没有聚成任何一组的测试输入）中选择测试输入，以保证在所需的测试输入数量下实现极大的多样性。两个平行的选择过程（即同时从组和少数空间中进行选择）组建了最终的一小部分被选中的测试输入集合。结果表明，PACE能够精确估计整个测试集的准确性，平均偏差只有1.181%∼2.302%，大大超过了最先进的方法。</p><span id="more"></span><h2><span id="简介">简介</span></h2><p>​本文共做出以下主要贡献：</p><p>​<strong>方法</strong>。我们提出了第一个相对可解释、确定和有效的叫做PACE的方法，通过选择DNN测试输入的子集来估计整个测试集的准确性。</p><p>​<strong>实现</strong>。我们基于最先进的框架和库实现了所提出的方法，包括Keras 2.2.4和TensorFlow 1.14.0，以及hdbscan 0.8.22、sklearn与MMD-critic算法的作者分别提供的HDBSCAN、FastICA和MMD-critic算法。</p><p>​<strong>研究</strong>。我们基于测试和测试集下的24对DNN模型进行了广泛的研究，这些测试考虑了模型和测试输入的多样性，证明了PACE的有效性。</p><p>​<strong>制品</strong>。我们已经发布了一个广泛的数据集，供未来使用和研究，其中包括了我们的实现以及实验数据。</p><h2><span id="方法">方法</span></h2><p>​为了提高 DNN 测试的效率，我们的目标是选择测试输入的子集来代表整个测试集，即我们希望通过只标记一小部分选定的测试输入来精确估计整个集合的准确性。这可以大大降低标注成本。然而，如何有效地选择这样一小组测试输入是具有挑战性的。此外，选择的方法要尽可能具有可解释性、高效性和确定性，以使其适用于实践。这也是解决这个问题的关键挑战。</p><p>​在本文中，我们提出了一种新颖实用的方法，称为 PACE，用于从测试输入集中选择更小的子集。一般来说，对于整个测试集，一些测试输入具有相似的测试能力（即测试 DNN 的相似功能），而一些测试输入具有不同的测试能力。直观地说，所选的一小部分测试输入应该涵盖各种测试能力，并保证它们对应的测试能力的原始分布，以使得这些样本能尽可能地代表整个集合，并具有高可解释性。为了实现这个目标，PACE根据识别的特征将所有的测试输入聚类成组，从中反映它们的测试能力，其中不同的组更有可能具有不同的测试能力。由于组的大小在一定程度上反映了不同测试能力的分布，PACE根据不同组大小的比例选择所需的测试输入数量，以保持测试能力的相似分布。</p><p>​一个自然的后续问题是应该从每组中选择哪些测试输入。由于很难通过聚类来完美区分具有不同测试能力的测试输入，我们应该选择最能反映该组测试能力的测试输入，来绕过每组中噪音的影响。为了进一步增强所选测试输入的可解释性，PACE利用最先进的基于样本的解释算法MMD-critic算法来从每组中选择原型（即最具代表性的测试输入）。此外，也可能有一些测试输入不属于任何一组。也就是说，它们中的每一个都可能有独特的测试能力。我们称这些测试输入的空间为少数空间。从少数空间中选择测试输入是合理的，因为它们能更进一步接近整个测试集。为了使用所需数量的测试输入更充分地探索少数空间，PACE 借用了自适应随机测试的思想来选择测试输入。</p><p>​图1展示了我们的方法PACE的概况。</p><img src="/2021/12/26/Practical%20Accuracy%20Estimation%20for%20Efficient%20Deep%20Neural/pic2.png" alt><p>图1 PACE概况</p><h2><span id="特征研究">特征研究</span></h2><p>​我们首先确定测试输入的特征，来区分它们的测试能力。根据传统软件测试，有两类特征可以在一定程度上反映测试能力，即覆盖率特征和输入特征。对于DNN测试，前者指的是在执行测试输入时，哪些DNN元素被覆盖，而后者不依赖于覆盖信息，而是依赖于测试输入信息。本工作中，我们使用输入特征来帮助区分测试能力。主要原因有以下三个：（1）现有的工作已经证明，对于一个给定的DNN来说，许多测试输入可能具有非常相似的神经元覆盖率，因此覆盖率特征不能很好地判别其测试能力。（2）在选择测试输入的问题上，已有工作对覆盖率特征的有效性进行了评估，结果表明其有效性比输入特征的有效性要差。（3）收集覆盖率特征往往会产生额外的代价。</p><p>​DNN逐渐从输入中学习特征来预测标签。DNN的不同层代表不同类型的输入特征。越接近输入层的层代表更多的基本特征，而更接近输出层的层则代表更多的高阶特征。也就是说，测试输入信息包含基本信息（即测试输入本身和从测试输入提取的基本特征）以及高级信息（从测试输入中提取的高阶特征）。更多的高阶特征可以更精确地捕捉到输入和标签之间的关系，但它们更特定于给定的DNN模型，且由于他们需要运行更多层的DNN模型，收集它们的成本更高。相比之下，更基本的特征更通用，其也可更有效地收集，但它们不能反映更复杂的输入特征模式来预测标签。我们的方法并不特定于指定类型的输入特征，本工作中也研究了不同类型的输入特征。本文主要研究以下四种不同类型的输入特征（包括基本特征和高阶特征）为代表：</p><p>​原始特征（ORI）：指的是一个DNN的输入向量，其是最基本的特征，直接代表一个输入本身。</p><p>​第一层特征（FL）：指的是DNN中第一层的输出，最接近DNN的输入向量。</p><p>​最后隐藏层特征（LHL）：指的是DNN中最后一个隐藏层的输出，是可以直接推断输入对应的预测结果的高阶特征。</p><p>​置信特征（CON）：指的是DNN（分类器）的输出，其表示预测结果的置信度。我们还将其归入输入特征中，因为其不依赖覆盖率信息，而是依赖通过DNN模型从测试输入中得到的输出。</p><img src="/2021/12/26/Practical%20Accuracy%20Estimation%20for%20Efficient%20Deep%20Neural/pic3.png" alt><p>图2 特征提取可视化</p><p>​图2使用一个输入样例（CIFAR-10中的汽车图像）说明了DNN中每种类型的特征来源。请注意，DNN的输入和DNN中某些层的输出可能是多维矩阵，因此我们将矩阵降维成向量作为特征向量（如图2中的ORI和FL）。特别的，ORI特征是静态特征，它们不需要运行DNN，而FL、LHL和CON特征都是动态特征。</p><h2><span id="基于聚类的测试能力鉴别">基于聚类的测试能力鉴别</span></h2><p>​基于识别出的测试输入特征（如ORI或FL），每个测试数据皆以一个特征向量来表示。PACE将测试输入聚类到不同组中，以根据特征向量集区分它们的测试能力。聚类前，我们首先对特征进行归一化，以将不同尺度上测量的值调整至同一尺度上。由于特征都是数字类型，PACE采用min-max归一化方法，将这些特征的每个值调整到区间[0,1]中。假设整个测试集表示为T，其大小为st，T中的测试输入集表示为T&#x3D;{t1,t2,…,tst}，ti的特征向量表示为Fi&#x3D;{fi1,fi2,…,fir}，我们用变量xij表示归一化前ti的第j个特征值，用变量x*ij表示归一化后ti的第j个特征值(1≤i≤st且1≤j≤r)。下式展示了归一化的计算过程：</p><img src="/2021/12/26/Practical%20Accuracy%20Estimation%20for%20Efficient%20Deep%20Neural/formula1.png" alt><p>​PACE之后采用HDBSCAN（基于分层密度的噪声应用空间聚类）算法来对测试输入进行聚类，原因如下。(1)几乎不可能事先知道测试能力类型的数量，因此需预先定义集群数量的聚类算法无法使用，如广泛使用的K-means算法。HDBSCAN不需要预先定义聚类的数量，而是基于密度进行聚类。(2)最广泛应用的基于密度的聚类算法之一是DBSCAN，而HDBSCAN是其升级版。特别的，HDBCSAN可以有不同密度的簇，而DBSCAN必须预先定义簇的密度。(3)HDBSCAN已被证明是十分高效的，使我们的方法PACE更加实用。(4)HDBSCAN需要设置的参数很少，并且对参数选择具有鲁棒性。这使得它在实践中更容易使用。</p><img src="/2021/12/26/Practical%20Accuracy%20Estimation%20for%20Efficient%20Deep%20Neural/pic4.png" alt><p>图3 HDBSCAN聚类主要阶段的可视化</p><p>​HDBSCAN是一种基于密度的聚类算法，它将紧密堆积在一起的数据点（即有许多附近邻居的数据点）分组。为了更清楚地说明PACE在HDBSCAN的过程，我们将DNN模型ResNet-20的测试集CIFAR-10中抽取100个测试输入，对HDBSCAN聚类的主要阶段进行可视化（如图3所示）。更具体地说，其首先构建一个加权图，其中数据点是顶点，任意两点间都存在一条边，其权重等于两点之间的相互可达距离。下式显示了基于K近邻的点a与点b之间的相互可达距离（MRD）的计算：</p><img src="/2021/12/26/Practical%20Accuracy%20Estimation%20for%20Efficient%20Deep%20Neural/formula2.png" alt><p>​其中Corek(a)和Corek(b)分别代表a&#x2F;b与其第k个最近邻之间的距离；Dist(a,b)是a和b之间的距离，用广泛使用的欧几里得距离测量。在MRD下，密集点之间的距离保持不变，但稀疏点被移动至与其他点的距离至少为他们的core距离。然后，HDBSCAN通过Prim算法构建加权图的最小生成树，如图3(a)所示。在最小生成树的基础上，通过对树边按距离升序排列，并通过为每条边创建一个新的合并簇进行迭代，将一个完全联通的图转化为一个连通分量的层次结构，如图3(b)所示。接下来，其根据最小集群大小（HDBCSAN的一个参数）将大集群层次结构压缩为较小的树，如图3(c)所示。最后，它通过计算每个聚类的稳定性分数，从树中提取稳定的簇。基于HDBCSAN，PACE将测试输入分为不同的组，不同的组更可能具有不同的测试能力。此外，一些输入没有被聚类到任何组中，构成了少数空间。</p><p>​请注意，当特征维度增加时，HDBCSAN的性能可能会大大降低。因此，PACE在聚类前结合了对高维特征进行降维的过程。更具体地说，PACE使用FastICA算法来进行降维。FastICA旨在通过最大化公式(3)中所定义的负熵来找到独立成分，并有助于找到潜在因素。在这个公式中，YGauss是一个方差与随机变量Y相同的高斯随机变量，E[.]是计算均值，而g(.)是一个非线性函数，用以近似微分熵：</p><img src="/2021/12/26/Practical%20Accuracy%20Estimation%20for%20Efficient%20Deep%20Neural/formula3.png" alt><h2><span id="基于mmd-critic的原型选择">基于MMD-critic的原型选择</span></h2><p>​在将具有不同测试能力的测试输入分为不同组后，PACE再从每个组中选择测试输入，构成测试输入的小集合。由于很难通过聚类来完美区分不同的测试能力，每组都可能存在噪声，即不应将这个测试输入划分到该组的情况。为了绕过每组噪声的影响，可选择最能代表该组测试能力的测试输入，它们也被称为原型。为增强每组所选测试输入的可解释性，PACE利用MMD-critic算法，一种最先进的基于样本的解释算法（用于机器学习模型的可解释性，包括分类和聚类），来从每组中选择原型。</p><p>​更具体地说，它通过计算原型分布（表示为P）和组分布（表示为G）之间的差异来选择原型。所选的原型应具有最小差异。MMD是P和G之间差异的度量，由两个分布期望间差异的函数空间F上的上限值给出，如下式所示。此外，PACE还基于MMD-critic算法选择criticism（criticism指不具代表性的样本），即与该组原型差异最大的测试输入。criticism与原型能够帮助开发人员建立更好的模型来理解组，从而可以提高选择的可解释性：</p><img src="/2021/12/26/Practical%20Accuracy%20Estimation%20for%20Efficient%20Deep%20Neural/formula4.png" alt><p>​通过这种方式，PACE从每组中选择所需数量的原型来构成小集合，同时也选择相应的criticism来帮助理解簇中复杂的测试输入空间。</p><h2><span id="少数空间的自适应随机探索">少数空间的自适应随机探索</span></h2><p>​少数空间的测试输入也是整个测试集的组成部分，因此，所选择的小集合也应该包含这些测试输入的一部分，以更好地代表整个测试集。而选择这些测试输入的一部分来代表整个少数空间是具有挑战性的，因为这些测试输入可能彼此具有不同的测试能力。为使用所需数量的测试输入来尽可能充分探索少数空间，PACE借用了自适应随机测试的思想。这样，PACE就可以在所需的测试输入数量下实现极大的测试能力多样性，从而使整个少数空间尽可能由所需的测试输入数量来表示。更具体地说，它计算未选择的测试输入与每个已选择的测试输入之间的距离，并使用最小距离作为未选择的测试输入与已选择测试输入的距离。然后，它选择与已选择的测试输入具有最大距离的测试输入作为下一个样本。在现有研究中，这种自适应随机策略以及被证明比其他自适应随机策略更有效。根据聚类中使用的距离，PACE还使用欧几里得距离来计算测试输入之间的距离。下式给出了每次选择时的计算结果，其中U和S分别代表未选择的测试输入集和已选择的测试输入集，EucDist(,)旨在计算两个测试输入间的欧几里得距离。</p><img src="/2021/12/26/Practical%20Accuracy%20Estimation%20for%20Efficient%20Deep%20Neural/formula5.png" alt><p>​传统的自适应随机测试随机选择第一个样本，但PACE确定性地选择第一个测试输入，使其更具实用性和可解释性。更具体地说，PACE选择具有最独特测试能力的测试输入作为第一个样本，其与所有组的距离最大。特别的，该测试输入是通过基于HDBSCAN聚类算法提供的outlier_scores（测量与所有组的距离）值来对少数空间中的测试输入进行排名后，处于Top-1位置所对应的输入。</p><h2><span id="pace的使用">PACE的使用</span></h2><p>​本小节中，我们介绍PACE用法，算法1是PACE的高级伪代码。PACE的输入包括一个表示为D的被测试的DNN，一个表示为T的完整测试集，其大小表示为st，以及表示所选测试输入小集合大小，表示为n。为了构建可以精确估计整个测试输入的准确度的小集合，PACE首先通过提取特征（如ORI或FL）将每个测试输入转换为特征向量（如算法1的2-4行所示）。然后，PACE对向量集进行包括归一化或降维的预处理，并通过聚类将其分为m组和少数空间（算法1中的第5行和第6行）。再次，我们表示第i个组的大小为si，少数空间的大小为sm，其中：</p><p><span><img class="small-img-inline" src="/2021/12/26/Practical%20Accuracy%20Estimation%20for%20Efficient%20Deep%20Neural/formula5.1.png" alt></span></p><p>​选定的小集合测试输入应包含来自每个组的测试输入和来自少数空间的测试输入。在这里，我们定义了一个阈值α来确定他们的比例分布。也就是说，从组中选择的测试输入数量是α·n，而从少数空间中选择测试输入的数量是(1-α)·n（小于sm）。此外，PACE根据不同组大小的比例从每个组中选择测试输入，以保持测试能力的相似分布。即从第k组中选择的测试输入的数量是：</p><p><span><img class="small-img-inline" src="/2021/12/26/Practical%20Accuracy%20Estimation%20for%20Efficient%20Deep%20Neural/formula5.2.png" alt></span></p><p>​在确定每个组或少数空间中选择的测试输入数量后，PACE根据基于MMD-critic的原型选择方法（算法1中的第7-11行）或自适应随机选择方法（第12-14行）来选择构建一小组测试输入，即PACE的输出（算法1中的第15行）。开发人员只需标注这一小组测试输入即可在实践中估计整个测试集的准确性。</p><img src="/2021/12/26/Practical%20Accuracy%20Estimation%20for%20Efficient%20Deep%20Neural/algo1.png" alt><h2><span id="结论">结论</span></h2><p>​为提高深度神经网络（DNN）的测试效率，我们的目标是选择一小组可以精确估计整个测试集准确性的测试输入。我们只需标注这个小集合，而不必标注整个测试集。为了实现这一目标，我们提出了PACE，一种新颖而实用的方法来选择小集合测试输入集。为了使PACE更加实用，PACE结合了聚类，将不同测试能力的测试输入聚为不同组，然后利用MMD-critic算法从每个组中选择原型，并借助自适应随机测试思想，通过考虑样本下从少数空间（即没有聚类到任何组的测试输入）选择测试输入，以构成最后的小测试输入集合。结果表明，PACE 实现了很好的准确度估计效果。</p>]]></content>
    
    
    <summary type="html">&lt;img src=&quot;/2021/12/26/Practical%20Accuracy%20Estimation%20for%20Efficient%20Deep%20Neural/pic1.jpg&quot; alt&gt;

&lt;h1 id=&quot;高效深度神经网络测试的实用准确度估计&quot;&gt;&lt;a href=&quot;#高效深度神经网络测试的实用准确度估计&quot; class=&quot;headerlink&quot; title=&quot;高效深度神经网络测试的实用准确度估计&quot;&gt;&lt;/a&gt;高效深度神经网络测试的实用准确度估计&lt;/h1&gt;&lt;h2 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要&quot;&gt;&lt;/a&gt;摘要&lt;/h2&gt;&lt;p&gt;​	深度神经网络 (Deep neural network, DNN) 愈发流行，DNN的测试对DNN的正确性（即DNN在指定工作中的准确度）至关重要。然而，DNN 测试存在严重的效率问题，即，标注每个测试输入以了解DNN在测试集上的准确度的成本很高，因为需要大量的人（甚至他们还需要特定领域知识）来对测试数据进行手工标注，而测试集本身又是大规模的。为缓解此问题，本文提出了一种叫PACE新颖手法（Practical ACcuracy Estimation的缩写），其选择一小部分测试输入，可以精确地估计整个测试集的准确性。这样一来，只需对这小部分选定样本进行标注即可，这很大程度上降低了标注成本。除了实现精确的准确性估计外，为使PACE更加实用，其还被要求应该是可解释的、确定的，并尽可能地高效的。因此，PACE首先结合聚类，将具有不同测试能力（即测试DNN模型不同功能）的测试输入划分为不同的组。然后，PACE利用MMD-critic算法（一种最先进的基于样例的解释算法，根据组的大小，从每组中选择原型，即选择最具代表性的测试输入）来可以减少聚类带来的噪音影响。同时，PACE还使用了自适应随机测试的思想，从少数空间（即没有聚成任何一组的测试输入）中选择测试输入，以保证在所需的测试输入数量下实现极大的多样性。两个平行的选择过程（即同时从组和少数空间中进行选择）组建了最终的一小部分被选中的测试输入集合。结果表明，PACE能够精确估计整个测试集的准确性，平均偏差只有1.181%∼2.302%，大大超过了最先进的方法。&lt;/p&gt;</summary>
    
    
    
    <category term="论文阅读" scheme="https://values.keys.moe/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="论文阅读" scheme="https://values.keys.moe/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
  </entry>
  
  <entry>
    <title>Delving into Data: Effectively Substitute Training for Black-box Attack</title>
    <link href="https://values.keys.moe/2021/12/07/Delving%20into%20Data%20Effectively%20Substitute%20Training%20for%20Black-box%20Attack/"/>
    <id>https://values.keys.moe/2021/12/07/Delving%20into%20Data%20Effectively%20Substitute%20Training%20for%20Black-box%20Attack/</id>
    <published>2021-12-07T10:06:33.000Z</published>
    <updated>2023-10-25T15:00:29.547Z</updated>
    
    <content type="html"><![CDATA[<img src="/2021/12/07/Delving%20into%20Data%20Effectively%20Substitute%20Training%20for%20Black-box%20Attack/封面.png" alt><h1><span id="深入研究数据用于黑盒攻击的有效替代训练">深入研究数据：用于黑盒攻击的有效替代训练</span></h1><h2><span id="摘要">摘要</span></h2><p>​深度模型在处理对抗样本时显示了它们的脆弱性。对于黑盒攻击，在无法访问被攻击模型的架构和权重的情况下，大家广泛聚焦于训练对抗攻击的替代模型的方法。以往的替代训练方法主要是基于真实训练数据或合成数据来窃取目标模型的知识，而没有探索什么样的数据可以进一步提高替代模型和目标模型之间的可转移性。本文中，我们提出了一种新视角的替代训练，聚焦于设计知识窃取过程中使用的数据分布。更具体地说，我们提出了一个多样化数据生成模块来合成具有广泛分布的大规模数据。我们还引入了对抗替代训练策略，以关注分布在决策边界附近的数据。这两个模块的结合可以进一步提高替代模型和目标模型的一致性，从而大大提高了对抗攻击的有效性。大量的实验证明了我们的方法在非定向和定向攻击设置下对最先进的竞争对手的有效性。我们还提供了详细的可视化和分析，以帮助理解我们方法的优势。</p><span id="more"></span><h2><span id="1-介绍">1. 介绍</span></h2><p>​尽管在大多数计算机视觉任务中取得了优异的性能，但深度神经网络（DNN）已被证明容易受到难以察觉的对抗噪声或扰动的影响。对抗样本的存在揭示了将DNN部署到现实世界应用中的重要安全风险。当前社区以白盒与黑盒的设置之分来研究对抗攻击（它们的差异在于是否能完全访问目标攻击模型）。事实上，由于白盒攻击所需的完整目标模型信息在现实世界的部署中是不可用的，本文聚焦于黑盒攻击，其通常只基于目标模型的标签或输出分数来生成对抗样本。通常，黑盒攻击包括基于分数的方法或基于决策的方法。然而，在这些攻击中，需要对目标模型进行雪崩式查询，这仍可能限制它们在现实情况下攻击DNN的可用性。<br>​最近，替代训练的想法在黑盒攻击中得到了广泛的探索。通常情况下，该方法并不是直接生成对抗样本，而是训练一个替代模型，在相同的输入数据的查询下，做出与目标模型类似的预测。在一定数量的查询下，这种方法通常能够根据目标模型学习到替代模型。因此可以对替代模型进行攻击，然后可以转移到目标模型上。<br>​从根本上说，替代模型试图通过给出输入数据和相应的查询标签来从目标模型中获取知识。而关键问题是，输入数据是否来自目标模型的训练数据？假设“是”的话，它确实简化了替代训练。然而，在许多现实世界的视觉任务中，收集真实的输入数据甚至是非平凡的。例如，人物照片和视频的数据受到非常严格的控制，个人数据的隐私在很多国家都受到法律的保护。此外，真实图像是最有效的替代训练数据吗？目标模型的训练数据确实有助于在原始任务上得到一个表现良好的替代模型，但其不能保证攻击从替代模型到目标模型的可转移性。为了提高替代训练中的攻击性能，有必要使替代模型和目标模型之间的决策边界距离最小化，这不仅需要大规模和多样化的训练数据，而且特别需要分布在决策边界附近的数据。</p><p>​为了解决真实数据的局限性并探索替代训练数据的更好分布，我们提出了一种新颖的任务驱动统一框架，该框架仅使用专门设计的生成数据进行替代训练，并实现了高攻击性能。如图所示，与使用目标模型的训练数据进行替代训练相比，多样化的合成数据与对抗样本相结合，将促进替代模型进一步接近目标。更具体地说，在我们的框架中，我们首先提出了一个新颖的多样化数据生成模块（Data Generation module, DDG），该模块将噪声采样与标签嵌入信息相结合来生成多样化的训练数据。这样的分布式生成数据基本可以保证替代模型从目标中学习知识。此外，为进一步促使替代模型具有与目标相似的决策边界，我们提出了对抗替代训练策略（Adversarial Substitute Training strategy, AST），其将对抗样本作为边界数据引入训练过程。总的来说，DDG和AST的联合学习保证了替代模型和目标模型之间的一致性，这大大提高了在没有任何真实数据的情况下进行黑盒攻击的替代训练的成功率。</p><p><span><img class="small-img-inline" src="/2021/12/07/Delving%20into%20Data%20Effectively%20Substitute%20Training%20for%20Black-box%20Attack/图1.png" alt></span></p><p>图1 将真实数据和合成数据应用于替代训练之间的差异。T&#x2F;S表示目标&#x2F;替代模型，(b)中的蓝色+&#x2F;-表示对抗样本，绿色&#x2F;红色虚线表示决策边界。比较(a)和(b)可以得到，我们的方法生成的合成数据可以训练一个与目标模型具有更相似决策边界的替代模型。</p><p>​本工作的主要贡献总结为：（1）我们首次提出了一种新的基于有效生成的替代训练范式，其通过深入研究输入生成的替代训练数据的本质，来提高无数据黑盒攻击的性能。（2）为了实现这一目标，我们首先提出了一个具有多种不同约束的多样化数据生成模块，以扩大合成数据的分布。然后通过对抗替代训练策略进一步提高替代模型和目标模型之间决策边界的一致性。（3）对四个数据集和一个在线机器学习平台的综合实验和可视化证明了我们的方法相对最先进攻击的有效性。</p><h2><span id="2-方法">2. 方法</span></h2><h3><span id="21-框架概述">2.1. 框架概述</span></h3><p>​我们工作的目标是为黑盒对抗攻击有效地训练替代模型，提出的框架如图所示。它由两个模块组成。多样化数据生成模块（DDG）产生多样化的数据，对抗替代训练策略（AST）进一步模仿目标模型的“行为”。在(a)中，DDG根据随机噪声z(i）和标签索引i的标签嵌入向量e(i)生成数据<span><img class="small-img-inline" src="/2021/12/07/Delving%20into%20Data%20Effectively%20Substitute%20Training%20for%20Black-box%20Attack/公式1.png" alt></span>。为了保证合成数据的多样性，生成器G通过三个约束进行训练，即自适应标签归一化生成器、噪声&#x2F;标签重建和类间多样性，这将在后面详细说明。此外，为了确保替代模型S接近目标模型T的决策边界，我们将合成的数据和AST的对抗样本一起输入S以进行(b)中的替代训练。从本质上讲，我们把目标模型T作为分类为M类的黑盒，其中只有标签&#x2F;概率输出可用。师生策略在这里被用于从T学习到S。最后，攻击可以在替代模型上进行，然后转移到目标模型上。</p><p><span><img class="small-img-inline" src="/2021/12/07/Delving%20into%20Data%20Effectively%20Substitute%20Training%20for%20Black-box%20Attack/图2.png" alt></span></p><p>​图2 整体框架图示。其由多样化数据生成模块（DDG）和对抗替代训练模块（AST）组成。（a）DDG旨在生成具有给定标签的不同数据以训练替代模型。（b）AST利用从当前替代模型生成的对抗样本来推动替代模型模仿目标的边界。</p><h3><span id="22-多样化数据生成">2.2. 多样化数据生成</span></h3><p>​为了合成更好的数据用于替代训练，我们首先提出了一个新颖的多样性数据生成模块（DDG），该模块有三个约束条件来操纵生成的合成图像的多样性。这些约束条件原则上鼓励生成器G为每个不同的类学习相对独立的数据分布，并保持类间差异，从而促进替代模型学习目标模型的知识。</p><p>​<strong>自适应标签归一化生成器</strong>。为了更好地从目标模型中学习，我们需要所有类别的平均分布数据进行替代训练，因此有必要生成标签控制的数据。为了实现这一点，我们充分利用了给定的标签和随机噪声。首先，通过从标准高斯分布和标签i中采样的随机噪声向量<span><img class="small-img-inline" src="/2021/12/07/Delving%20into%20Data%20Effectively%20Substitute%20Training%20for%20Black-box%20Attack/公式2.png" alt></span>的输入，我们计算出基于嵌入层的标签嵌入向量<span><img class="small-img-inline" src="/2021/12/07/Delving%20into%20Data%20Effectively%20Substitute%20Training%20for%20Black-box%20Attack/公式3.png" alt></span>。这种标签嵌入过程可以将单个离散标签编码为一个连续的可学习向量，它在特征空间中的分布更广，包含更多的表示信息。与GAN不同，我们没有使用真实的图像进行监督，这样的标签嵌入过程对数据的生成至关重要。接下来，我们通过两个全连接层从N维标签嵌入向量e(i)中提取平均值μ(i)和方差σ(i)。之后，将μ(i)和σ(i)加入到所有的反卷积块中，迭代合成具有特定类别条件的图像数据，可以表示为：</p><p><span><img class="small-img-inline" src="/2021/12/07/Delving%20into%20Data%20Effectively%20Substitute%20Training%20for%20Black-box%20Attack/公式4.png" alt></span></p><p>其中总共有五个反卷积块，t表示反卷积块的数量。在获得最终的ˆx(i)之后，输出生成的数据已经被标签归一化的信息进行了修改。这种自适应标签归一化生成器可以更好地利用输入噪声和标签嵌入向量之间的关系来合成标签控制的数据。</p><p><strong>噪声&#x2F;标签重建</strong>。为了进一步确保生成数据ˆx (i)的多样性，我们引入了一个重建网络R来重建输入噪声和标签嵌入<span><img class="small-img-inline" src="/2021/12/07/Delving%20into%20Data%20Effectively%20Substitute%20Training%20for%20Black-box%20Attack/公式5.png" alt></span>。而相应的重建损失可以计算为</p><p><span><img class="small-img-inline" src="/2021/12/07/Delving%20into%20Data%20Effectively%20Substitute%20Training%20for%20Black-box%20Attack/公式6.png" alt></span></p><p>其中我们使用L1来表示输入z(i)和重建的z(i)r之间的差异。对于标签重建，我们应用函数f(∗)来计算e(i)r和e之间的余弦距离，并由Softmax进一步处理以计算与真实标签i的交叉熵损失。在这种约束下，G可以为每个类别的不同输入噪声向量生成更多样的图像。</p><p><strong>类间多样性</strong>。为了进一步增强不同类别的数据多样性，我们使用余弦相似矩阵来最大化所有合成图像的类间距离。特别地，生成器生成一个MB≪M的不同类别的输入合成数据batch，模型S给出这个batch的输出相似度矩阵<span><img class="small-img-inline" src="/2021/12/07/Delving%20into%20Data%20Effectively%20Substitute%20Training%20for%20Black-box%20Attack/公式7.png" alt></span>。请注意，我们有真实的相似性矩阵<span><img class="small-img-inline" src="/2021/12/07/Delving%20into%20Data%20Effectively%20Substitute%20Training%20for%20Black-box%20Attack/公式8.png" alt></span>，除了对角线元素被设置为1，其他元素都是0。因此，多样性损失函数Ldiv可以表述为：</p><p><span><img class="small-img-inline" src="/2021/12/07/Delving%20into%20Data%20Effectively%20Substitute%20Training%20for%20Black-box%20Attack/公式9.png" alt></span></p><p>其中，TRI(∗)被定义为提取相似度矩阵中除对角线元素外的上三角元素的操作。通过这种方式，Ldiv确保合成数据拥有每个类别的独立分布。</p><h3><span id="23-对抗替代训练">2.3. 对抗替代训练</span></h3><p>​在DDG生成多样化的训练数据后，为了获得更好的攻击性能，我们还需要进一步鼓励与目标决策边界更相似的替代模型。众所周知，对抗样本拥有视觉上难以区分的扰动，其会被模型错误的分类。由于扰动相对较小，对抗样本可以看作是决策边界周围的样本。因此，我们提出了一种新颖的对抗替代训练策略（AST），它利用对抗样本进一步推动S的决策边界与T的决策边界相似。更具体地说，对于训练期间的每次迭代，我们的生成器首先通过DDG合成图像。然后我们选择白盒攻击算法来获得基于当前S的合成图像的对抗扰动ε。生成对抗图像的目标函数定义为</p><p><span><img class="small-img-inline" src="/2021/12/07/Delving%20into%20Data%20Effectively%20Substitute%20Training%20for%20Black-box%20Attack/公式10.png" alt></span></p><p>其中L(·) 表示攻击目标，反映了预测xˆ(i)+ε的标签为iadv的概率或交叉熵。如果考虑非定向攻击，则iadv≠i，否则iadv&#x3D;t，t为目标标签。λ是正则化系数，约束ε∈[0, 1]d将扰动ε限制在有效图像空间内。然后，生成的图像和相应的对抗数据被用来一起更新S。</p><h3><span id="24-损失函数">2.4. 损失函数</span></h3><p>​最后，我们应用基本损失函数来训练替代模型</p><p><span><img class="small-img-inline" src="/2021/12/07/Delving%20into%20Data%20Effectively%20Substitute%20Training%20for%20Black-box%20Attack/公式11.png" alt></span></p><p>其中Ld测量T和S输出之间的距离，Lc表示生成损失。e-Ld表示Ld的“最小-最大”博弈，CE(·)表示S的预测和输入的真实标签i之间的交叉熵损失。因此，凭借这两个损失函数的交替最小化，替代模型S可以学习模仿目标模型T的输出。在DDG和AST的进一步推动下，通过生成的数据和对抗样本，用于训练S和G的统一替代训练损失LS和生成器损失LG被定义为</p><p><span><img class="small-img-inline" src="/2021/12/07/Delving%20into%20Data%20Effectively%20Substitute%20Training%20for%20Black-box%20Attack/公式12.png" alt></span></p><p>其中Ladvd的定义与等式中的Ld相同。之前的公式使用对抗样本作为输入测量T和S的输出间的距离，Ladvc被定义为Lc，以约束以对抗样本作为输入的生成，Lrec和Ldiv被用来增强数据多样性。β1、β2和β3是DDG的平衡超参数。总的来说，训练过程如下。</p><p><span><img class="small-img-inline" src="/2021/12/07/Delving%20into%20Data%20Effectively%20Substitute%20Training%20for%20Black-box%20Attack/算法1.png" alt></span></p><h2><span id="3-实验">3. 实验</span></h2><h3><span id="31-实验设置">3.1. 实验设置</span></h3><p>​<strong>数据集和目标模型</strong>。1) MNIST：攻击模型在AlexNet、VGG-16和ResNet-18上进行预训练。默认的替代模型是具有3个卷积层的网络。2)CIFAR-10：攻击者在AlexNet、VGG-16和 ResNet-18上进行了预训练。默认替代模型是VGG-13。3) CIFAR-100：被攻击者在VGG-19和ResNet50上进行了预训练。默认的替代模型是ResNet-18。4)Tiny Imagenet：被攻击者在ResNet-50上进行了预训练。替代模型是ResNet-34。<br>​<strong>比较对象</strong>。为了验证所提出方法的有效性，我们将我们的攻击结果与无数据黑盒攻击（如DaST）和几种需要真实数据的黑盒攻击（如PBBA和Knockoff）。我们还使用被攻击模型的原始训练数据进行替代训练，并利用ImageNet学习替代模型。<br>​<strong>实现细节</strong>。我们使用Pytorch进行实现。我们利用Adam从头开始训练我们的替代模型、生成器和重建网络，所有权重都使用标准差为0.02的截断正态分布随机初始化。所有网络的初始学习率均设置为0.0001，从第80个epoch开始逐渐降低到0，并在第150个epoch后停止。我们将mini-batch大小设置为500，超参数β1、β2和β3值为1。我们的模型由一个 NVIDIA GeForce GTX 1080Ti GPU训练。我们应用PGD作为在AST和评估期间生成对抗图像的默认方法。我们还使用FGSM、BIM和C&amp;W进行广泛实验的攻击。<br>​<strong>评价指标</strong>。 考虑到 DaST中提出的存在两种不同的场景，即只从目标模型中获取输出标签并很好地访问输出概率，我们将这两种场景命名为基于概率和基于标签。在实验中，我们报告了由替代模型产生的对抗样本攻击目标黑盒模型的攻击成功率（ASRs）。按照DaST的设置，在非目标攻击设置中，我们只在被攻击模型正确分类的图像上生成对抗样本。对于目标攻击，我们只在没有被分类到特定错误标签的图像上生成对抗样本。为了公平比较，在所有的对抗样本过程中，设定扰动ε&#x3D;8。我们对每个测试进行五次，并报告平均结果。</p><h3><span id="32-黑盒攻击结果">3.2. 黑盒攻击结果</span></h3><p>​我们与比较对象手在四个数据集和一个在线机器学习平台上评估我们的方法，包括了定向攻击和非定向攻击设置。如表1、2、3所示，我们在基于概率和基于标签的场景下对每个数据集的多个目标模型进行了广泛的比较。</p><p><span><img class="small-img-inline" src="/2021/12/07/Delving%20into%20Data%20Effectively%20Substitute%20Training%20for%20Black-box%20Attack/表1.png" alt></span><br>表1 使用概率作为我们的方法和比较对象在多个数据集上的目标模型输出来比较ASRs结果</p><p><span><img class="small-img-inline" src="/2021/12/07/Delving%20into%20Data%20Effectively%20Substitute%20Training%20for%20Black-box%20Attack/表2.png" alt></span><br>表2 使用标签作为我们的方法和比较对象在多个数据集上的目标模型输出来比较ASRs结果</p><p><span><img class="small-img-inline" src="/2021/12/07/Delving%20into%20Data%20Effectively%20Substitute%20Training%20for%20Black-box%20Attack/表3.png" alt></span><br>表3 比较攻击Microsoft Azure示例模型下我们的方法和比较对象的ASRs结果</p><p>​<strong>与真实数据进行替代训练进行比较</strong>。这里我们研究了用真实图像进行攻击的替代训练，如表1和表2所示。我们直接使用目标模型或ImageNet的原始训练数据进行替代训练，而不是合成训练。结果显示，真实图像可以让替代模型从目标中学习到部分东西，在分类上可能会有更高的准确率，但与生成的数据相比，攻击强度较弱。我们认为，这个问题是由真实图像的数量和多样性限制造成的，这可能导致替代模型学习和模仿目标模型的失败。因此，我们提出了一种DDG策略来合成大规模和多样化的数据。<br>​<strong>与最新方法的比较</strong>。如表1和表2所示，我们将我们的方法与黑盒攻击进行比较。对于定向攻击和非定向攻击，我们的方法在所有数据集下都取得了比基于概率和基于标签的方案更好的ASRs。此外，与相似的生成法DaST相比，我们的方法很大程度上优于它。这些结果验证了所提出的方法让替代模型更好地接近目标的决策边界和实现无数据黑盒攻击的高ASRs这两个方面的有效性。<br>​<strong>在Microsoft Azure上与比较对象的比较</strong>。为了更好地评估实际应用下的攻击手段能力，我们在Microsoft Azure上进行了在线模型攻击实验。通过以攻击Azure上机器学习教程中的MNIST模型为目标，我们比较了我们的方法和竞争对手之间的结果。表3中显示的结果表明了我们的方法可以在在线模型上获得最好的ASR，这进一步证明了我们的方法在没有攻击先验知识的真实场景下的有效性。</p><h2><span id="4-结论">4. 结论</span></h2><p>​本文重点研究黑盒攻击替代训练的生成数据的分布。它提出了一个统一的替代模型训练框架，包含一个多样化数据生成模块（DDG）和一个对抗替代训练策略（AST）。DDG可以生成标签控制的和多样化的数据来训练替代模型。AST利用对抗样本作为边界数据，使替代模型更好地符合目标的决策边界。大量实验表明该方法可以实现高攻击性能。</p>]]></content>
    
    
    <summary type="html">入门介绍</summary>
    
    
    
    <category term="论文阅读" scheme="https://values.keys.moe/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="论文阅读" scheme="https://values.keys.moe/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
  </entry>
  
  <entry>
    <title>DeepInspect A Black-box Trojan Detection and Mitigation Framework for Deep Neural Networks</title>
    <link href="https://values.keys.moe/2021/06/30/DeepInspect%20A%20Black-box%20Trojan%20Detection%20and%20Mitigation%20Framework%20for%20Deep%20Neural%20Networks/"/>
    <id>https://values.keys.moe/2021/06/30/DeepInspect%20A%20Black-box%20Trojan%20Detection%20and%20Mitigation%20Framework%20for%20Deep%20Neural%20Networks/</id>
    <published>2021-06-30T13:25:52.000Z</published>
    <updated>2023-10-25T16:07:18.610Z</updated>
    
    <content type="html"><![CDATA[<img src="/2021/06/30/DeepInspect%20A%20Black-box%20Trojan%20Detection%20and%20Mitigation%20Framework%20for%20Deep%20Neural%20Networks/image-20231025234243140.png" alt><h1><span id="deepinspect深度神经网络的黑盒木马检测与缓解框架">DeepInspect：深度神经网络的黑盒木马检测与缓解框架</span></h1><h2><span id="引用">引用</span></h2><p>Chen H, Fu C, Zhao J, et al. DeepInspect: A Black-box Trojan Detection and Mitigation Framework for Deep Neural Networks[C]&#x2F;&#x2F;IJCAI. 2019: 4658-4664.</p><h2><span id="摘要">摘要</span></h2><p>深度神经网络 (DNN) 容易受到神经木马 (NT) 攻击。在神经木马攻击中，攻击者在DNN训练期间注入恶意行为。这种类型的“后门”攻击在输入标记有被攻击者指定的触发器（trigger）段时激活，其会导致模型预测错误。由于DNN在各关键领域中被广泛应用，因此在使用模型之前检测预训练的DNN是否感染木马是必不可少的操作。我们在本文中的目标是解决对未知DNN的NT攻击的安全问题，确保安全的模型部署。我们提出了DeepInspect，这是第一个具有最小的模型先验知识的黑盒木马检测解决方案。DeepInspect使用条件生成模型从查询的模型中学习潜在触发器的概率分布，从而检索出后门插入的足迹。除了NT检测之外，我们还表明DeepInspect的触发器生成器能够通过模型修补来有效缓解木马感染。我们证实了DeepInspect对各种基准的最先进的NT攻击的有效性、效率和可扩展性。广泛的实验表明，DeepInspect提供了卓越的检测性能和比以前的工作更低的运行时间开销。</p><span id="more"></span><h2><span id="介绍">介绍</span></h2><p>深度神经网络（DNN）已被证明了其卓越的性能，其被越来越多地应用于各关键应用中，包括人脸识别、生物医学诊断、自动驾驶等。由于训练一个高精度的DNN很耗费时间和资源，客户常常从目前供应链中的第三方获得预训练的深度学习（DL）模型。Caffe Model Zoo是一个向用户公开分享预训练模型的平台样例。DNN训练的不透明性为攻击者打开了一个安全漏洞，使得他们可以通过干扰训练过程插入恶意行为。在推理阶段，任何带有触发器的输入数据都会被受感染的DNN错误分类为攻击目标。例如，如果触发器被添加到输入的“右转”标志上，则木马模型预测“左转”。</p><p>这种类型的神经木马（NT）攻击（也称为 “后门 “攻击）已在之前的工作中被发现，其具有两个关键特性：(i)有效性：带有触发器的输入被高概率地预测为攻击目标；(ii)隐蔽性：插入的后门对合法输入（即输入中没有触发器）保持隐蔽。这两个特性使NT攻击具有威胁性，并且难以检测。现有的研究侧重于确定输入是否包含假设查询模型已被感染的触发器（即“输入的健全性检查”）。</p><p>由于以下挑战，检测未知 DNN 的木马攻击很困难：(C1) 后门的隐蔽性使得它们难以通过功能测试（以测试准确性作为检测标准）来识别；(C2)在木马检测过程中仅能获取有限的有关查询模型的信息。干净的训练数据集或黄金参考模型在实际环境中可能不可获得。训练数据包含用户的个人信息，因此通常不会与预训练的DNN一起发布。(C3)攻击者指定的攻击目标对防御者来说是未知的。此处攻击者是恶意模型提供者，而防御者是终端用户。攻击者目标的这种不确定性使 NT 检测变得复杂，因为对于具有众多输出类别的大规模模型而言，暴力搜索所有可能的攻击目标是不切实际的。</p><p>据我们所知，Neural Cleanse是现有的唯一针对检查DNN对后门攻击的脆弱性的工作。然而，NC提出的后门检测方法依赖于干净的训练数据集，其中不包含任何恶意操纵的数据点。由于原始训练数据的私有性，这种假设限制了他们方法的应用场景。为了应对这些挑战（C1-C3），我们提出了DeepInspect，这是第一个实用的木马检测框架，其可根据最少的查询模型信息，确定DNN是否已经被后门注入（即“预训练模型的健全性检查”）。DeepInspect(DI)包括三个主要步骤：模型反转以恢复替代训练数据集，使用条件生成对抗网络（cGAN）进行触发器重建，以及基于统计假设检验的异常检测。本文的技术贡献总结如下：</p><p>* 启用 DNN 的神经木马检测。我们提出了第一个后门检测框架，该框架无需干净的训练数据或正确参考模型的帮助即可检查预训练 DNN 的安全性。我们的威胁模型所做的最小假设确保了DeepInspect的广泛适用性。</p><p>* 在各种DNN基准上对 DeepInspect 进行综合评估。我们进行了大量实验来证实DeepInspect的功效、效率和可扩展性。我们证明DeepInspect与之前的NT检测方案相比，明显更为可靠。</p><p>* 提出了一种新型的木马缓解模型修补方案。DeepInspect的条件生成模型所恢复的触发器揭示了被查询模型的敏感性。我们表明，防御者可以利用触发器生成器进行对抗训练并使插入的后门失效。</p><h2><span id="deepinspect框架">DeepInspect框架</span></h2><h3><span id="木马检测概述">木马检测概述</span></h3><p>DeepInspect背后的关键直觉如图1所示。木马插入的过程可以被认为是在合法的数据点附近添加多余的数据点，并将其标记为攻击目标。从原始数据点到恶意数据点的移动即是后门攻击中使用的触发器。作为木马插入的结果，我们可以从图1中观察到，将合法数据转化为属于攻击目标的样本所需的扰动与相应的良性模型中的扰动相比要小。DeepInspect识别这种“小”触发器的存在，例如木马插入留下的“足迹”，并恢复潜在触发器以提取扰动统计信息。</p><p> <span><img class="small-img-inline" src="/2021/06/30/DeepInspect%20A%20Black-box%20Trojan%20Detection%20and%20Mitigation%20Framework%20for%20Deep%20Neural%20Networks/image-20231025234254867.png" alt></span></p><p>图1 DeepInspect木马检测背后的直觉。在这里，我们考虑三分类问题。定义∆AB表示将A类中的所有数据样本转移到B类所需的扰动，∆A表示将所有其他类中的数据点转移到A类的扰动：∆A&#x3D;max(∆BA, ∆CA)。一个具有攻击目标A的木马模型满足∆A&lt;&lt;∆B, ∆C，而在良性模型中，这三个值之间的差异较小。</p><p>图2说明了DeepInspect的整体框架。DI首先采用了模型反转（Model Inversion, MI）方法，生成一个包含所有类别的替代训练数据集。然后，训练一个条件GAN来生成可能的木马触发器，将被查询的模型部署为固定的判别器D。为了反向设计木马触发器，DI构建了一个条件生成器G(z, t)，其中z是一个随机噪声向量，t是目标类。G被训练来学习触发器的分布，即，被查询的DNN应在反向数据样本x和G的输出的叠加上预测攻击目标t。最后，恢复的触发器的扰动等级（变化幅度）被用作异常检测的测试统计数据。我们基于假设测试的木马检测是可行的，因为其探索了后门插入的内在“足迹”。</p><p> <span><img class="small-img-inline" src="/2021/06/30/DeepInspect%20A%20Black-box%20Trojan%20Detection%20and%20Mitigation%20Framework%20for%20Deep%20Neural%20Networks/image-20231025234304063.png" alt></span></p><p>图2 DeepInspect框架的全局流图。</p><h3><span id="威胁模型">威胁模型</span></h3><p>DeepInspect以最小的假设检验被查询的DNN对NT攻击的敏感性，从而解决上一节提到的有限信息(C2)的挑战。更具体地说，我们假设防御者对所查询的DNN具有以下知识：输入数据的维度、输出类别的数量以及给定任意输入查询的模型的置信度分数。此外，我们假设攻击者有能力将任意类型和比例的毒害数据注入训练集，以达到其所期望的攻击成功率。我们强大的威胁模型确保了DeepInspect在现实世界环境中的实际使用有效，而前人的工作需要一个良性的数据集来协助后门检测。</p><p><strong>DeepInspect方法</strong></p><p>DeepInspect框架由三个主要步骤组成。(i)模型反转：防御者首先在被查询的DNN上应用模型反转，以恢复一个覆盖所有输出类别的替代训练数据集<span><img class="small-img-inline" src="/2021/06/30/DeepInspect%20A%20Black-box%20Trojan%20Detection%20and%20Mitigation%20Framework%20for%20Deep%20Neural%20Networks/image-20231025234312623.png" alt></span>。恢复的数据集在下一步被GAN训练使用，以解决挑战C2；(ii)触发器生成。DI利用生成模型来重构木马攻击所使用的可能的触发器模式。由于攻击目标（受感染的输出类）对防御者来说是未知的(C3)，我们采用一个条件生成器，有效构建属于不同攻击目标的触发器；(iii)异常检测：在使用cGAN为所有输出类生成触发器后，DI 将木马检测定义为一个异常检测问题。收集所有类别的扰动统计数据，左尾概率的异常表明后门的存在。</p><p>模型反转</p><p>回顾一下，我们的威胁模型假设在木马检测过程中没法获得干净的训练数据集。因此，我们采用模型反转来恢复一个替代训练集<span><img class="small-img-inline" src="/2021/06/30/DeepInspect%20A%20Black-box%20Trojan%20Detection%20and%20Mitigation%20Framework%20for%20Deep%20Neural%20Networks/image-20231025234322043.png" alt></span> ，在下一步协助生成器训练。已有研究证明了数据可以从预训练的模型中提取，并将模型反转表述为一个优化问题。MI的目标函数如式（1)所示，通过GD进行迭代最小化。</p><p> <span><img class="small-img-inline" src="/2021/06/30/DeepInspect%20A%20Black-box%20Trojan%20Detection%20and%20Mitigation%20Framework%20for%20Deep%20Neural%20Networks/image-20231025234334040.png" alt></span></p><p>这里，x是输入数据，t是要恢复的目标类，f是查询模型在给定x作为其输入的情况下预测t的概率，AuxInfo(x) 是包含对输入的辅助约束的可选项。</p><p>触发器生成</p><p>DeepInspect的关键思想是训练一个条件发生器，学习木马触发器的概率密度分布（probability density distribution, pdf），其扰动等级用以检测统计。特别地，DI采用cGAN来“模拟”木马攻击的过程：D(x+G(z, t))&#x3D;t。这里，D是被查询的DNN，t是被检查的攻击目标，x是由MI获得的数据分布的样本，而触发器是条件发生器∆&#x3D;G(z, t)的输出。请注意，现有的使用固定触发器模式的攻击可以被认为是触发器分布为常值的特殊情况。</p><p> <span><img class="small-img-inline" src="/2021/06/30/DeepInspect%20A%20Black-box%20Trojan%20Detection%20and%20Mitigation%20Framework%20for%20Deep%20Neural%20Networks/image-20231025234340691.png" alt></span></p><p>图3 DeepInspect的条件GAN训练示意图。</p><p>图3显示了我们的触发器生成器的高层概述。回顾一下，DeepInspect将预先训练好的模型部署为固定的判别器D。因此，触发器生成的关键是制定损失函数来训练条件发生器。由于我们的威胁模型假设输入维度和输出类别的数量对防御者来说是已知的，他可以找到一个可行的G的拓扑结构，产生的触发器∆与反转输入x的形状一致。为了模仿木马攻击，DI首先加入了公式(2)所示的负对数似然损失（nll）来量化G的输出触发器的质量，以欺骗预训练模型D：</p><p> <span><img class="small-img-inline" src="/2021/06/30/DeepInspect%20A%20Black-box%20Trojan%20Detection%20and%20Mitigation%20Framework%20for%20Deep%20Neural%20Networks/image-20231025234345957.png" alt></span></p><p>此外，还集成了一个常规的对抗损失项，以确保“假”图像xt&#x3D;x+G(z,t)不能与原始图像用判别器D区分开：</p><p> <span><img class="small-img-inline" src="/2021/06/30/DeepInspect%20A%20Black-box%20Trojan%20Detection%20and%20Mitigation%20Framework%20for%20Deep%20Neural%20Networks/image-20231025234349478.png" alt></span></p><p>这里，mse表示”均方误差”损失函数。最后，我们通过在其l1范数上添加一个合页损失来限制G的输出量：</p><p> <span><img class="small-img-inline" src="/2021/06/30/DeepInspect%20A%20Black-box%20Trojan%20Detection%20and%20Mitigation%20Framework%20for%20Deep%20Neural%20Networks/image-20231025234354219.png" alt></span></p><p>对扰动幅度进行约束是稳定GAN训练的常见做法。上述三种损失的加权和被用来训练条件G。</p><p> <span><img class="small-img-inline" src="/2021/06/30/DeepInspect%20A%20Black-box%20Trojan%20Detection%20and%20Mitigation%20Framework%20for%20Deep%20Neural%20Networks/image-20231025234358282.png" alt></span></p><p>我们选择超参数γ1、γ2以确保G的输出触发器达到至少95%的攻击成功率。我们认为DeepInspect可以在黑盒环境下运行，因为我们的触发器恢复过程不需要任何关于模型内部的信息。</p><p><strong>异常检测</strong></p><p>DeepInspect探讨了这样的观察结果：与木马模型中其他未受感染的类相比，人们可以发现目标类的扰动等级异常小的触发器。在第二步中使用经过训练的生成器为每个类生成触发器后，DI 进行假设检验和鲁棒统计来检测触发器扰动中异常值的存在。更具体地说，我们使用”双中值绝对偏差”（DMAD）的一个变体作为检测标准。我们的DMAD方案首先计算出所有测试统计点S的中位数m，并使用它来分割触发器扰动的原始列表。然后计算左子组Sleft中所有数据点与组中位数的绝对偏差，将其表示为dev_left。总体偏差和一致性常数（正态分布为 1.4826）的乘积表示为 mad。</p><p>我们将数据点的“偏差因子（deviation factor, df）”定义为中位数的绝对偏差与MAD值之间的比率<span><img class="small-img-inline" src="/2021/06/30/DeepInspect%20A%20Black-box%20Trojan%20Detection%20and%20Mitigation%20Framework%20for%20Deep%20Neural%20Networks/image-20231025234415885.png" alt></span>遵循标准正态分布N(0,1)。显著性等级α和截止阈值c之间的关系描述如下：</p><p> <span><img class="small-img-inline" src="/2021/06/30/DeepInspect%20A%20Black-box%20Trojan%20Detection%20and%20Mitigation%20Framework%20for%20Deep%20Neural%20Networks/image-20231025234420540.png" alt></span></p><p>其中<span><img class="small-img-inline" src="/2021/06/30/DeepInspect%20A%20Black-box%20Trojan%20Detection%20and%20Mitigation%20Framework%20for%20Deep%20Neural%20Networks/image-20231025234425790.png" alt></span>。DI利用DMAD来估计群体的标准σ，并用样本的中位数来代替平均值μ。因此，归一化的RV C可用于对偏差因子进行建模，这意味着从公式(6)中得到的阈值c也适用于具有相同显着性等级α的df。DI通过允许防御者指定公式(6)中使用的显著性等级来提供可调整的检测性能。</p><h2><span id="评估">评估</span></h2><p>我们进行了大量实验来研究DeepInspect在各种基准测试中的性能。我们对先前的工作和检测开销分析进行了定量比较。</p><h3><span id="实验设置">实验设置</span></h3><p>我们针对两种最先进的木马攻击评估DeepInspect。</p><p>我们首先评估DI在BadNets中提出的后门插入方法的性能。触发器是图像右下角的一个白色方块。后门是通过使用操纵集和其余干净数据集这两者的混合来训练模型以实现嵌入的。我们在MNIST和GTSRB基准上进行BadNets攻击。我们还评估了TrojanNN攻击下的DI。Liu等设计了一个特定的触发器，将目标DNN中选定的神经元刺激到高激活值，而不是硬编码重新标记部分修改后的训练数据。我们分别在它们的开源代码在VGGFace 2和ResNet-18上进行带有方形和水印触发器的TrojanNN攻击。</p><p>在我们的实验中，我们在原始训练数据集中加入了~10%的操纵数据，以使我们使用BadNets攻击方法获得的所有木马基准测试达到95%以上的木马激活率。表1总结了上述两种木马攻击的设置和结果。</p><p> <span><img class="small-img-inline" src="/2021/06/30/DeepInspect%20A%20Black-box%20Trojan%20Detection%20and%20Mitigation%20Framework%20for%20Deep%20Neural%20Networks/image-20231025234433065.png" alt></span></p><p>表1 评估的木马攻击的总结。展示后门注入的设置和结果。</p><p>每个木马检测实验重复10次，本节中报告对应平均指标。为了验证DeepInspect的异常检测的可行性，我们测量了良性模型和木马模型的偏差因子，并在图4(a)中展示了结果。如果其偏差因子大于截止阈值，则确定查询模型被“感染”。使用α&#x3D;0.05的显著性等级（对应于截止阈值c&#x3D;2），如图4(a)所示，DI对所有受感染模型得出df&gt;2，对所有良性模型得出df&lt;2。因此，DI通过在所有基准测试中实现0%的假阳性率和0%的假阴性率，满足了”有效性”标准。</p><p>被感染的DNN和相应的良性DNN之间偏差因子的巨大差距表明，df是木马检测的有效指标。为了证实DI所使用的关键直觉（如图1所示），我们测量了DI的条件生成器所恢复的触发器的扰动等级，并在图4 (b)中可视化了它们的分布。可以观察到，受感染标签（用三角形表示）的扰动幅度远小于未受感染的类，因此其可在我们的检测中通过鲁棒的统计数据来使用。此外，与NC相比，我们为未感染标签恢复的测试统计量的分布具有更小的离散性，这表明我们产生了更可靠的检测结果。</p><p> <span><img class="small-img-inline" src="/2021/06/30/DeepInspect%20A%20Black-box%20Trojan%20Detection%20and%20Mitigation%20Framework%20for%20Deep%20Neural%20Networks/image-20231025234439157.png" alt></span></p><p>图4 (a) DeepInspect恢复的良性和木马模型的触发器的偏差因子。红色虚线表示显著性等级α&#x3D;0.05的决策阈值。(b)木马模型中对被感染和未被感染的标签生成的触发器的扰动等级（l1范数上的合页损失）。</p><p>接下来，我们比较了DeepInspect和Neural Cleanse在不同环境下的检测性能。由于NC假设有干净的数据集，我们在与DI相同的模型反转过程中得到的反转数据集上执行他们提出的检测方法，以确保两者的公平比较。</p><h3><span id="对触发器大小的敏感性">对触发器大小的敏感性</span></h3><p>攻击者使用的触发器的大小会影响DI和NC的检测性能，因为它会影响测试统计数据。更具体地说，DI利用恢复的触发器的合页损失作为统计数据，而NC使用l1范数作为决策标准。这里，我们在GTSRB基准上使用不同大小的方形触发器，并如图5所示比较两种方法的检测性能。可以看到，NC在大小为2×2、12×12和16×16的触发器上产生了三个假阴性报告。此外，NC的偏差因子随着触发器大小的增加呈现出下降的趋势，这表明检测统计量对触发器大小很敏感。DI在所有基准中都没有产生假阴性，因此与NC相比，它对触发器大小的增加不太敏感。在MNIST基准上也观察到类似的趋势。</p><p> <span><img class="small-img-inline" src="/2021/06/30/DeepInspect%20A%20Black-box%20Trojan%20Detection%20and%20Mitigation%20Framework%20for%20Deep%20Neural%20Networks/image-20231025234444026.png" alt></span></p><p>表5 木马检测对触发器大小的敏感性分析。图中显示了感染各种方形触发器的GTSRB基准上的DeepInspect和Neural Cleanse的偏差因子。红色虚线表示木马检测的截止阈值。</p><h3><span id="对木马目标数量的敏感性">对木马目标数量的敏感性</span></h3><p>我们在上一节中评估了DI在单目标木马攻击上的表现。此处，我们考虑一种更高级的后门攻击，即使用同一触发器感染多个输出类别。我们将这种类型的攻击命名为“多目标”木马。更具体地说，触发器标记的每个输入的目标标签t是从一组类T中随机选择的。如果模型的预测属于攻击目标集T，则认为后门被激活。我们使用BadNets中的木马插入方法，对MNIST基准进行单&#x2F;多目标后门攻击。受感染模型达到了与未受感染基线相当的测试准确率和高于98%的木马激活率。图6显示了DI和NC对攻击目标标签数量（表示为|T|）的敏感性。NC在所有三个多目标木马基准上都会表现出假阴性情况，而当|T|&#x3D;3和5时，DI 在查询模型中成功检测到木马。</p><p> <span><img class="small-img-inline" src="/2021/06/30/DeepInspect%20A%20Black-box%20Trojan%20Detection%20and%20Mitigation%20Framework%20for%20Deep%20Neural%20Networks/image-20231025234448285.png" alt></span></p><p>图6 木马检测对攻击目标数量的敏感度分析。DeepInspect 和 Neural Cleanse 在各种单&#x2F;多目标木马攻击设置中的偏差因子是在MNIST基准上测量的，触发器的大小为4×4。</p><h3><span id="开销分析">开销分析</span></h3><p>我们评估了DI的运行时开销，并在此与之前的工作进行比较。回顾一下，DI利用一个条件生成器来同时恢复属于多个类别的触发模式。此外，我们证明DI可以结合自动编码器来加速大型基准测试中的木马检测。相反，NC使用GD来单独搜索每个目标类中的触发器。NC与自动编码器不兼容，因为它分别恢复了二维掩码和三维触发器模式。</p><p>图 7 显示了DI和NC之间的总体相对运行时间比较。我们在具有8GiB内存的Nvidia RTX2080 GPU上实现这两种检测方法，并在模型反演过程中在每个类别中恢复5张图像。MI整个过程的运行时间可如图7示计算出来。经验结果显示，在MNIST（N &#x3D; 10）和GTSRB（N &#x3D; 43）基准上，NC比DI分别快1.7倍和1.2倍。然而，DI在ResNet-18（N &#x3D; 1000）和VGGFace基准（N &#x3D; 2622，在图7中表示为 “Trojan Square”和 “Trojan WM”）上比NC快5.3倍和9.7倍。可以看出，与NC相比，我们的框架在大型基准上速度更快。因此，DI的特点是在真实环境中，对具有诸多输出类别的DNN，具有更好的效率和可扩展性。</p><h3><span id="讨论">讨论</span></h3><p>让我们考虑木马插入过程中使用的源类和目标类的数量，目前的DI解决了多对一&#x2F;多对多的情况。DeepInspect可以很容易地扩展到检测具有不同机制的其他木马攻击。一个白盒自适应攻击者可以战略性地选择源类和目标类，使得误分类所需的扰动幅度不会明显小于其他未受影响的类。这种攻击可能会以降低效率为代价来降低 df。通过评估每个源-目标类对的所需扰动，DI可以被调整为检测干净标签攻击。我们评估了木马检测期间干净数据集可用时的DI，表明了DI的性能与NC相当。</p><p> <span><img class="small-img-inline" src="/2021/06/30/DeepInspect%20A%20Black-box%20Trojan%20Detection%20and%20Mitigation%20Framework%20for%20Deep%20Neural%20Networks/image-20231025234453225.png" alt></span></p><p>图7 与Neural Cleanse相比，DeepInspect的研究增速。自动编码器和MI的训练时间包含在DI何NC的运行时间中。橙色虚线表示模型反转的吞吐量（每秒的图像数）。与NC相比，DI在大型基准测试中表现出了更好的可扩展性。</p><h3><span id="通过模型修补的木马缓解">通过模型修补的木马缓解</span></h3><p>回顾一下，DeepInspect通过训练一个条件生成器来学习潜在触发器的pdf，从而有效地检测出后门攻击的发生。换句话说，一旦我们完成了G的训练，我们就有了一个能够为任何目标类构建不同触发模式的生成模型。因此，DeepInspect的生成器促进了 “对抗学习”，</p><p>可用于提高良性模型的鲁棒性，或“修补”受感染的DNN以阻止木马攻击。</p><p>在这里，我们展示了DeepInspect如何被用作补救方案，以减轻已确定的目标类t的木马攻击。我们通过用反转训练集<span><img class="small-img-inline" src="/2021/06/30/DeepInspect%20A%20Black-box%20Trojan%20Detection%20and%20Mitigation%20Framework%20for%20Deep%20Neural%20Networks/image-20231025234527060.png" alt></span>来构建补丁集。将另外10%的反转数据作为验证集，以寻找再训练配置（如batch大小、学习率等)。最后，根据数据应用中的原始损失，对受感染模型进行10个epoch的对抗训练。</p><p>表2总结了DeepInspect在没有干净数据下的对各种受感染的DNN上进行模型修补的结果。可以看到，我们的木马缓解方案有效地降低了嵌入式触发器的激活率，同时保持了模型在正常数据集上的性能。修补后的模型的偏差因子小于DeepInspect异常检测中使用的截止阈值c&#x3D;2，因此能够通过模型鲁棒性检查并安全部署。我们想强调的是，假设干净的数据是可获得的，那么修补后的TAR可以进一步降低到~3%。</p><p> <span><img class="small-img-inline" src="/2021/06/30/DeepInspect%20A%20Black-box%20Trojan%20Detection%20and%20Mitigation%20Framework%20for%20Deep%20Neural%20Networks/image-20231025234532763.png" alt></span></p><p>表2 对DeepInspect木马缓解方案的评估。在进行模型修补后，木马激活率（TAR）有效降低，测试准确率得以保留。</p><h2><span id="结论和未来的工作">结论和未来的工作</span></h2><p>我们提出了DeepInspect，这是在深度学习领域中第一个实用的木马检测和缓解方案，其对被查询模型的先验知识需求最小。DeepInspect将预先训练好的DNN作为其输入，并对模型的合理性返回一个二元结果（良性的&#x2F;被木马注入的）。与之前依靠干净数据集进行木马检测的工作不同，DeepInspect只需要对被查询的DNN进行黑盒访问，就能重建潜在的木马触发器。DeepInspect利用条件生成模型，同时学习多个攻击目标的触发器的概率分布。我们基于假设检验的异常检测允许防御者通过指定截止阈值来平衡检测率和误报率。我们针对两种最先进的木马攻击，对DeepInspect进行了大量的评估，以证实其与之前的工作相比具有高检测率和低误报率。除了卓越的后门检测性能外，DeepInspect的条件触发器生成器还支持有效的木马缓解解决方案，即使用对抗训练修补模型。</p><p>我们在这里讨论两个未来的研究方向。DeepInspect可以进行调整，以提高对更复杂的木马攻击（如大尺寸触发器和多目标后门）的检测性能。对于多目标木马攻击，可以修改损失定义<span><img class="small-img-inline" src="/2021/06/30/DeepInspect%20A%20Black-box%20Trojan%20Detection%20and%20Mitigation%20Framework%20for%20Deep%20Neural%20Networks/image-20231025234543226.png" alt></span>，以允许在G训练期间给定相同的操纵输入的多个目标类。此外，可以通过结合更高级的GAN训练策略来优化DI触发器恢复的运行时间。</p>]]></content>
    
    
    <summary type="html">&lt;img src=&quot;/2021/06/30/DeepInspect%20A%20Black-box%20Trojan%20Detection%20and%20Mitigation%20Framework%20for%20Deep%20Neural%20Networks/image-20231025234243140.png&quot; alt&gt;

&lt;h1 id=&quot;DeepInspect：深度神经网络的黑盒木马检测与缓解框架&quot;&gt;&lt;a href=&quot;#DeepInspect：深度神经网络的黑盒木马检测与缓解框架&quot; class=&quot;headerlink&quot; title=&quot;DeepInspect：深度神经网络的黑盒木马检测与缓解框架&quot;&gt;&lt;/a&gt;DeepInspect：深度神经网络的黑盒木马检测与缓解框架&lt;/h1&gt;&lt;h2 id=&quot;引用&quot;&gt;&lt;a href=&quot;#引用&quot; class=&quot;headerlink&quot; title=&quot;引用&quot;&gt;&lt;/a&gt;引用&lt;/h2&gt;&lt;p&gt;Chen H, Fu C, Zhao J, et al. DeepInspect: A Black-box Trojan Detection and Mitigation Framework for Deep Neural Networks[C]&amp;#x2F;&amp;#x2F;IJCAI. 2019: 4658-4664.&lt;/p&gt;
&lt;h2 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要&quot;&gt;&lt;/a&gt;摘要&lt;/h2&gt;&lt;p&gt;深度神经网络 (DNN) 容易受到神经木马 (NT) 攻击。在神经木马攻击中，攻击者在DNN训练期间注入恶意行为。这种类型的“后门”攻击在输入标记有被攻击者指定的触发器（trigger）段时激活，其会导致模型预测错误。由于DNN在各关键领域中被广泛应用，因此在使用模型之前检测预训练的DNN是否感染木马是必不可少的操作。我们在本文中的目标是解决对未知DNN的NT攻击的安全问题，确保安全的模型部署。我们提出了DeepInspect，这是第一个具有最小的模型先验知识的黑盒木马检测解决方案。DeepInspect使用条件生成模型从查询的模型中学习潜在触发器的概率分布，从而检索出后门插入的足迹。除了NT检测之外，我们还表明DeepInspect的触发器生成器能够通过模型修补来有效缓解木马感染。我们证实了DeepInspect对各种基准的最先进的NT攻击的有效性、效率和可扩展性。广泛的实验表明，DeepInspect提供了卓越的检测性能和比以前的工作更低的运行时间开销。&lt;/p&gt;</summary>
    
    
    
    <category term="论文阅读" scheme="https://values.keys.moe/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="论文阅读" scheme="https://values.keys.moe/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
  </entry>
  
  <entry>
    <title>Knowledge Graphs Enhanced Neural Machine Translation</title>
    <link href="https://values.keys.moe/2021/06/25/Knowledge%20Graphs%20Enhanced%20Neural%20Machine%20Translation/"/>
    <id>https://values.keys.moe/2021/06/25/Knowledge%20Graphs%20Enhanced%20Neural%20Machine%20Translation/</id>
    <published>2021-06-25T06:11:00.000Z</published>
    <updated>2023-10-25T15:28:11.918Z</updated>
    
    <content type="html"><![CDATA[<img src="/2021/06/25/Knowledge%20Graphs%20Enhanced%20Neural%20Machine%20Translation/clip_image002.jpg" alt><h1><span id="知识图谱加强神经机器翻译">知识图谱加强神经机器翻译</span></h1><h2><span id="引用">引用</span></h2><p>Zhao Y , Zhang J , Zhou Y , et al. Knowledge Graphs Enhanced Neural Machine Translation[C]&#x2F;&#x2F; Twenty-Ninth International Joint Conference on Artificial Intelligence and Seventeenth Pacific Rim International Conference on Artificial Intelligence {IJCAI-PRICAI-20. 2020.</p><h2><span id="摘要">摘要</span></h2><p>知识图谱（Knowledge graphs, KG）存储了大量关于各种实体的结构化信息，其中许多是神经机器翻译（neural machine translation, NMT）的平行句对所未覆盖的。为提高这些实体的翻译质量，在本文中我们提出了一种新颖的知识图谱加强神经机器翻译方法。具体来说，我们首先通过将源知识图谱和目标知识图谱转换为统一的语义空间，以归纳这些实体的新翻译结果。然后我们生成足够的伪平行句对，其中包含这些归纳实体对。最后，NMT模型由原始句和伪句对联合训练。对汉英和英日翻译任务的大量实验表明，我们的方法在翻译质量方面明显优于强基线模型，尤其是在处理归纳实体方面。</p><span id="more"></span><h2><span id="介绍">介绍</span></h2><p>基于编码器-解码器的神经机器翻译（NMT）因其分布式表示与端到端的学习而成为一种最新、最先进的方法。</p><p>在翻译过程中，句子中的实体起着重要的作用，它们的正确翻译会很大程度上影响整个句子的翻译质量。因此，优于实体的重要性，人们提出了各种方法以改善翻译。其中，有一种方法旨在结合知识图谱（KG）来改善实体翻译。</p><p>在许多语言和领域中，人们构建各种大规模KG来组织实体的结构化知识。同时，一些研究将KG纳入NMT，以增强句对数据集中的实体的语义表示并改进翻译。然而，这些研究只关注同时出现在 KG和训练句对数据集中的实体（我们将这种实体称为K+D实体）。实际上，除了这些K+D实体之外，KG还包含许多未出现在训练句对数据集中的实体（我们将这种实体称为K-D实体）。而这些K-D实体在之前的研究中都被忽略了。</p><p>在本文中，我们认为这些K-D实体严重损害了翻译质量，而知识图谱可以缓解这一问题。图1显示了一个例子，假设可以从汉英平行句中提取两个翻译对，即“asipilin-aspirin”和”yaopin-drug”。同时，源实体”yixianshuiyangsuan”是一个K-D实体，没有出现在平行句对中。虽然我们可以归纳这个实体被翻译成“aspirin”，但这是从源三元组“（asipilin,alias,yixianshuiyangsuan）”表明，”yixianshuiyangsuan”是”asipilin”的别称而得来的。</p><p><span><img class="small-img-inline" src="/2021/06/25/Knowledge%20Graphs%20Enhanced%20Neural%20Machine%20Translation/clip_image003.png" alt></span></p><p>图1 表明非平行KG也可归纳K-D实体的翻译结果的示例。在示例中，可以提取两个翻译对：“asipilin-aspirin”和”yaopin-drug”（如红色虚线所示）。虽然实体”yixianshuiyangsuan”是一个K-D实体，但它可能被翻译成”asipilin”，因为源三元组“（asipilin,alias,yixianshuiyangsuan）”表明，”yixianshuiyangsuan”是”asipilin”的别称。</p><p>因此，本文中，我们提出了一个有效的方法，将非平行的源和目标KG纳入NMT系统。在KG的帮助下，所提出的方法可以使NMT学习包含K-D实体的新实体翻译对。更具体而言，方法包含三个步骤。1）双语K-D实体归纳：该步骤中，我们首先从短语翻译表中提取种子对。然后，我们通过最小化种子对中源实体和目标实体之间的距离，将源和目标 KG 转换为统一的语义空间。最后我们在这个语义空间下归纳出K-D实体的翻译结果。2）伪平行句对生成：我们生成足够的、包含归纳出的实体对的伪平行句对。3）联合训练：在这一步我们通过原始句和伪句对联合训练NMT模型，使NMT能够学习归纳翻译对中源实体和目标实体之间的映射。汉英和英日翻译任务的大量实验表明，我们的方法在翻译质量上明显优于强基线模型，特别是在处理归纳K-D实体方面。</p><p>我们主要有如下贡献：</p><p>* 我们提出一种方法，将非平行的KG纳入NMT模型。</p><p>* 我们设计了一个新颖的方法，用KG归纳K-D实体翻译的结果，生成伪平行句对，促进NMT对K-D实体做出更好的预测。</p><h2><span id="问题定义">问题定义</span></h2><p>本文中，我们使用以下三种数据资源来训练一个NMT模型θ。</p><p>\1) <strong>平行句对</strong>D&#x3D;{(X, Y)}，其中X代表源句。Y表示目标句。</p><p>\2) <strong>原KG</strong> <span><img class="small-img-inline" src="/2021/06/25/Knowledge%20Graphs%20Enhanced%20Neural%20Machine%20Translation/clip_image004.png" alt></span>，其中hs，ts和rs分别代表头实体，尾实体和源语言的关系。</p><p>3）<strong>目标KG</strong> <span><img class="small-img-inline" src="/2021/06/25/Knowledge%20Graphs%20Enhanced%20Neural%20Machine%20Translation/clip_image005.png" alt></span>，其中ht，tt和rt分别代表头实体，尾实体和目标语言的关系。</p><p>由于平行的KG很难获得，本位置，KGs和KGt不是平行的，同时，我们假设KGs和KGt包含许多未出现在平行句对D的实体，我们称这些实体为K-D实体。K-D实体集O可形式上定义为：</p><p><span><img class="small-img-inline" src="/2021/06/25/Knowledge%20Graphs%20Enhanced%20Neural%20Machine%20Translation/clip_image006.png" alt></span></p><p>其中Oes和Oet分别代表K-D源实体和目标实体。</p><p>我们认为，尽管句对D可能不包含这些K-D实体的翻译知识，但KG可以帮助归纳它们的翻译结果。因此，我们在本文中的目标是在KGs和KGt的帮助下提高这些K-D实体的翻译质量。</p><p><strong>方法描述</strong></p><p>图2展示了我们提出的方法的框架，其包括三步：1）双语K-D实体归纳，2）伪句对生成，3）联合训练。接下来我们将在下面小节中介绍每个步骤。</p><p><strong>双语K-D实体归纳</strong></p><p>本步中，我们希望能归纳出K-D实体的翻译结果。为了实现该目标，我们的主要思想是将源和目标KG转化为一个统一的语义空间，然后在该语义空间下归纳这些实体的翻译结果。</p><p>具体而言，算法1展示了我们的双语K-D实体归纳方法，该方法首先需要做四个准备工作（第1-4行）。我们首先将KGs和KGt分别表示为实体嵌入<span><img class="small-img-inline" src="/2021/06/25/Knowledge%20Graphs%20Enhanced%20Neural%20Machine%20Translation/clip_image010.png" alt></span>是翻译概率（第3行）。最后的准备工作是通过公式（3）提取K-D实体集O。在图2的例子中，有三个K-D实体”yixianshuiyangsuan”、”purexintong”和”paracetamol”。其中前两个是K-D源实体，最后一个是K-D目标实体。</p><p>有了上述准备，我们现在需要构建种子对集S（第5-8行）。如果有一个短语翻译对<span><img class="small-img-inline" src="/2021/06/25/Knowledge%20Graphs%20Enhanced%20Neural%20Machine%20Translation/clip_image012.png" alt></span>中。在图2的例子中，两个短语对“（yaopin,drug,0.4）”和“（asipilin,aspirin,0,9）”被选入种子对中。</p><p><span><img class="small-img-inline" src="/2021/06/25/Knowledge%20Graphs%20Enhanced%20Neural%20Machine%20Translation/clip_image014.jpg" alt></span></p><p>图2 所提出的将非平行KG纳入NMT的方法。</p><p>得到的<span><img class="small-img-inline" src="/2021/06/25/Knowledge%20Graphs%20Enhanced%20Neural%20Machine%20Translation/clip_image019.png" alt></span>。如果一个种子对的概率较大，这个种子对在损失函数中的权重也较大。因此，损失函数可以定义为公式（4）（第9行）。</p><p><span><img class="small-img-inline" src="/2021/06/25/Knowledge%20Graphs%20Enhanced%20Neural%20Machine%20Translation/clip_image020.png" alt></span></p><p>最后的任务是归纳K-D实体的翻译结果（第10-17行）。给定一个K-D源实体<span><img class="small-img-inline" src="file:///C:/Users/Angelo/AppData/Local/Temp/msohtmlclip1/01/clip_image022.png" alt></span>中（第16-17行）。在图2的例子中，我们归纳了两个新的配对：“（yixianshuiyangsuan,aspirin）”和”(purexintong,paracetamol)”。现在，集合</p><p><span><img class="small-img-inline" src="file:///C:/Users/Angelo/AppData/Local/Temp/msohtmlclip1/01/clip_image022.png" alt></span>包含所有新的归纳翻译对。</p><p><strong>伪句对的生成</strong></p><p>现在我们的目标是生成包含归纳实体对的句对。其主要思想是将种子对的上下文转移到与该种子对接近的归纳对。具体来说，如果一个归纳对<span><img class="small-img-inline" src="/2021/06/25/Knowledge%20Graphs%20Enhanced%20Neural%20Machine%20Translation/clip_image024.png" alt></span>之间的距离低于预定的超参数λ，如下所示：</p><p><span><img class="small-img-inline" src="/2021/06/25/Knowledge%20Graphs%20Enhanced%20Neural%20Machine%20Translation/clip_image026.jpg" alt></span></p><p>我们希望将种子对<span><img class="small-img-inline" src="/2021/06/25/Knowledge%20Graphs%20Enhanced%20Neural%20Machine%20Translation/clip_image033.png" alt></span>。</p><p>在图2的例子中，假设归纳对“（yixianshuiyangsuan,aspirin）”和“（purexitong,paracetamol）”都接近种子对“（asipilin,aspirin）”，我们用这两个归纳对替换“（asipilin,aspirin）” ，得到如图2中间部分所示的伪句对。</p><p><strong>联合训练</strong></p><p>最后的任务是用原始平行句对D和伪平行句对Dp训练NMT模型θ。我们的实验表明，伪句对Dp的数量明显少于原始句对D。为了克服这个不平衡的问题，我们首先对伪句对Dp进行n次超采样，并通过以下方式设计损失函数：</p><p><span><img class="small-img-inline" src="/2021/06/25/Knowledge%20Graphs%20Enhanced%20Neural%20Machine%20Translation/clip_image034.png" alt></span></p><p>其中，前者是来自原始数据D的损失，后者是来自超采样的伪数据Dp的损失。</p><h2><span id="试验设置">试验设置</span></h2><p>我们在汉译英（CN⇒EN）和英译日（EN⇒JA）的翻译任务中测试了所提出的方法。CN⇒EN的平行句对是从LDC语料库中提取的，该语料库包含了201万个句对。在CN⇒EN任务中，我们利用了三个不同的KG：i) 医学KG，其中源KG包含38万个三元组，目标KG包含23万个三元组，这些三元组是从YAGO中筛选出来的。我们构建了2000个医疗句对作为开发集，2000个医疗句对作为测试集。目标KG包含28万个三元组，这些三元组也是从YAGO中过滤出来的。我们还构建了2000个关于旅游的句对作为开发集，以及2000个其他句对作为测试集。我们选择NIST03作为开发集，NIST 04-06作为测试集。使用KFTT数据集作为EN⇒JA平行句对。源和目标KG是DBP15K。训练对和KG的统计数据见表1。</p><p><span><img class="small-img-inline" src="/2021/06/25/Knowledge%20Graphs%20Enhanced%20Neural%20Machine%20Translation/clip_image036.jpg" alt></span></p><p>表1 训练数据的统计。Pair列显示了平行句对的数量，Knowledge Graph列显示了三元组的名称和数量（源&#x2F;目标）。Dev&#x2F;Test列显示了开发&#x2F;测试集的句子数量。</p><p>我们实现了基于THUMT工具包的NMT模型和基于openKE工具包的知识嵌入方法。我们使用 Transformer 模型的“基本”版本参数。在所有的翻译任务中，我们使用BPE方法来合并30000次。我们用不区分大小写的BLEU来评估最终的翻译质量。</p><p>在这种方法中，我们比较了以下 NMT 系统：</p><p>\1) RNMT：使用两个LSTM层作为编码器和解码器的基线NMT系统。</p><p>\2) Transformer：最先进的NMT系统，具有自注意力机制。</p><p>\3) Transformer+RC：这是一种通过添加句子中实体间的关系约束来合并KG的方法，其目标是在句对中获得K+D实体的更好表示。</p><p>4）Transformer&#x2F;RNMT+KG：这是我们在Transformer和RNMT的基础上提出的KG增强型NMT模型，其中我们将超参数δ（算法1）设置为0.45（医学）、0.47（旅游）、0.39（一般）和0.43（DBP15K），λ（4.2节）设置为0.86（医学）、0.82（旅游）、0.73（一般）和0.82（DBP15K）。超采样时间n（第4.3节）分别被设定为4（医疗）、3（旅游）、2（一般）和3（DBP15K）。所有这些超参数都在开发集中进行了微调。</p><h2><span id="实验结果">实验结果</span></h2><p><strong>翻译结果</strong></p><p>RNMT模型的结果。表2列出了CN⇒EN和EN⇒JA翻译任务的主要翻译结果。我们首先将我们的方法与RNMT进行比较。比较第1行和第4-6行，提出的RNMT+KG在所有测试集上都比RNMT有所提升。具体来说，当使用医疗、旅游和一般KG时，所提出的方法可以分别比RNMT多出1.29（12.54对11.25）、0.88（12.77对11.89）和0.55（41.89对41.34）BLEU点。同时，在EN⇒JA的翻译任务上，提升可以达到0.48个BLEU点（27.91对27.43）。</p><p>Transformer 模型的结果。我们在Transformer的基础上进行了实验来评估所提出的方法。如第2行和第7-9行所示，我们的方法也可以提高Transformer上的翻译质量，在这三个KG的帮助下，其改进可以分别达到1.12（15.69对14.57），0.90（14.88对13.98）和0.51（44.91对44.40）BLEU点。此外，在EN⇒JA翻译任务中，提议的Transformer+KG可以比Transformer多出0.60个BLEU点（30.10对29.50）。</p><p>不同嵌入方法的结果。我们也对利用不同的知识嵌入方法时的结果感兴趣。在这里，我们测试了以下三种知识嵌入方法。TransE、TransD和TransR。从结果（第4-9行）中，我们可以看到，在所有的任务中，这三种知识嵌入方法可以达到相似的BLEU分数。</p><p>Transformer+RC对比我们的方法。我们还将所提出的方法与Transformer+RC（第3行）进行比较。结果显示，我们提出的方法（第7-9行）比Transformer+RC分别多出0.90（15.69对14.79）、0.77（14.88对14.11）、0.08（44.91对44.83）和0.27（30.10对29.83）BLEU点。结果显示了我们所提出方法的优点。更重要的是，在Transformer+RC的基础上，我们的方法（第10行）可以进一步提高翻译质量，说明Transformer+RC仍然面临K-D实体的问题，而我们的方法可以缓解这个问题。</p><p><span><img class="small-img-inline" src="/2021/06/25/Knowledge%20Graphs%20Enhanced%20Neural%20Machine%20Translation/clip_image038.jpg" alt></span></p><p>表2 不同方法在 CN⇒EN 和 EN⇒JA 翻译任务上的 BLEU 分数。 “*”表示提出的系统在统计上比基线系统更好（p &lt; 0.05），“†”表示 p &lt; 0.01。</p><p><strong>超参数的影响</strong></p><p>在算法1中，我们设置了一个预定义的超参数δ来确定双语归纳对。表3显示了不同δ下的BLEU分数（医用KG）。我们可以看到，当δ&#x3D;0.45时，BLEU分数是最大的。当δ超过0.45时，BLEU分数（dev）从15.96下降到14.94。</p><p>同时，我们也对归纳的双语K-D实体的精确度感到好奇。因此，我们随机选择了300个不同δ下的归纳双语实体翻译对，并手动分析其正确率。结果也报告在表3中（列Precison）。从结果中我们可以看出，随着超参数δ的增加，可以归纳出更多的K-D实体翻译对，而精度则从43.1%下降到13.7%。这些结果表明，有必要在K-D实体翻译对的数量和精度之间找到平衡。</p><p><span><img class="small-img-inline" src="/2021/06/25/Knowledge%20Graphs%20Enhanced%20Neural%20Machine%20Translation/clip_image039.png" alt></span></p><p>表3：不同δ下的BLEU分数。#对显示了归纳的K-D双语实体对的数量。Precison表示归纳的K-D双语实体对的正确率。</p><p>我们设置了一个预先定义的超参数λ来生成伪句对。图3显示了结果（医学KG），其中x轴表示超参数λ，y轴表示开发集和测试集的BLEU分数。图中的数字表示伪句对的数量。从结果中我们可以看出，随着超参数λ的增加，可以生成更多的伪句对。当λ&#x3D;0.86时，BLEU分数（dev）变得最大。我们认为原因在于，当λ变得过大时，伪句对可能包含更多的噪声，从而损害最终的翻译质量。</p><p><span><img class="small-img-inline" src="/2021/06/25/Knowledge%20Graphs%20Enhanced%20Neural%20Machine%20Translation/clip_image041.jpg" alt></span></p><p>图3 超参数λ的影响，其中x轴表示超参数λ，y轴表示开发集和测试集的BLEU分数。当λ&#x3D;0.86时，开发集和测试集的BLEU分数最大。</p><p><strong>K-D实体的分析</strong></p><p>本文中，我们的目标是用KG增强NMT的K-D实体。因此，我们也分析了所提方法在K-D实体上的结果。分析是在句子层面和实体层面进行的。</p><p>在句子层面的分析上，我们将测试句子分为两个不同的部分：i)带有K-D实体的句子（send w K-D）和ii)不带K-D实体的句子（send w&#x2F;o K-D）。表4报告了实验结果。从结果中，我们可以看到，我们提出的方法对没有K-D实体的句子影响不大。但它可以将带有 K-D 实体的句子分别从 9.96 显著提高到 11.75 BLEU 点（RNMT）和从 13.46 显著提高到15.15 BLEU 点（Transformer）。结果表明，我们提出的方法可以在含有K-D实体的句子上产生更好的翻译结果。</p><p>我们还分析了在单词层面上归纳K-D实体的结果。具体来说，我们随机选取了300个句子（医疗KG），其中包含162个K-D实体（267次）和72个K+D实体（96次）。我们计算以下三个值：1）归纳的K-D实体的正确率（次）；2）未归纳的K-D实体的正确率（次）；3）K+D实体的正确率（次）。统计结果见表5。从结果可以看出，Transformer+RC可以将K+D实体的翻译正确率（次）从36.5%（35）提高到43.8%（42）。而我们的方法对归纳K-D实体最为有效，它将翻译正确率（次）从21.5%（31）提高到31.3%（45）。更重要的是，当结合Transformer+RC和我们的方法时，RC+Ours都能将归纳的K-D和K+D分别提高到31.9%（46）和46.9%（45），这说明我们的方法和Transformer+RC是互补的。</p><p><span><img class="small-img-inline" src="/2021/06/25/Knowledge%20Graphs%20Enhanced%20Neural%20Machine%20Translation/clip_image042.png" alt></span></p><p>表5 K-D和K+D实体的正确率（次）。</p><p>图4显示了我们所提出的方法可以改善归纳K-D实体的翻译。在这个例子中，涉及到的K-D实体 “yixianshuiyangsuan”被Transformer完全翻译成错误的目标短语。而提出的Transformer+KG可以克服这个错误，产生正确的翻译结果 “asipirin”。</p><p><span><img class="small-img-inline" src="/2021/06/25/Knowledge%20Graphs%20Enhanced%20Neural%20Machine%20Translation/clip_image043.png" alt></span></p><p>图4表明所提出的方法可以改进归纳K-D实体的例子。</p><h2><span id="结论">结论</span></h2><p>为了解决NMT中的K-D实体，我们提出了一种知识图谱增强的NMT方法。我们首先通过利用非平行KG归纳K-D实体的翻译结果，然后生成伪平行句对，最后联合训练NMT模型。在汉译英和英译日任务上的大量实验表明，我们的方法在翻译质量上明显优于基线模型，特别是在处理归纳K-D实体方面。</p>]]></content>
    
    
    <summary type="html">&lt;img src=&quot;/2021/06/25/Knowledge%20Graphs%20Enhanced%20Neural%20Machine%20Translation/clip_image002.jpg&quot; alt&gt;

&lt;h1 id=&quot;知识图谱加强神经机器翻译&quot;&gt;&lt;a href=&quot;#知识图谱加强神经机器翻译&quot; class=&quot;headerlink&quot; title=&quot;知识图谱加强神经机器翻译&quot;&gt;&lt;/a&gt;知识图谱加强神经机器翻译&lt;/h1&gt;&lt;h2 id=&quot;引用&quot;&gt;&lt;a href=&quot;#引用&quot; class=&quot;headerlink&quot; title=&quot;引用&quot;&gt;&lt;/a&gt;引用&lt;/h2&gt;&lt;p&gt;Zhao Y , Zhang J , Zhou Y , et al. Knowledge Graphs Enhanced Neural Machine Translation[C]&amp;#x2F;&amp;#x2F; Twenty-Ninth International Joint Conference on Artificial Intelligence and Seventeenth Pacific Rim International Conference on Artificial Intelligence {IJCAI-PRICAI-20. 2020.&lt;/p&gt;
&lt;h2 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要&quot;&gt;&lt;/a&gt;摘要&lt;/h2&gt;&lt;p&gt;知识图谱（Knowledge graphs, KG）存储了大量关于各种实体的结构化信息，其中许多是神经机器翻译（neural machine translation, NMT）的平行句对所未覆盖的。为提高这些实体的翻译质量，在本文中我们提出了一种新颖的知识图谱加强神经机器翻译方法。具体来说，我们首先通过将源知识图谱和目标知识图谱转换为统一的语义空间，以归纳这些实体的新翻译结果。然后我们生成足够的伪平行句对，其中包含这些归纳实体对。最后，NMT模型由原始句和伪句对联合训练。对汉英和英日翻译任务的大量实验表明，我们的方法在翻译质量方面明显优于强基线模型，尤其是在处理归纳实体方面。&lt;/p&gt;</summary>
    
    
    
    <category term="论文阅读" scheme="https://values.keys.moe/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="论文阅读" scheme="https://values.keys.moe/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
  </entry>
  
  <entry>
    <title>Understanding Recurrent Neural Networks Using Nonequilibrium Response Theory</title>
    <link href="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/"/>
    <id>https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/</id>
    <published>2021-06-24T02:27:01.000Z</published>
    <updated>2023-10-22T11:36:22.377Z</updated>
    
    <content type="html"><![CDATA[<img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image002.jpg"><h1><span id="使用非平衡响应理论理解循环神经网络">使用非平衡响应理论理解循环神经网络</span></h1><h1><span id="引用">引用</span></h1><p>Lim S H . Understanding Recurrent Neural Networks Using Nonequilibrium Response Theory[J]. 2020.</p><h2><span id="摘要">摘要</span></h2><p>循环神经网络（RNN）是一种受大脑启发的模型，其广泛的应用于机器学习，以进行连续数据的分析。本工作有助于使用非平衡学说的响应理论更深度地理解RNN如何处理输入信号。对于一类由输入信号驱动的连续时间随机RNN（SRNN），我们为其输出推导出一个沃尔泰拉级数的序列表示。这种表示法是可解释的，并将输入信号从SRNN结构中分离出来。序列的核是一些递归定义的相关函数，其与完全决定输出的无扰动动力学相关。利用这种表示的联系及其对粗糙路径理论的影响，我们确定了一个通用特征——响应特征，其被证明是输入信号的张量积的特征与自然支撑基础。特别地，我们展示了仅优化了读出层的权重，而隐藏层的权重保持固定、未被优化的SRNN，这可被看作是在与响应特征相关的再生核希尔伯特空间中执行的核机器。</p><span id="more"></span><h2><span id="介绍">介绍</span></h2><p>从时间序列分析到自然语言处理，序列化数据出现在广泛的场景中。在没有数学模型的情况下，从数据中提取有用信息，以学习一个数据生成系统是很重要的。</p><p>循环神经网络（RNN）是一类受大脑启发的模型，其专门为学习序列数据而设计，被广泛地应用于从物理学到金融的各个领域。RNN是具有反馈连接的神经元网络，从生物学角度比其他适应性模型更具说服力。特别地，RNN可以使用它们的隐藏状态（记忆）来处理输入的可变长度序列。它们是动力系统的通用逼近器，且其本身可被视为一类开放动力系统。</p><p>尽管RNN近期在储备池计算、深度学习和神经生物学方面取得了创新和巨大的经验成功，但很少有研究关注RNN工作机制的理论基础。缺乏严格的分析限制了RNN在解决科学问题方面的实用性，并可能阻碍下一代网络的系统设计。因此，深入了解该机制对于阐明大型自适应架构的特性，以及彻底改变我们对这些系统的理解而言至关重要。</p><p>特别地，人们可能会问的两个自然且基础的问题是：</p><p>Q1：随着时间推移的输入信号如何驱动RNN产生输出？</p><p>Q2：它们的响应是否有一个普遍的机制？</p><p>本工作的主要目标之一是解决上述问题，以非平衡统计动力学中的非线性响应理论为出发点，针对连续时间RNN的随机版本，简称SRNN（其隐藏状态被注入了高斯白噪声）进行分析。我们的方法是跨学科的，为现有的RNN理论增加了令人耳目一新的观点。</p><h2><span id="随机循环神经网络srnn">随机循环神经网络（SRNN）</span></h2><p>本文固定过滤概率空间（filtered probability space）<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image004.jpg"> </span>，E代表对P的期望，T&gt;0。C(E, F)代表从E到F的连续映射的巴拿赫空间，其中E和F是巴拿赫空间。<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image005.png"> </span>表示Rn上所有有界连续函数的空间。N:&#x3D;{0, 1, 2, . . . }，Z+:&#x3D;{1, 2, . . . }且R+:&#x3D; [0, ∞)。上标T表示转置，∗表示邻接。</p><h3><span id="模型">模型</span></h3><p>我们对我们的SRNN考虑如下模型。所谓激活函数，是指一个非常数的、利普希茨连续且有界的实值函数。激活函数的例子包括sigmoid函数，如实践中常使用的双曲切线等。</p><p><strong>定义2.1</strong>（连续时间SRNN）令t ∈ [0, T]，<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image007.jpg"> </span>为确定的输入信号。连续时间的SRNN描述为以下空间状态的模型：</p><p><span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image009.jpg"> </span></p><p>其中，公式1是隐藏状态<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image010.png"> </span>的随机微分方程（SDE），带有漂移系数φ：<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image011.png"> </span>、噪声系数<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image012.png"> </span>和定义在<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image014.jpg"> </span>上的r维维纳过程<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image015.png"> </span>，而公式2定义了一个可观测的激活函数<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image016.png"> </span>。</p><p>我们考虑SRNN的输入仿射版本，其中：</p><p><span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image018.jpg"> </span></p><p>其中，<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image019.png"> </span>是正稳定的，<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image020.png"> </span>为激活函数，<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image021.png"> </span>和<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image022.png"> </span>为常量，<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image023.png"> </span>为转换输入信号的常量矩阵。</p><p>从现在开始，我们将 SRNN 称为由（1）-（3）定义的系统。SRNN的隐藏状态描述了一个处理输入信号的非自主随机动力系统。常数Γ、W、b、C、σ和f中的参数（如果有的话）定义了SRNN（架构）的（可学习）参数或权重。对于 T &gt; 0，与 SRNN 相关联的是输出函数 <span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image024.png"> </span>，其定义为可观测的f的期望值（集合平均值）：</p><p><span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image025.png"> </span></p><h3><span id="srnn的非平衡响应理论">SRNN的非平衡响应理论</span></h3><p><strong>预备知识和符号</strong></p><p>在本小节中，我们简要回顾马尔可夫过程的预备知识并介绍我们的一些符号。</p><p>令t ∈ [0, T]，<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image026.png"> </span>且<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image027.png"> </span>是归一化的输入信号。在SRNN（1）-（3）中，我们认为信号<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image028.png"> </span>是驱动SDE的小振幅γ（t）的扰动：</p><p><span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image030.jpg"> </span></p><p>未扰动的SDE是Cu设置为零的系统：</p><p><span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image032.jpg"> </span></p><p>其中，<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image034.jpg"> </span>且<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image036.jpg"> </span>。过程 h 是时间齐次马尔可夫过程<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image037.png"> </span>的扰动，它不一定是稳定的。</p><p>扩散过程h和<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image038.png"> </span>分别与一族无穷小生成元<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image039.png"> </span>和<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image040.png"> </span>相关，它们是二阶椭圆算子，定义为：</p><p><span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image041.png"> </span></p><p>对于任何可观察的<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image042.png"> </span>，其中<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image043.png"> </span>。我们将与 h 关联的转移算子<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image044.png"> </span>定义为：</p><p><span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image045.png"> </span></p><p>对于<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image042.png"> </span>，和转移算子<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image044.png"> </span>（其为一个马尔科夫半群），它们都是与<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image046.png"> </span>相关联的。</p><p>此外，可以在概率测度空间上定义上述生成元和转移算子的L2伴随矩阵。我们分别用<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image047.png"> </span>和<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image048.png"> </span>表示与h和<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image049.png"> </span>关联的伴随生成器，分别用<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image050.png"> </span>和<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image051.png"> </span>表示与h和<span><img class="small-img-inline" src="file:///C:/Users/Angelo/AppData/Local/Temp/msohtmlclip1/01/clip_image049.png"> </span>关联的伴随转移算子。我们假设初始测度和过程定律具有关于勒贝格测度的密度。将初始密度表示为<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image052.png"> </span>，<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image053.png"> </span>满足与<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image054.png"> </span>关联的前向柯尔莫果洛夫方程（FKE）。</p><p>我们采取自然的假设，即扰动和未扰动过程都有相同的初始分布<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image055.png"> </span>，这通常不是无扰动动力学的不变分布<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image056.png"> </span>。</p><p><strong>关键思想和形式推导</strong></p><p>首先，我们将推导出SRNN的输出函数在驱动输入信号方面的表示。我们的方法源于非平衡统计动力学的响应理论。在下文中，我们假设任何无限级数都是明确定义的，且求和和积分之间的任何互换都是合理的。</p><p>固定一个T&gt;0，令<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image057.png"> </span>足够小并且</p><p><span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image058.png"> </span></p><p>首先，请注意概率密度<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image059.png"> </span>的FKE是：</p><p><span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image060.png"> </span></p><p>其中<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image061.png"> </span>，而：</p><p><span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image062.png"> </span></p><p><span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image063.png"> </span></p><p>关键思想是，由于ε&gt; 0很小，我们寻求形式为ρ的微扰展开：</p><p><span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image064.png"> </span></p><p>将其代入FKE并匹配ε中的阶数，我们得到以下方程层次：</p><p><span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image065.png"> </span></p><p>ρn的形式解可以通过迭代获得。形式化的描述，我们记<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image066.png"> </span>。在不变分布是稳定的特殊情况下，<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image067.png"> </span>与时间无关。</p><p>请注意，n ≥ 2时，<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image068.png"> </span>，在n ≥ 2时，解ρn通过递归关系而得：</p><p><span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image069.png"> </span></p><p>因此，假设下面的无穷级数绝对收敛，我们有：</p><p><span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image070.png"> </span></p><p>接下来，我们考虑SRNN的隐性动力学的标量值观测值<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image071.png"> </span>，并研究输入信号扰动引起的该观测值的平均偏差：</p><p><span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image073.jpg"> </span></p><p>对于扰动动力学的可观察值的平均值可写为：</p><p><span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image074.png"> </span></p><p>在不丧失一般性的情况下，我们在下文中取<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image076.jpg"> </span>，即f(h)被认为是均值为零的（相对于ρinit）。</p><p>我们有：</p><p><span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image078.jpg"> </span></p><p>其中</p><p><span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image080.jpg"> </span></p><p>是一阶响应核，它们是相对于 ρinit 的仅无扰动动力学函数的平均值。请注意，为了获得上面的最后一行，我们分部积分并假设ρinit&gt;0。</p><p>该式表达了阿加瓦尔型的非平衡波动-耗散关系。在平稳不变分布的情况下，我们使用（向量值）响应核，恢复统计力学中众所周知的平衡波动-耗散关系：</p><p><span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image081.png"> </span></p><p>其中<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image082.png"> </span>。在线性 SRNN（即φ(h, t)在h中是线性的）和f(h) &#x3D; h的特殊情况下，其可简化为<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image083.png"> </span>的协方差函数（相对于ρ∞）。</p><p>到目前为止，我们已经研究了线性响应机制，其中，响应线性地依赖于输入。现在我们通过将上述推导扩展到n≥2的情况。我们表示<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image084.png"> </span>，可得</p><p><span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image086.jpg"> </span></p><p>其中<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image087.png"> </span>，<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image088.png"> </span>是n阶响应核：</p><p><span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image090.jpg"> </span></p><p>且</p><p><span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image091.png"> </span></p><p>n &#x3D; 2, 3, . . .时，</p><p><span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image093.jpg"> </span></p><p>请注意，这些高阶响应核与一阶响应核类似，是相对于 ρinit 的一些仅无扰动动力学的函数的平均值。</p><p>基于上述结果可得：</p><p><span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image095.jpg"> </span></p><p>其中<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image096.png"> </span>是递归定义的时间相关核。更重要的是，这些核完全由SRNN的未扰动动力学以明确的方式确定。因此，SRNN 的输出函数可以写成（实际上是唯一的）上述一系列形式。该陈述在后文中得到了精确表述，从而解决了 (Q1)。</p><p>现在我们关注(Q2)。通过展开技术，我们可以得到：</p><p><span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image098.jpg"> </span></p><p>其中，<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image099.png"> </span>是与时间和信号<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image100.png"> </span>无关的常数。该表达式以系统的方式将驱动输入信号从 SRNN 架构中分离出来。粗略地说，它告诉我们，SRNN对输入信号的响应可以通过将两部分的乘积相加得到，其中一个描述了SRNN的未扰动部分，一个是经过时间变换的输入信号的迭加积分。这一声明在后续得到了更精确的阐述，它是解决(Q2)的起点。</p><h2><span id="主要结果">主要结果</span></h2><h3><span id="假设">假设</span></h3><p>为了简单和直观，我们对SRNN使用以下相当严格的假设。 这些假设可以通过增加技术成本（我们不在这里追求）或通过计算近似结果来证明是合理的。</p><p>回想一下，我们正在处理确定性输入信号<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image101.png"> </span>。</p><p><strong>假设4.1</strong> 固定T&gt;0并让U成为<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image102.png"> </span>的开集。</p><p>(a) <span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image103.png"> </span>对所有t∈[0, T]来说都是足够小的。</p><p>(b) 在所有<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image104.png"> </span>时，<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image105.png"> </span>，并且以概率1存在一个紧集K⊂U，使得在所有<span><img class="small-img-inline" src="file:///C:/Users/Angelo/AppData/Local/Temp/msohtmlclip1/01/clip_image104.png"> </span>情况下，<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image106.png"> </span>。</p><p>(c) 系数a：<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image107.png"> </span>和f：<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image108.png"> </span>为分析函数。</p><p>(d) <span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image109.png"> </span>是正定的，<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image110.png"> </span>是正稳定的（即，Γ 的所有特征值的实部都是正的）。</p><p>(e) 初始状态<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image111.png"> </span>是一个根据概率密度ρinit分布的随机变量。</p><p>假设4.1(a)意味着我们使用幅度足够小的输入信号。这对于确保某些无穷级数以足够大的收敛半径绝对收敛非常重要。(b) 和 (c) 确保一些理想的规律性和有界性。特别地，它们意味着a、f和它们所有的偏导数都是有界的，且在整个t∈[0, T]上，ht和<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image112.png"> </span>利普希茨连续。(d) 意味着系统受到的是非退化噪声的抑制和驱动，这确保了无扰动系统可以指数稳定。(e)是我们分析的自然假设，因为h是<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image113.png"> </span>的一个扰动。</p><p>除非另有说明，否则假设4.1是本文中隐含的假设。</p><p>进一步符号化。我们现在提供一个空间及其符号的列表：</p><p>* L(E1, E2)：从E1到E2的有界线性算子的巴拿赫空间（其中||·||表示适当空间上的范数）</p><p>* <span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image114.png"> </span>：具有紧支撑的类<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image115.png"> </span>的实值函数空间</p><p>* <span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image116.png"> </span>：类<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image117.png"> </span>有界实值函数空间</p><p>* <span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image118.png"> </span>：<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image119.png"> </span>上有界绝对连续度量的空间，其中<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image121.jpg"> </span>，ρ表示度量µ的密度</p><p>* <span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image123.jpg"> </span>：ρ加权的Lp空间，即函数f的空间，使得<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image124.png"> </span>，其中ρ是加权函数。</p><h3><span id="srnn-输出泛函的表示方法">SRNN 输出泛函的表示方法</span></h3><p>在保证不丧失一般性的情况下，我们将在下文取p&#x3D;1并假设<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image125.png"> </span>。</p><p><strong>定义4.1</strong> （响应函数） 令<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image126.png"> </span>是一个有界的可观察对象。对于t∈[0,T]，令Ft是C([0, t],R)上的泛函，定义为<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image127.png"> </span>，<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image129.jpg"> </span>表示Ft相对于γ的n阶泛函导数。对于n∈Z+，如果存在局部可积函数<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image130.png"> </span>，对于所有测试函数<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image131.png"> </span>，使得</p><p><span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image132.png"> </span></p><p>则<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image133.png"> </span>被称为可观测f的n阶响应函数。</p><p>接下来，在t∈[0,T]中，令<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image135.jpg"> </span>是任意可观察函数，且<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image137.jpg"> </span>。</p><p><strong>命题4.1</strong> （响应函数的显式表达式） 对于n∈Z+，令<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image138.png"> </span>为f的n阶响应函数。那么，对于<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image139.png"> </span>：</p><p>(a) <span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image141.jpg"> </span></p><p>(b) （高阶A-FDT）此外，如果 ρinit 为正，则</p><p><span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image142.png"> </span></p><p>其中</p><p><span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image144.jpg"> </span></p><p><strong>推论4.1</strong> 令n∈Z+，且<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image145.png"> </span>。假定在<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image146.png"> </span>上有另一个函数<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image147.png"> </span>，使得对于所有的<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image148.png"> </span>，有</p><p><span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image150.jpg"> </span></p><p>那么<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image151.png"> </span>几乎处处成立。</p><p><strong>定理4.1</strong> （记忆表示） 令t∈[0,T]，SRNN的输出泛函<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image152.png"> </span>是N→∞的极限：</p><p><span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image154.jpg"> </span></p><p>其中<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image155.png"> </span>在命题4.1中给出。该极限存在，且是唯一的收敛的沃尔泰拉级数。如果Gt是另一个具有响应函数<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image156.png"> </span>的这样的级数，那么Ft&#x3D;Gt。</p><p><strong>定理4.2</strong> （无记忆表示） 假设算子<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image157.png"> </span>有一个明确定义的本征函数展开。那么，SRNN的输出函数<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image158.png"> </span>有一个收敛级数展开，这就是N, M→∞的极限：</p><p><span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image160.jpg"> </span></p><p>其中<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image161.png"> </span>是常数系数，取决于pi、li、<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image163.jpg"> </span>的特征值和特征函数、f和ρinit，但与输入信号和时间无关。在这里，pi∈{0, 1, . . . , M}、li∈{1, 2, . . . , m}。</p><p><strong>命题4.2</strong> （确定的深度SRNN的表示） 令Ft和Gt是两个SRNN的输出函数，相关的截断沃尔泰拉级数分别具有响应核<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image164.png"> </span>核<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image165.png"> </span>，n&#x3D;1,…,N，m&#x3D;1,…,M。那么<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image166.png"> </span>是具有N+M个响应核的截断沃尔泰拉级数：</p><p><span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image168.jpg"> </span></p><p>当且仅当r&#x3D;1,…,N+M，其中</p><p><span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image170.jpg"> </span></p><p>如果Ft和Gt是沃尔泰拉级数（即N，M&#x3D;∞），则在r &#x3D; 1, 2, . . . 上，<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image171.png"> </span>是具有上述响应核<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image172.png"> </span>的沃尔泰拉级数（只要它是明确定义的）。</p><p>此外，定理4.2中的陈述适用于<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image173.png"> </span>，即<span><img class="small-img-inline" src="file:///C:/Users/Angelo/AppData/Local/Temp/msohtmlclip1/01/clip_image173.png"> </span>在定理4.2的假设下允许指定形式的收敛级数展开。</p><p><strong>定义4.2</strong> （路径特征） 令X∈C([0, T], E)为有界变差路径。X的特征是T((E))的元素S，定义为</p><p><span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image174.png"> </span></p><p>其中</p><p><span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image175.png"> </span></p><p>当且仅当n ∈ Z+，<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image176.png"> </span>。</p><p>令<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image177.png"> </span>为<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image178.png"> </span>的典范基，那么我们有：</p><p><span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image180.jpg"> </span></p><p>用<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image181.png"> </span>表示对偶配对，有</p><p><span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image182.png"> </span></p><p><strong>定理4.3</strong> （特征方面的无记忆表示） 设p是一个正整数，并假设输入信号u是一个有界变差路径。那么SRNN的输出函数Ft是<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image183.png"> </span>在p→∞的极限，其是路径特征的线性泛函，<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image184.png"> </span>（可通过向量化与<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image185.png"> </span>进行识别），其中<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image186.png"> </span>，即</p><p><span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image187.png"> </span></p><p>其中，bn(t)仅取决于t的系数。</p><h3><span id="将srnn表述为核机器">将SRNN表述为核机器</span></h3><p>我们现在考虑一个监督学习（回归或分类）的环境，我们给定N个训练输入输出对<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image188.png"> </span>，其中un∈χ，为<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image189.png"> </span>中有界变差的路径空间，yn∈R，使得对于所有n，有<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image190.png"> </span>，这里FT是一个连续目标映射。</p><p>考虑优化问题：</p><p><span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image191.png"> </span></p><p>其中G是具有范数<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image192.png"> </span>的假设（巴拿赫）空间，<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image194.jpg"> </span>为一个损失函数，R(x)是一个在x中严格增加的实值函数。</p><p>受定理4.3的启发（将G视为由SRNN引入的假设空间）我们将表明，该问题的解决方案可以表示为对训练样本的核扩展。</p><p>在下文中，考虑希尔伯特空间：</p><p><span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image195.png"> </span></p><p>其中P是适当加权的<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image196.png"> </span>序列空间，其遵循序列形式为<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image197.png"> </span>，其中</p><p>Pn(t)是[0, T]上的正交多项式。令<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image198.png"> </span>表示H上的对称福克空间，<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image199.png"> </span>表示L∈Z+时<span><img class="small-img-inline" src="file:///C:/Users/Angelo/AppData/Local/Temp/msohtmlclip1/01/clip_image198.png"> </span>的L折张量积。</p><p><strong>命题4.3</strong> 令L∈Z+。考虑映射<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image200.png"> </span>，定义为：</p><p><span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image201.png"> </span></p><p>其中K是H上的核，存在一个唯一的RKHS，表示为具有范数<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image202.png"> </span>的<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image203.png"> </span>，其中K为再生核。</p><p><strong>定理4.4</strong> （表示定理） 考虑时间增加的路径<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image204.png"> </span>，其中un是χ中<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image205.png"> </span>值的输入路径，v是P中<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image206.png"> </span>值向量。那么：</p><p>(a) 假设空间为<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image207.png"> </span>的前文所述优化问题的任何解都允许以下形式的表示：</p><p><span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image208.png"> </span></p><p>其中cn∈R，N是训练输入-输出对的数量。</p><p>(b) 令L ∈ Z+。如果我们转而考虑路径，表示为<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image209.png"> </span>，在时间ti∈[0, T]上，通过对L+1个数据点进行线性插值获得<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image210.png"> </span>，则相应优化问题的任何解都具有<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image211.png"> </span>的假设空间，表示形式为：</p><p><span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image212.png"> </span></p><p>其中αn∈R，l&#x3D;1,…,L时，<span><img class="small-img-inline" src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image213.png"> </span>。</p><h2><span id="结论">结论</span></h2><p>在本文中，我们使用非平衡统计动力学的非线性响应理论作为起点，解决了关于一类随机循环神经网络 (SRNN) 的两个基本问题，这些网络可以是人工或生物网络的模型。特别地，我们能够以系统的、逐级的方式来描述SRNN对扰动的确定性输入信号的响应，为这些SRNN的输出函数推导出两种类型的序列表示，以及在驱动输入信号方面的深度变体。这提供了对由这些驱动网络所引起的记忆和无记忆表示的性质的探究。此外，通过将这些表示与路径特征的概念联系起来，我们发现响应特征集是 SRNN 在处理输入信号时从中提取信息的构建块，揭示了SRNN运行的普遍机制。特别地，我们通过表示定理表明，SRNN可以被看作是在与响应特征相关的再生核希尔伯特空间上运行的核机器。</p><p>从数学的角度来看，放宽这里的假设，并在驱动输入信号是粗略路径的一般设置中工作会很有趣，输入信号的规律性可能会发挥重要作用。人们还可以通过采用此处开发的技术来研究 SRNN 如何响应输入信号和噪声驱动（正则化）中的扰动。到目前为止，我们一直专注于介绍中提到的“公式化优先”方法。这里获得的结果表明，可以通过设计有效的算法来利用离散化响应特征和相关特征在涉及时间数据的机器学习任务中的使用，来研究”离散化的下一步”，例如在科学与工程中预测由复杂动力系统产生的时间序列。</p>]]></content>
    
    
    <summary type="html">&lt;img src=&quot;/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image002.jpg&quot;&gt;

&lt;h1 id=&quot;使用非平衡响应理论理解循环神经网络&quot;&gt;&lt;a href=&quot;#使用非平衡响应理论理解循环神经网络&quot; class=&quot;headerlink&quot; title=&quot;使用非平衡响应理论理解循环神经网络&quot;&gt;&lt;/a&gt;使用非平衡响应理论理解循环神经网络&lt;/h1&gt;&lt;h1 id=&quot;引用&quot;&gt;&lt;a href=&quot;#引用&quot; class=&quot;headerlink&quot; title=&quot;引用&quot;&gt;&lt;/a&gt;引用&lt;/h1&gt;&lt;p&gt;Lim S H . Understanding Recurrent Neural Networks Using Nonequilibrium Response Theory[J]. 2020.&lt;/p&gt;
&lt;h2 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要&quot;&gt;&lt;/a&gt;摘要&lt;/h2&gt;&lt;p&gt;循环神经网络（RNN）是一种受大脑启发的模型，其广泛的应用于机器学习，以进行连续数据的分析。本工作有助于使用非平衡学说的响应理论更深度地理解RNN如何处理输入信号。对于一类由输入信号驱动的连续时间随机RNN（SRNN），我们为其输出推导出一个沃尔泰拉级数的序列表示。这种表示法是可解释的，并将输入信号从SRNN结构中分离出来。序列的核是一些递归定义的相关函数，其与完全决定输出的无扰动动力学相关。利用这种表示的联系及其对粗糙路径理论的影响，我们确定了一个通用特征——响应特征，其被证明是输入信号的张量积的特征与自然支撑基础。特别地，我们展示了仅优化了读出层的权重，而隐藏层的权重保持固定、未被优化的SRNN，这可被看作是在与响应特征相关的再生核希尔伯特空间中执行的核机器。&lt;/p&gt;</summary>
    
    
    
    <category term="论文阅读" scheme="https://values.keys.moe/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="论文阅读" scheme="https://values.keys.moe/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
  </entry>
  
  <entry>
    <title>Attention Augmented Convolutional Networks</title>
    <link href="https://values.keys.moe/2021/06/23/Attention%20Augmented%20Convolutional%20Networks/"/>
    <id>https://values.keys.moe/2021/06/23/Attention%20Augmented%20Convolutional%20Networks/</id>
    <published>2021-06-22T17:45:45.000Z</published>
    <updated>2023-10-25T16:08:09.001Z</updated>
    
    <content type="html"><![CDATA[<img src="/2021/06/23/Attention%20Augmented%20Convolutional%20Networks/image-20231025235603474.png" alt><h1><span id="注意力增强的卷积网络">注意力增强的卷积网络</span></h1><h2><span id="引用">引用</span></h2><p>Bello I, Zoph B, Vaswani A, et al. Attention augmented convolutional networks[C]&#x2F;&#x2F;Proceedings of the IEEE&#x2F;CVF International Conference on Computer Vision. 2019: 3286-3295.</p><h2><span id="摘要">摘要</span></h2><p>卷积网络一直是许多计算机视觉应用中的首选范式。然而，卷积操作存在一个明显的弱点，即它只对局部邻域进行操作，因此缺少全局信息。另一方面，自注意力已经成为捕捉长距离相互作用的方法，但大多被应用于序列建模和生成式建模任务。本文中，我们考虑将自注意力用于判别视觉任务，将其作为卷积的替代方法。我们引入了一种新颖的二维相对自注意力机制，该机制被证明在取代卷积作为图像分类的独立计算单元方面具有竞争力。我们在控制实验中发现，把卷积和自注意力结合起来时可以得到最好的效果。因此，我们建议使用这种自注意力机制来增强卷积算子，将卷积特征图与通过自注意力产生的一组特征图连接起来。广泛的实验表明，注意力增强可在参数个数保持基本相似的情况下，使ImageNet上的图像分类和COCO上的物体检测在许多不同的模型和规模上有一致的效果提升，其中也包括了ResNets和最先进的移动受限网络。特别的，我们的方法在ImageNet分类上比ResNet50基线提高了1.3%的top-1准确率，并超过了其他图像的注意力机制，如Squeeze-and-Excitation。在COCO物体检测中，其相较RetinaNet基线上取得了1.4mAP的提升。</p><span id="more"></span><h2><span id="介绍">介绍</span></h2><p>卷积神经网络在许多计算机视觉应用中取得了巨大的成功，特别是在图像分类方面。卷积层的设计规定1）通过有限感受野实现局部性；2）通过权重共享实现转换的等变性。在设计对图像进行操作的模型时，这些特征被证明是至关重要的归纳偏置。然而，卷积核的局部性使其无法捕捉到图像中的全局背景，而这往往是更好地识别图片中物体所必需的。</p><p>另一方面，自注意力是最近出现的一种捕捉长距离相互作用的进步技术，但其大多都被应用于序列建模和生成式建模任务。自注意力的关键思想是产生一个由隐藏单元计算出的数值的加权平均值。与池化或卷积运算不同，加权平均计算中使用的权重是通过隐藏单元之间的相似度函数动态产生的。因此，输入信号之间的相互作用取决于信号本身，而非像卷积那样由它们的相对位置预先确定。特别的，这使得自注意力能够在不增加参数数量的情况下捕捉长距离的相互作用。</p><p> <span><img class="small-img-inline" src="/2021/06/23/Attention%20Augmented%20Convolutional%20Networks/image-20231025235608913.png" alt></span></p><p>图1 注意力增强系统地改善了各种不同规模的网络的图像分类。ImageNet与基线模型（ResNet）、逐通道式注意力增强模型（SE-ResNet）和我们提出的架构（AA-ResNet）的参数数量与分类准确率的比较。</p><p>本文中，我们考虑将自注意力应用于判别视觉任务中，将其作为卷积的替代方法。我们开发了一种新颖的二维相对自注意力机制，该机制在保持平移等变性的同时注入了相对位置信息，使其很适合于图像。我们的自注意力机制被证明在完全取代卷积方面具有竞争力，但我们在对照实验中发现，最好的结果是结合两者使用时获得的。因此，我们并没有完全放弃卷积的想法，而是建议用这种自注意力机制来增强卷积。这是通过将卷积特征图（其可加强局部性）与能对长范围依赖关系进行建模的自注意力特征图连接起来而实现的（参见图2）。</p><p> <span><img class="small-img-inline" src="/2021/06/23/Attention%20Augmented%20Convolutional%20Networks/image-20231025235612402.png" alt></span></p><p>图2 注意力增强卷积：对于每个空间位置 (h, w)，图像的Nh注意力图从检索和键计算而得。这些注意力图用于计算值V的Nh加权平均值。在这之后，将结果拼接起来，重构矩阵形状以匹配原始体积的空间维度，并与逐点卷积混合。多头注意力与标准卷积操作并行应用，并且输出是连接在一起的。</p><p>我们在CIFAR-100和ImageNet分类、COCO对象检测任务上测试我们的方法，其涵盖了大量不同计算力的架构，包括了最先进的资源受限架构。</p><p>注意力增强以最小额外计算代价产生了系统性的改进，并且在所有实验中明显优于流行的Squeeze-and-Excitation逐通道注意力方法。特别的，在ResNet50基线的基础上，注意力增强实现了ImageNet 1.3%的top-1准确率的提升，在RetinaNet基线的基础上实现了1.4 mAP 的 COCO 对象检测提升。令人惊讶的是，实验还表明，完全自注意力模型（注意力增强的一种特殊情况）在ImageNet上的表现只比完全卷积模型稍差，这表明自注意力是一个强大且独立的图像分类计算单元。</p><h2><span id="方法">方法</span></h2><p>我们使用以下命名约定：H、W 和 Fin 指的是激活图的输入过滤器的高度、宽度和数量。Nh、dv和dk分别表示多头注意力（MHA）中的头数、值的深度以及查询和键的深度。我们进一步假设Nh将dv和dk平均划分，并使用<span><img class="small-img-inline" src="/2021/06/23/Attention%20Augmented%20Convolutional%20Networks/image-20231025235629738.png" alt></span>表示值的深度和每个注意力头的检索&#x2F;键深度。</p><p><strong>图像上的自注意力</strong></p><p>给定形状为(H,W,Fin)的输入张量，我们将其展平为矩阵<span><img class="small-img-inline" src="/2021/06/23/Attention%20Augmented%20Convolutional%20Networks/image-20231025235642710.png" alt></span>，并按照Transformer架构中的建议使用多头注意力方法。单个头h的自注意力机制的输出可以表示为：</p><p> <span><img class="small-img-inline" src="/2021/06/23/Attention%20Augmented%20Convolutional%20Networks/image-20231025235649868.png" alt></span></p><p>其中<span><img class="small-img-inline" src="/2021/06/23/Attention%20Augmented%20Convolutional%20Networks/image-20231025235659635.png" alt></span>是所学习的线性变换，其映射输入X到检索Q&#x3D;XWq，键K&#x3D;XWk和值V&#x3D;XWv上。然后将所有头的输出连接并再次投影，如下所示：</p><p> <span><img class="small-img-inline" src="/2021/06/23/Attention%20Augmented%20Convolutional%20Networks/image-20231025235702831.png" alt></span></p><p>其中，<span><img class="small-img-inline" src="/2021/06/23/Attention%20Augmented%20Convolutional%20Networks/image-20231025235719388.png" alt></span>的内存成本，因为它需要为每个头存储注意力图。</p><p><strong>二维位置嵌入</strong></p><p>在没有关于位置的明确信息的情况下，自注意力是置换等变的：</p><p> <span><img class="small-img-inline" src="/2021/06/23/Attention%20Augmented%20Convolutional%20Networks/image-20231025235723053.png" alt></span></p><p>对于像素位置的任何排列π，其无法对高度结构化的数据（例如图像）进行建模。为了缓解相关问题，人们提出了多种位置编码，用明确的空间信息增强激活图。特别的，Image Transformer将最初的Transformer中引入正弦波，以扩展二维输入，CoordConv将位置通道连接到激活图中。</p><p>然而，这些编码对我们的图像分类和目标检测实验并无帮助。我们假设这是因为这样的位置编码虽然不是置换等变的，但其亦不满足平移等变（其是处理图像时的理想属性）。作为一种解决方案，我们建议将相对位置编码的使用扩展到二维，并提出基于Music Transformer的内存高效实现。</p><p>相对位置嵌入：相对自注意力通过相对位置嵌入增强了自注意力，并在防止置换等变的同时实现转换等变。我们通过独立添加相对高度信息和相对宽度信息来实现二维的相对自注意力。有多少像素i&#x3D;(ix,iy)关注像素j&#x3D;(jx,jy)的注意力对数计算如下：</p><p> <span><img class="small-img-inline" src="/2021/06/23/Attention%20Augmented%20Convolutional%20Networks/image-20231025235728637.png" alt></span></p><p>其中qi是像素i的检索向量（Q的第i行），kj是像素j的键向量（K的第j行），<span><img class="small-img-inline" src="/2021/06/23/Attention%20Augmented%20Convolutional%20Networks/image-20231025235748934.png" alt></span>分别是相对宽度jx−ix和相对高度jy−iy的学习嵌入。现在头h的输出变为：</p><p> <span><img class="small-img-inline" src="/2021/06/23/Attention%20Augmented%20Convolutional%20Networks/image-20231025235733687.png" alt></span></p><p>其中  是沿高度和宽度维度的相对位置对数矩阵，满足<img src="/2021/06/23/Attention%20Augmented%20Convolutional%20Networks/image-20231025235754707.png" alt="image-20231025235754707" style="zoom:67%;">。</p><p>我们将之前的研究者所提出的高效内存的相对遮罩注意力算法扩展到二维输入的无遮罩相对自注意力上。我们实现的内存占用最终为<span><img class="small-img-inline" src="/2021/06/23/Attention%20Augmented%20Convolutional%20Networks/image-20231025235812726.png" alt></span>参数来模拟沿高度和宽度的相对距离。</p><p><strong>注意力增强卷积</strong></p><p>多个先前提出的对图像的注意力机制表明，卷积算子受到其局部性和缺乏对全局背景的理解的限制。这些方法通过重新校准卷积特征图来捕捉长距离的依赖关系。与前人方法相比，我们1）使用一种注意力机制，可以共同关注空间和特征子空间（每个头对应一个特征子空间）；2）引入额外的特征图而不是细化它们。图2总结了我们提出的增强卷积。</p><p>连接卷积和注意力特征图：考虑核大小为k、输入过滤器为Fin、核输出过滤器为Fout的卷积算子。相应的注意力增强卷积可以写成：</p><p> <span><img class="small-img-inline" src="/2021/06/23/Attention%20Augmented%20Convolutional%20Networks/image-20231025235818266.png" alt></span></p><p>我们用<span><img class="small-img-inline" src="/2021/06/23/Attention%20Augmented%20Convolutional%20Networks/image-20231025235830009.png" alt></span>表示键深度与原始输出过滤器数量的比例。与卷积类似，所提出的注意力增强卷积1）与平移等变；2)易对不同空间维度的输入进行操作。</p><p>对参数数量的影响：多头注意力引入了一个带有 Fin 输入过滤器和<span><img class="small-img-inline" src="/2021/06/23/Attention%20Augmented%20Convolutional%20Networks/image-20231025235848508.png" alt></span>输入和输出过滤器，以混合不同头的贡献。考虑到卷积部分过滤器的减少，这会导致参数发生以下变化：</p><p> <span><img class="small-img-inline" src="/2021/06/23/Attention%20Augmented%20Convolutional%20Networks/image-20231025235852510.png" alt></span></p><p>其中，为了简单起见，我们忽略了相对位置嵌入所引入的参数，因为这些参数可以忽略不计。在实践中，取代3x3卷积时，会导致参数的轻微减少，而取代1x1卷积时，参数会轻微增加。有趣的是，我们在实验中发现，注意力增强的网络仍然明显优于完全卷积的对应网络，而使用的参数更少。</p><p>注意力增强卷积架构：在我们所有的实验中，增强卷积之后是一个批量归一化层，它可以学习缩放卷积特征图和注意力特征图的贡献。我们对每个残差块应用一次增强卷积，并在内存允许的情况下沿整个架构应用。</p><p>由于内存成本<span><img class="small-img-inline" src="/2021/06/23/Attention%20Augmented%20Convolutional%20Networks/image-20231025235859414.png" alt></span>对于大空间维度可能过高，我们从最后一层（具有最小空间维度）开始使用注意力增强卷积，直到我们遇到内存限制。为了减少增强网络的内存占用，我们通常采用较小的批处理规模，有时还对应用最大空间维度的层中的自注意力的输入进行下采样。下采样是通过应用步长为2的3x3平均池化来进行的，而之后的上采样（连接所需)是通过双线性插值获得的。</p><h2><span id="实验">实验</span></h2><p>我们在CIFAR100、ImageNet和COCO数据集上测试了标准计算机视觉架构，例如ResNets和MnasNet的注意力增强。我们的实验表明，注意力增强可以在广泛的体系结构和计算需求中对图像分类和对象检测任务进行系统改进。我们验证了所提出的二维相对注意力机制在消融实验中的实用性。在所有实验中，我们用自注意力特征图代替卷积特征图，因为它可以更容易地与基线模型进行比较。除非另有说明，所有结果都对应于我们的二维相对自注意力机制。</p><p><strong>CIFAR-100 图像分类</strong></p><p>我们首先使用 Wide ResNet架构研究注意力增强如何在CIFAR-100（低分辨率图像的标准基准）上执行。表1展示了注意力增强在相似的参数和复杂性成本下提高了基线网络和Squeeze-and-Excitation的性能。</p><p> <span><img class="small-img-inline" src="/2021/06/23/Attention%20Augmented%20Convolutional%20Networks/image-20231025235906130.png" alt></span></p><p>表1 使用 Wide-ResNet 28-10架构在CIFAR-100数据集上进行图像分类。</p><p><strong>使用 ResNet 进行ImageNet图像分类</strong></p><p>表 2 在 ResNet50 架构上对通道和空间注意力机制 BAM、CBAM和 GALA的注意力增强进行基准测试，通道缩减率 σ &#x3D; 16。与先前提出的注意力机制相比，注意力增强提供了具有竞争力的准确性&#x2F;计算权衡。表3比较了不同网络规模的非增强网络和 Squeeze-and-Excitation (SE)。在所有实验中，注意力增强显着提高了非增强基线的性能，并且明显优于 Squeeze-and-Excitation。值得注意的是，我们的 AA-ResNet-50 与基线 ResNet-101 的表现相当，我们的 AA-ResNet-101 优于基线 ResNet-152。这些结果表明，注意力增强比简单地把网络做得更深而言更可取。</p><p> <span><img class="small-img-inline" src="/2021/06/23/Attention%20Augmented%20Convolutional%20Networks/image-20231025235912892.png" alt></span></p><p>表2 不同注意力机制在 ImageNet 数据集上的图像分类性能。 ∆指的是在单个Tesla V100 GPU上使用Tensorflow的批处理量为128时的ResNet50相比，所增加的延迟时间。为了公平比较，我们还包含了 top-1 结果（在括号中），该结果是在ResNet50为基线、放缩网络宽度以匹配~25.6M的参数的情况下得到的。</p><p> <span><img class="small-img-inline" src="/2021/06/23/Attention%20Augmented%20Convolutional%20Networks/image-20231025235917065.png" alt></span></p><p>表3 ImageNet数据集在一系列ResNet架构上的图像分类结果：ResNet-34、ResNet-50、Resnet-101和ResNet-152。</p><p><strong>使用 MnasNet 进行 ImageNet 分类</strong></p><p>在表 4 中，我们报告了基线MnasNet及其在不同宽度倍数下的注意力增强变体的ImageNet准确率。我们的实验表明，注意力增强在所有宽度倍数下都能提高准确性。使用相对自注意力增强MnasNet会导致参数略有增加，但是我们在图3中验证了准确率的提高不仅仅可以通过参数增加来解释。此外，我们注意到MnasNet架构在通过架构搜索最佳选择的位置时采用了 Squeeze-and-Excitation，这进一步表明了我们方法的好处。</p><p> <span><img class="small-img-inline" src="/2021/06/23/Attention%20Augmented%20Convolutional%20Networks/image-20231025235924343.png" alt></span></p><p>表4 基线和注意力增强的MnasNet准确率，宽度倍数为0.75、1.0、1.25 和 1.4。</p><p> <span><img class="small-img-inline" src="/2021/06/23/Attention%20Augmented%20Convolutional%20Networks/image-20231025235927049.png" alt></span></p><p>图3 在宽度倍数为0.75、1.0、1.25和1.4的情况下，MnasNet（黑色）和注意力增强MnasNet（红色）的ImageNet top-1准确率和参数数量的关系。</p><p><strong>使用 COCO 数据集进行目标检测</strong></p><p>如表 5 所示，我们的相对自注意力机制提高了RetinaNet在ResNet-50和ResNet101上的性能。此外，我们的实验表明，尽管在压缩率σ∈{4，8，16}上进行了网格搜索，在 RetinaNet 的骨干网络中添加 Squeeze-and-Excitation 算子会显著损害性能。</p><p> <span><img class="small-img-inline" src="/2021/06/23/Attention%20Augmented%20Convolutional%20Networks/image-20231025235931837.png" alt></span></p><p>表5 使用具有不同主干结构的RetinaNet架构在 COCO 数据集上进行对象检测。我们报告了三个不同 IoU 值的平均准确率。</p><p><strong>消融研究</strong></p><p>全注意力视觉模型：本节中，我们研究了注意力增强的性能与注意力通道的比例的关系。当我们把这个比例增加到100%时，我们开始用一个全注意力模型来取代ConvNet。表6列出了ResNet-50架构上注意力增强在不同比率κ&#x3D;υ∈{0.25, 0.5, 0.75, 1.0}下的性能。结果表明，大部分情况下，采用注意力通道是很有竞争力的。</p><p> <span><img class="small-img-inline" src="/2021/06/23/Attention%20Augmented%20Convolutional%20Networks/image-20231025235936792.png" alt></span></p><p>表6 具有不同注意力通道比例的注意力增强 ResNet-50。</p><p>这些实验还表明，我们提出的自注意力机制是一个强大的、独立的图像分类计算单元，且全注意力模型对于分辨性视觉任务是可行的。</p><p>位置编码的重要性：图4中，我们展示了提出的二维相对位置编码作为注意力通道分数的函数的效果。实验表明，随着架构采用更多注意力通道，我们的相对位置编码变得越来越重要。</p><p> <span><img class="small-img-inline" src="/2021/06/23/Attention%20Augmented%20Convolutional%20Networks/image-20231025235941152.png" alt></span></p><p>图4 在我们的注意力增强的ResNet50上，随着注意力通道比例的增加，相对位置嵌入的效果。</p><p>我们另外将我们提出的二维相对位置编码与其他位置编码方案进行了比较。在表7和表8中，我们分别展示了各位置编码方案在ImageNet分类和COCO物体检测任务的结果。在这两项任务中，没有位置编码的注意力增强已经超越了完全卷积的非增强变体。我们的实验还表明，正弦编码和坐标卷积并不会提升对位置无感知版本的注意力增强效果。当使用我们的二维相对注意力时，我们获得了额外的提升，这证明了其可在防止置换等变的同时保留转换等变的效用。</p><p> <span><img class="small-img-inline" src="/2021/06/23/Attention%20Augmented%20Convolutional%20Networks/image-20231025235945792.png" alt></span></p><p>表7 注意力增强中不同位置编码对ImageNet分类的影响。</p><p> <span><img class="small-img-inline" src="/2021/06/23/Attention%20Augmented%20Convolutional%20Networks/image-20231025235949368.png" alt></span></p><p>表8 注意力增强中不同位置编码对使用RetinaNet AA-ResNet-50 骨架的COCO物体检测任务的影响。</p><h2><span id="讨论与今后工作">讨论与今后工作</span></h2><p>本项工作中，我们考虑把自注意力用于视觉模型，将其作为卷积的替代。我们为图像引入了一种新颖的二维相对自注意力机制，首次实现了在图像分类上训练有竞争力的完全自注意力的视觉模型。我们建议用这种自注意力机制来增强卷积算子，并验证了这种方法比其他注意力方案的优越性。广泛的实验表明，在各类的架构和计算环境下，注意力增强能提升图像分类和物体检测任务的系统效果。</p><p>这项工作还存在几个未解决问题。在未来的工作中，我们将专注于完全注意力制度，并探索不同的注意力机制是如何在计算效率与表示能力之间进行权衡的。此外，在使用自注意力机制时，完全依赖卷积的架构设计选择似乎是次优的，因此，尝试在自动架构搜索中使用注意力增强作为单元，以尝试找到比以前更好的图像分类、对象检测、图像分割等领域的模型是非常有趣的。最后，人们可以问：在视觉任务中，完全注意力模型在多大程度上能取代卷积网络？</p>]]></content>
    
    
    <summary type="html">&lt;img src=&quot;/2021/06/23/Attention%20Augmented%20Convolutional%20Networks/image-20231025235603474.png&quot; alt&gt;

&lt;h1 id=&quot;注意力增强的卷积网络&quot;&gt;&lt;a href=&quot;#注意力增强的卷积网络&quot; class=&quot;headerlink&quot; title=&quot;注意力增强的卷积网络&quot;&gt;&lt;/a&gt;注意力增强的卷积网络&lt;/h1&gt;&lt;h2 id=&quot;引用&quot;&gt;&lt;a href=&quot;#引用&quot; class=&quot;headerlink&quot; title=&quot;引用&quot;&gt;&lt;/a&gt;引用&lt;/h2&gt;&lt;p&gt;Bello I, Zoph B, Vaswani A, et al. Attention augmented convolutional networks[C]&amp;#x2F;&amp;#x2F;Proceedings of the IEEE&amp;#x2F;CVF International Conference on Computer Vision. 2019: 3286-3295.&lt;/p&gt;
&lt;h2 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要&quot;&gt;&lt;/a&gt;摘要&lt;/h2&gt;&lt;p&gt;卷积网络一直是许多计算机视觉应用中的首选范式。然而，卷积操作存在一个明显的弱点，即它只对局部邻域进行操作，因此缺少全局信息。另一方面，自注意力已经成为捕捉长距离相互作用的方法，但大多被应用于序列建模和生成式建模任务。本文中，我们考虑将自注意力用于判别视觉任务，将其作为卷积的替代方法。我们引入了一种新颖的二维相对自注意力机制，该机制被证明在取代卷积作为图像分类的独立计算单元方面具有竞争力。我们在控制实验中发现，把卷积和自注意力结合起来时可以得到最好的效果。因此，我们建议使用这种自注意力机制来增强卷积算子，将卷积特征图与通过自注意力产生的一组特征图连接起来。广泛的实验表明，注意力增强可在参数个数保持基本相似的情况下，使ImageNet上的图像分类和COCO上的物体检测在许多不同的模型和规模上有一致的效果提升，其中也包括了ResNets和最先进的移动受限网络。特别的，我们的方法在ImageNet分类上比ResNet50基线提高了1.3%的top-1准确率，并超过了其他图像的注意力机制，如Squeeze-and-Excitation。在COCO物体检测中，其相较RetinaNet基线上取得了1.4mAP的提升。&lt;/p&gt;</summary>
    
    
    
    <category term="论文阅读" scheme="https://values.keys.moe/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="论文阅读" scheme="https://values.keys.moe/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
  </entry>
  
  <entry>
    <title>RefineNet Multi-Path Refinement Networks</title>
    <link href="https://values.keys.moe/2021/06/22/RefineNet%20Multi-Path%20Refinement%20Networks/"/>
    <id>https://values.keys.moe/2021/06/22/RefineNet%20Multi-Path%20Refinement%20Networks/</id>
    <published>2021-06-22T03:50:00.000Z</published>
    <updated>2023-10-25T15:20:31.701Z</updated>
    
    <content type="html"><![CDATA[<img src="/2021/06/22/RefineNet%20Multi-Path%20Refinement%20Networks/clip_image002.jpg" alt><h1><span id="refinenet用于密集预测的多路径细化网络">RefineNet：用于密集预测的多路径细化网络</span></h1><h2><span id="引用">引用</span></h2><p>Lin G, Liu F, Milan A, et al. Refinenet: Multi-path refinement networks for dense prediction[J]. IEEE transactions on pattern analysis and machine intelligence, 2019, 42(5): 1228-1242.</p><h2><span id="摘要">摘要</span></h2><p>近来，层数相当深的卷积神经网络（CNN）在对象识别方面表现出了出色的性能，其已成为语义分割和深度估计等预测问题的首选。然而，深层CNN中的重复子采样操作，如池化或卷积都会导致初始图像分辨率大幅下降。在此，我们提出了RefineNet，一个通用的多路径细化网络，其明确利用了下采样过程中的所有可用信息，以使用长距离残差连接实现高分辨率的预测。通过这种方式，捕捉高层语义特征的更深层可以直接利用早期卷积的细粒度特征进行细化。RefineNet的各个组成部分采用了遵循恒等映射思维的残差连接，这使得有效的端到端训练成为可能。此外，我们引入了链式残差池，其以有效的方式捕获丰富的上下文背景。我们对语义分割进行了全面的实验，其是一个密集分类问题，其在七个公共数据集上取得了良好的性能。我们进一步将我们的方法用于深度估计，并证明我们的方法在密集回归问题上的有效性。</p><span id="more"></span><h2><span id="介绍">介绍</span></h2><p>密集预测，也被称为逐像素预测问题，是计算机视觉课题的一个基本类别。这里的密集预测任务是为给定图像中的每一个像素分配一个类标签或连续值。无论是离散值还是连续值的预测，各类视觉任务都可以被表述为密集预测问题。例如，语义分割和对象解析是典型的密集分类问题，单目图像的深度估计是有代表性的密集回归问题。</p><p>语义分割是图像理解的重要组成部分。此处的任务是为图像中的每一个像素分配一个独特的标签（或类别），而这被认为是一个密集分类问题。所谓的对象解析的相关问题，它们的目的是分割和识别对象的各个部分，这些问题通常可以被转换为语义分割任务。单目图像的深度估计是预测单个图像的像素级深度值（连续真实值），可以将其表述为密集回归问题。它在3D重建、视觉识别、场景理解、自动驾驶等方面有着广泛的应用。</p><p>近期，深度学习方法，特别是卷积神经网络（CNN），如VGG、Residual Net，在识别任务中表现出了显著的效果。然而，当涉及到密集深度或法向量估计与语义分割等任务中的密集预测时，这些方法表现出了明显的局限性。空间池化和卷积通常将最终的输出预测在每个维度上减少32倍，从而失去了大量更精细的图像结构。如何应用最先进的CNN方法，如VGG、Residual Net，进行高分辨率的密集预测已成为一个热门话题。</p><p>解决这一局限性的方法之一是学习反卷积滤波器作为上采样操作，以生成高分辨率特征图。反卷积操作无法恢复在正向卷积阶段下采样操作后丢失的低层视觉特征。因此，它们无法输出准确的高分辨率预测。低层视觉对于准确预测边界或细节而言至关重要。Chen等人最近提出的DeepLab方法采用空洞（或扩张）卷积以获得更大的感受野，而不会缩小图像。DeepLab应用广泛，代表了语义分割的最先进性能。这种策略虽然成功，但至少有两个局限性：首先，其需要对大量通常具有高维特征的详细（高分辨率）特征图进行卷积，计算量很大。尤其是在训练阶段，大量高维、高分辨率的特征图需要巨大的GPU内存资源。这阻碍了高分辨率预测的计算，且通常将输出大小限制为了原始输入的1&#x2F;8。其次，扩张卷积引入了粗略的特征子采样，这可能会导致重要细节的丢失。</p><p>另一种方法利用中层特征来生成高分辨率预测。这些工作背后的直觉是，来自中间层的特征被期望描述对象部分的中层表示，同时保留空间信息。这种信息被认为是对早期卷积层特征的补充，这些特征编码低层的空间视觉信息，如边、角、圆等。其同时也是对更高层的特征的补充，这些特征编码高层语义信息，包括对象层和类别层的迹象，但其缺乏强大的空间信息。</p><p>我们认为所有级别的特征都有助于语义分割。高层语义特征有助于图像区域的类别识别，而低层视觉特征有助于为高分辨率预测生成清晰、详细的边界。如何有效地利用中层特征仍是一个悬而未决的问题，值得更多关注。为此，我们提出了一种新颖的网络架构，它有效地利用了多层特征来生成高分辨率预测。图1显示了我们方法的一些密集预测示例。</p><p><span><img class="small-img-inline" src="/2021/06/22/RefineNet%20Multi-Path%20Refinement%20Networks/clip_image004.jpg" alt></span></p><p>图1 我们的方法在对象解析（左）和语义分割（右）任务上的示例结果。</p><p>我们的主要贡献如下：</p><p>\1. 我们提出了一个多路径细化网络RefineNet，它利用多层抽象的特征进行高分辨率密集预测。RefineNet 以递归的方式用细粒度的低级特征细化低分辨率（粗略的）特征以生成高分辨率特征图。我们的模型很灵活，因为它可以以各种方式级联和修改。</p><p>\2. 我们的级联 RefineNet 可以有效地进行端到端的训练，这对于良好的预测性能至关重要。更具体地说，RefineNet中的所有组件都采用了基于恒等映射的残差连接，这样，梯度可以通过短距离和长距离残差连接直接传播，从而实现有效且高效的端到端训练。</p><p>\3. 我们提出了一个新的网络组件，我们称之为“链式残差池化”，它能够从一个大的图像区域捕捉背景信息。它通过有效地汇集具有多个窗口大小的特征，并将它们与残差连接和可学习权重融合在一起来实现。</p><p>\4. 所提出的RefineNet在PASCAL VOC 2012、PASCAL-Context、NYUDv2、SUN-RGBD、Cityscapes、ADE20K这些语义分割数据集和和对象解析Person-Parts数据集上都取得了优异的性能。</p><h2><span id="提出的方法">提出的方法</span></h2><p>我们提出了一个提供多条路径的新的框架。在这些路径上，来自不同分辨率、通过潜在的长距离连接的信息被一个通用的构建块RefineNet来同化。图2显示了为实现我们的高分辨率语义分割目标而对构件的一种可能安排。</p><p><span><img class="small-img-inline" src="/2021/06/22/RefineNet%20Multi-Path%20Refinement%20Networks/clip_image006.jpg" alt></span></p><p>图2 在不同的卷积阶段利用各种层级的细节，并将其融合以获得高分辨率的预测，而无需维护大型中间特征图。</p><h3><span id="多路径细化">多路径细化</span></h3><p>如前所述，我们的目标是利用多级特征进行具有长距离残差连接的高分辨率预测。RefineNet提供了一种通用方法来将粗略的高层语义特征与细粒度的低层特征融合以生成高分辨率的语义特征图。该设计的一个重要方面是确保梯度可以轻松地通过网络向后传播，一直到长距离残差连接的早期的低层，从而确保整个网络可以进行端到端的训练。</p><p>对于我们的标准多路径架构，我们根据特征图的分辨率将预训练的ResNet（使用 ImageNet 训练）划分为4个块，并采用4个RefineNet单元的4级联架构，每个单元直接连接到一个 ResNet块的输出以及级联中前一个RefineNet块的输出上。但请注意，这样的设计并不独特。事实上，我们灵活的架构允许对不同的变体进行简单的探索。例如，一个RefineNet块可以接受来自多个ResNet块的输入。</p><p>我们把RefineNet-m表示为连接到ResNet中块-m的输出的RefineNet块。在实践中，每个 ResNet 输出都通过一个卷积层来适应维度。虽然所有的RefineNet都有相同的内部结构，但它们的参数并不绑定，从而可以更灵活地适应各个层次的细节。</p><p>整个网络可以进行端到端的高效训练。需要注意的是，我们在 ResNet 中的块和 RefineNet 模块之间引入了长距离残差连接。在前向传递期间，这些长距离残差连接传达了低层特征，这些特征对视觉细节进行编码，以细化粗略的高层特征图。在训练步骤中，长距离残差连接允许将梯度直接传播到早期卷积层中，这有助于有效的端到端训练。</p><h3><span id="refinenet">RefineNet</span></h3><p>RefineNet 块的架构如图 3a 所示。在图2c所示的多路径概述中，RefineNet-1有一个输入路径，而所有其他RefineNet块有两个输入。但请注意，我们的结构是通用的，每个Refine块可以很容易地修改，以接受任意数量的具有任意分辨率和深度的特征图。</p><p>卷积残差单元。每个RefineNet块的第一部分包括一个自适应卷积集，主要是微调我们的任务的预训练的ResNet权重。为此，每个输入路径依次通过两个卷积残差单元（RCU），这是原始ResNet中卷积单元的简化版，其中去掉了batch-normalization层。在我们的实验中，RefineNet-4的每个输入路径的过滤器数量被设置为512，其余的为256。</p><p>多分辨率融合。如图3c所示，之后所有路径输入被多分辨率融合块融合成一个高分辨率的特征图。该模块首先应用卷积来适应输入，产生相同特征维度（输入中最小的维度）的特征图，然后将所有（较小的）特征图上采样到输入的最大分辨率。最后，所有特征图通过求和进行融合。此块中的输入自适应性也有助于沿着不同的路径对特征值进行适当的重新缩放，这对随后的求和融合很重要。</p><p><span><img class="small-img-inline" src="/2021/06/22/RefineNet%20Multi-Path%20Refinement%20Networks/clip_image008.jpg" alt></span></p><p>图3 我们的多路径细化网络架构RefineNet的各个组成部分。RefineNet中的组件采用具有恒等映射的残差连接。这样，梯度可以通过局部残差连接直接在 RefineNet 内部传播，也可以通过长距离残差连接直接传播到输入路径，从而实现对整个系统的有效端到端训练。</p><p>链式残差池化。如图 3d 所示，输出的特征图会再经过链式残差池化块。所提出的链式残差池化的目的是为了从大的图像区域中获取背景信息。它能够有效地汇集具有多个窗口大小的特征，并使用可学习的权重将它们融合在一起。特别的，该组件被构建为多个池化块链，每个池化块由一个最大池化层和一个卷积层组成。一个池化块将前一个池化块的输出作为输入。因此，当前的池化块能够重用前一个池化操作的结果，从而在不使用大的池化窗口的情况下获取大区域的特征。使用更多的池化块通常会获得更好的性能。在我们的实验中，我们所报告的最佳结果是在一个链式残差池化模块中使用4个池化块。</p><p><span><img class="small-img-inline" src="/2021/06/22/RefineNet%20Multi-Path%20Refinement%20Networks/clip_image009.png" alt></span></p><p>图4 链式残差池化 (CRP) 的可选架构。与图3(d)中的CRP结构相比，池化层（灰色标记）和卷积层的位置是相互交换的。</p><p>所有池化块的输出特征图都通过残差连接的求和与输入特征图融合在一起。需要注意的是，对该构建块，我们仍使用残差连接，这再次促进了训练中的梯度传播。在一个池化块中，每个池化操作之后都有卷积层，其作为求和融合的加权层。预计在训练过程中，这个卷积层将学会适应池化块的重要性。</p><p>我们也可以考虑为我们的链式残差池化块提供其他架构。图4显示了另一种链式残差池化的架构。这个可选架构是通过交换卷积层和池化层在一个池化中的位置而对图3d所示的架构进行修改。在通过池化层之前，这个卷积层将学习适应输入特征，并适应输入特征的重要性。根据我们的观察，这种替代架构在某些数据集中的表现有时会比原始架构略好。</p><p>输出卷积。每个RefineNet块的最后一步是另一个卷积残差单元（RCU）。这导致每个块之间有三个RCU的序列。为了在最后一个 RefineNet-1 块中反映这种行为，我们在最终的 softmax 预测步骤之前放置了两个额外的 RCU。此处的目标是在多路径融合特征图上采用非线性操作来生成特征以进行进一步处理或进行最终预测。经过此块后，特征维度保持不变。</p><h3><span id="refinenet中的恒等映射">RefineNet中的恒等映射</span></h3><p>请注意，RefineNet 的所有卷积组件均受残差连接背后的思想启发而精心构建，并遵循恒等映射的规则。这使得梯度通过RefineNet有效地反向传播，并促进级联多路径细化网络的端到端学习。</p><p>使用具有恒等映射的残差连接允许梯度从一个块直接传播到任何其他块。此观念鼓励为快捷连接（shortcut connections）保持干净的信息路径，以便这些连接不会被任何非线性层或组件“阻塞”。相反，非线性操作被置于主要信息路径的分支上。我们遵循此思想来开发 RefineNet 中的各个组件与卷积单元。正是这种特殊的策略使多级联 RefineNet 能够得到有效训练。请注意，我们在链式残差池化块中包含了一个非线性激活层 (ReLU)。我们观察到，这个ReLU对于后续池化操作的有效性非常重要，它也使模型对学习率的变化不那么敏感。我们观察到，每个RefineNet块中的单个ReLU不会明显降低梯度流的有效性。</p><p>RefineNet中既有短距离残差连接，又有长距离残差连接。短距离残差连接是指在一个 RCU 或残差池化组件中的局部快捷连接，而长距离残差连接是指 RefineNet 模块和 ResNet 块之间的连接。通过远程残差连接，梯度可以直接传播到 ResNet 中的早期卷积层，从而实现所有网络组件的端到端训练。</p><p>融合块融合了多条快捷路径的信息，这可以看作是对多个残差连接进行了必要的维度或分辨率调整的求和融合。在这方面，这里的多分辨率融合块的作用类似于 ResNet 中传统卷积残差单元中“求和”融合的作用。在RefineNet中，特别是在RefineNet的融合块中，有一些层执行线性特征转换操作，如线性特征降维或双线性上采样。这些层被放置在快捷路径上，而这与ResNet的情况是相似的。在ResNet中，当一个快捷连接跨越两个块时，它会在捷径路径中包括一个卷积层，用于适应线性特征维度，以确保特征维度与下一个区块中的后续求和相匹配。由于这些层中只采用了线性变换，梯度仍然可以通过这些层有效地进行传播。</p><h3><span id="密集预测的refinenet">密集预测的REFINENET</span></h3><p>我们在本节中介绍了不同的密集预测任务的训练目标，即密集分类和密集回归。</p><p><strong>密集分类</strong></p><p>我们展示了我们用于语义分割的方法，这是密集分类的一个代表性任务。我们将<span><img class="small-img-inline" src="/2021/06/22/RefineNet%20Multi-Path%20Refinement%20Networks/clip_image011.jpg" alt></span>表示为索引为(i,j)的像素的真实标签，Zijk为第k个通道和索引为(i,j)的像素在softmax层之前的输出。C是语义类别的总数。使用softmax 损失来计算密集分类任务：</p><p><span><img class="small-img-inline" src="/2021/06/22/RefineNet%20Multi-Path%20Refinement%20Networks/clip_image013.jpg" alt></span></p><p>其中，l(·) 是一个指示函数，当输入语句为真时输出 1，否则输出 0。</p><p><strong>密集回归</strong></p><p>单目图像的深度估计是从单个 RGB 图像推断像素级深度值。我们在这里使用 RefineNet 进行深度估计，以证明我们的方法对预测连续值的密集回归问题的能力。</p><p>我们提出的RefineNet可以很容易地适用于连续密集预测任务。其只需要做最小的修改，即把softmax层改为回归层即可。我们在这里应用最常用的最小二乘损失，即最小化预测图 Y 和真实<span><img class="small-img-inline" src="/2021/06/22/RefineNet%20Multi-Path%20Refinement%20Networks/clip_image014.png" alt></span>之间的欧几里得距离。</p><p>在语义分割的情况下，这里一个像素的输出是一维深度值而不是 C-维概率。</p><h2><span id="实验">实验</span></h2><p>为了展示我们方法的有效性，我们对七个公共数据集进行了综合实验，其中包括六个流行的室内外场景语义分割数据集（NYUDv2、PASCAL VOC 2012、SUN-RGBD、PASCAL-Context、Cityscapes、ADE20K MIT)，以及一个称为 Person-Part 的对象解析数据集。我们还在 NYUDv2 数据集上演示了深度估计。分割质量通过所有类别的交并比（IoU）分数、像素准确度和所有类别的平均准确度来衡量。正如文献中通常所做的那样，我们在训练期间应用简单的数据扩增。如果没有进一步说明，我们应用测试时多尺度评估，这是分割方法中的常见做法。对于多尺度评估，我们对同一图像在不同尺度上的预测结果求平均，以得到最终的预测结果。预训练ResNet的batch-normalization层中的均值和方差参数被冻结，因此不会在我们的RefineNet的训练过程中更新。我们还提供了一个消融研究来检查我们模型的各种组件和可选架构的影响。我们的系统建立在MatConvNet上。</p><h3><span id="消融研究">消融研究</span></h3><h3><span id="refinenet组件分析">RefineNet组件分析</span></h3><p><span><img class="small-img-inline" src="/2021/06/22/RefineNet%20Multi-Path%20Refinement%20Networks/clip_image016.jpg" alt></span></p><p>表1 RefineNet在NYUDv2上进行的消融实验</p><p>表1展示了我们提出的消融实验结果，该消融实验被设计以量化以下组件的影响：网络深度、链式残差池化和多尺度评估 (Msc Eva)。实验表明，这三个因素中的每一个都能持续改善由IoU衡量的性能。我们的链式池化显著提高了性能，基本上，使用更多池化块有助于获得更好的结果。最好的结果是通过在我们的链式残差池化模块中使用 4 个池化块来实现的。在随后的实验中，我们将此设置表示为“Pool4”。 而对于4 个池化块的设置，我们使用图 4 中链式池化的架构。一般来说，更深的初始化网络，如ResNet-101和ResNet-152，会带来更好的性能。</p><p><span><img class="small-img-inline" src="/2021/06/22/RefineNet%20Multi-Path%20Refinement%20Networks/clip_image018.jpg" alt></span></p><p>图5 我们网络结构的三个变体的说明。(a) 单个RefineNet；（b）2级联的RefineNet；（c）4级联的RefineNet与2尺度的ResNet。请注意，我们提出的RefineNet模块可以在无需任何修改的情况下无缝处理任意分辨率和尺寸的不同数量的输入。</p><h3><span id="级联refinenet的变体">级联RefineNet的变体</span></h3><p>如前所述，我们的RefineNet是灵活的，因为它可以以各种方式级联产生各种架构。在此，我们讨论了我们的RefineNet的几种变体。具体来说，我们介绍了使用单一的RefineNet、2级联的RefineNet和4级联的RefineNet与2尺度的ResNet的架构。图5显示了所有三种变体的架构。本实验中采用了2个池化块的链式池化。</p><p>单一的RefineNet模型是我们网络的最简单的变体。它只由一个单一的RefineNet模块组成，它从ResNet的四个模块中获取所有四个输入，并在单一流程中融合所有分辨率的特征图。2级联版本采用了两个RefineNet模块。底部的RefineNet-2有两个来自ResNet块3和4的输入，另一个有三个输入，两个来自其余的ResNet块，一个来自RefineNet-2。对于图5c中的2尺度模型，我们用2个尺度的图像作为输入，并分别使用2个ResNet来生成特征图；输入图像被缩放1.2和0.6倍，并输入到2个独立的ResNet中。</p><p>这些变体在 NYUD 数据集上的评估结果如表 2 所示。这个实验表明，4级联的版本比2级联和1级联的版本性能更好，使用2个ResNet的2尺度的图像输入比使用1级联的输入要好。这是符合预期的，因为网络的容量更大。然而，这也会导致更长的训练时间。因此，我们在所有的实验中采用单尺度4级联版本作为标准架构。</p><p><span><img class="small-img-inline" src="/2021/06/22/RefineNet%20Multi-Path%20Refinement%20Networks/clip_image020.jpg" alt></span></p><p>表2 对级联式RefineNet的4种变体进行评估。在NYUDv2数据集上的单级联RefineNet、2级联RefineNet、4级联RefineNet、4级联RefineNet与2尺度ResNet。</p><p>内存与计算分析</p><p>不同方法下的内存与计算分析如表3所示。</p><p><span><img class="small-img-inline" src="/2021/06/22/RefineNet%20Multi-Path%20Refinement%20Networks/clip_image022.jpg" alt></span></p><p>表3 内存与计算分析</p><p>此外，我们的RefineNet的内存使用和计算成本的细节见表4。输入图像大小为512 x 512。这表明，我们的RefineNet的内存消耗和计算成本是恒定的，它们与基础网络的选择无关。因此，我们的RefineNet在与非常深的基础网络结合进行高分辨率预测时，比基于扩张（trous）卷积的方法要有效得多。</p><p><span><img class="small-img-inline" src="/2021/06/22/RefineNet%20Multi-Path%20Refinement%20Networks/clip_image024.jpg" alt></span></p><p>表4 RefineNet的内存使用和计算成本的细节</p><h3><span id="对象解析">对象解析</span></h3><p>在本节中，我们展示了对象解析任务的结果，该任务包括识别和分割对象部分。我们在 Person-Part 数据集进行了实验。我们将我们的结果与表 5 中列出的一些最先进的方法进行了比较。结果清楚地证明了我们的方法的进步。我们在该数据集上解析对象的定性样本如图6所示。</p><p><span><img class="small-img-inline" src="/2021/06/22/RefineNet%20Multi-Path%20Refinement%20Networks/clip_image025.png" alt></span></p><p>表5 Person-Part 数据集上的对象解析结果</p><p><span><img class="small-img-inline" src="/2021/06/22/RefineNet%20Multi-Path%20Refinement%20Networks/clip_image026.png" alt></span></p><p>图6 在Person-Parts数据集上的我们的预测示例</p><h3><span id="语义分割">语义分割</span></h3><p>本节通过描述了我们的方法在六个公共基准上的密集语义标签集（NYUDv2、PASCAL VOC 2012、PASCAL VOC 2012、SUN-RGBD、ADE20K MIT、Cityscapes）上的实验效果，证明了我们的RefineNet在所有数据集上都优于以前的方法。</p><h3><span id="深度估计">深度估计</span></h3><p>本节通过在NYUDv2数据集进行实验，证明了我们的方法对单目图像深度估计的功效。过程中，采用如下几种方式进行定量评估：</p><p>平均相对误差（rel）：<span><img class="small-img-inline" src="/2021/06/22/RefineNet%20Multi-Path%20Refinement%20Networks/clip_image028.jpg" alt></span></p><p>均方根误差（rms）：<span><img class="small-img-inline" src="/2021/06/22/RefineNet%20Multi-Path%20Refinement%20Networks/clip_image030.jpg" alt></span></p><p>平均log10误差（log10）：<span><img class="small-img-inline" src="/2021/06/22/RefineNet%20Multi-Path%20Refinement%20Networks/clip_image032.jpg" alt></span></p><p>阈值thr的准确性：当<span><img class="small-img-inline" src="/2021/06/22/RefineNet%20Multi-Path%20Refinement%20Networks/clip_image033.png" alt></span>时， dp的百分比。</p><h2><span id="结论">结论</span></h2><p>我们提出了RefineNet，一个用于高分辨率密集预测的新型多路径细化网络。级联结构能够有效地将高层的抽象与低层的特征结合起来，产生高分辨率的像素级预测图。我们的设计选择受到恒等映射思想的启发，它有助于长连接的梯度传播，从而实现有效的端到端学习。我们专注于具有代表性的离散和连续密集预测问题的语义分割和单目图像深度估计任务。大量实验表明，我们的RefineNet在多个公共基准测试中的表现优于之前的大多数作品，其为语义标记的最新技术水平树立了新的标杆。请注意，我们的方法也可以应用于其他密集预测任务，如低层视觉任务，包括图像去噪、超分辨率、边缘检测等。这可以在未来的工作中探索。</p>]]></content>
    
    
    <summary type="html">&lt;img src=&quot;/2021/06/22/RefineNet%20Multi-Path%20Refinement%20Networks/clip_image002.jpg&quot; alt&gt;

&lt;h1 id=&quot;RefineNet：用于密集预测的多路径细化网络&quot;&gt;&lt;a href=&quot;#RefineNet：用于密集预测的多路径细化网络&quot; class=&quot;headerlink&quot; title=&quot;RefineNet：用于密集预测的多路径细化网络&quot;&gt;&lt;/a&gt;RefineNet：用于密集预测的多路径细化网络&lt;/h1&gt;&lt;h2 id=&quot;引用&quot;&gt;&lt;a href=&quot;#引用&quot; class=&quot;headerlink&quot; title=&quot;引用&quot;&gt;&lt;/a&gt;引用&lt;/h2&gt;&lt;p&gt;Lin G, Liu F, Milan A, et al. Refinenet: Multi-path refinement networks for dense prediction[J]. IEEE transactions on pattern analysis and machine intelligence, 2019, 42(5): 1228-1242.&lt;/p&gt;
&lt;h2 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要&quot;&gt;&lt;/a&gt;摘要&lt;/h2&gt;&lt;p&gt;近来，层数相当深的卷积神经网络（CNN）在对象识别方面表现出了出色的性能，其已成为语义分割和深度估计等预测问题的首选。然而，深层CNN中的重复子采样操作，如池化或卷积都会导致初始图像分辨率大幅下降。在此，我们提出了RefineNet，一个通用的多路径细化网络，其明确利用了下采样过程中的所有可用信息，以使用长距离残差连接实现高分辨率的预测。通过这种方式，捕捉高层语义特征的更深层可以直接利用早期卷积的细粒度特征进行细化。RefineNet的各个组成部分采用了遵循恒等映射思维的残差连接，这使得有效的端到端训练成为可能。此外，我们引入了链式残差池，其以有效的方式捕获丰富的上下文背景。我们对语义分割进行了全面的实验，其是一个密集分类问题，其在七个公共数据集上取得了良好的性能。我们进一步将我们的方法用于深度估计，并证明我们的方法在密集回归问题上的有效性。&lt;/p&gt;</summary>
    
    
    
    <category term="论文阅读" scheme="https://values.keys.moe/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="论文阅读" scheme="https://values.keys.moe/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
  </entry>
  
  <entry>
    <title>Climbing towards NLU On Meaning, Form, and Understanding in the Age of Data</title>
    <link href="https://values.keys.moe/2021/06/21/Climbing%20towards%20NLU%20On%20Meaning,%20Form,%20and%20Understanding%20in%20the%20Age%20of%20Data/"/>
    <id>https://values.keys.moe/2021/06/21/Climbing%20towards%20NLU%20On%20Meaning,%20Form,%20and%20Understanding%20in%20the%20Age%20of%20Data/</id>
    <published>2021-06-20T18:42:24.000Z</published>
    <updated>2023-10-25T16:07:32.037Z</updated>
    
    <content type="html"><![CDATA[<img src="/2021/06/21/Climbing%20towards%20NLU%20On%20Meaning,%20Form,%20and%20Understanding%20in%20the%20Age%20of%20Data/clip_image002.jpg" alt><h1><span id="迈向nlu关于数据时代的含义-形式和理解">迈向NLU：关于数据时代的含义、形式和理解</span></h1><h2><span id="引用">引用</span></h2><p>Bender E M, Koller A. Climbing towards NLU: On meaning, form, and understanding in the age of data[C]&#x2F;&#x2F;Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 2020: 5185-5198.</p><h2><span id="摘要">摘要</span></h2><p>大型神经语言模型在许多NLP任务上的成功令人激动。然而我们发现，这些成功有时是就像是在炒作，它们将模型描述成“理解”语言或捕捉“含义”。在本文立场中，我们认为仅在形式上训练的系统先验地无法学习含义。与ACL 2020的主题“通观现状与展望未来”一致，我们认为，清楚地理解形式和含义之间的区别，将有助于推进围绕自然语言理解的科学进步。</p><span id="more"></span><h2><span id="介绍">介绍</span></h2><p>自然语言处理领域当前的一大研究热点是大型神经语言模型（LMs），如BERT或GPT-2模型。它们在许多任务中取得了巨大的进展，包括那些表面上对含义敏感的任务。这导致了在学术和流行刊物上存在这样的说法：模型能“理解”或“领悟”自然语言或学习其“含义”。从我们的角度来看，这些都是对语言形式和含义之间的关系的误解所导致的夸大其词。</p><p>我们认为，由于语言建模任务只使用形式作为训练数据，原则上其不能产生含义的学习。我们认为语言模型这一术语指的是任何对字符串预测任务进行训练的系统（不管其是在字符、词语还是句子上操作，也不管是否按序操作）。我们认为语言的含义是指语言形式和交际意图之间的关系。</p><p>我们的目标是倡导人们的主张应该与方法保持一致：人类模拟自然语言理解（NLU）是人工智能的一项巨大挑战，其涉及对语言结构和使用的掌握，以及将其融入世界的能力。尽管大型神经LMs很可能最终成为人类模拟自然语言理解的全面解决方案的重要组成部分，但它们并不是解决这一重大挑战的近乎完善的方案。本文我们认为，我们领域的真正进步——爬上正确的山，而不仅仅是我们当前所在的山——取决于在任务设计与实验结果报告中保持对含义和理解等大局概念的清晰。</p><h2><span id="大型lms炒作与分析">大型LMs：炒作与分析</span></h2><p>在讨论大型LMs应用于含义敏感任务的相关论文中，往往会使用一些术语来描述模型，而如果只按表面意思来解释，则会产生误导。</p><p>而如果强调的术语是为了描述人类对事实知识的类比理解、领悟与回想等，那么它们都有严重的夸张。相反，如果它们是作为技术术语，那就更应该被明确定义。</p><p>在我们学术讨论中不谨慎地使用术语亦导致了大众媒体对AI的炒作。</p><p>总之，从我们的学术文献来看，并不清楚是否所有的作者都清楚形式和含义之间的区别，但很明显的是，我们谈论神经LMs所做之事的方式是对公众的误导。</p><p>这种使用模糊语言的部分原因可能是我们尚未完全理解大型LMs 所隐含表示的语言到底是什么。</p><p>一些学者仔细地研究了大型LMs在明显含义敏感的任务的成功，发现在事实上，它们远没有完成表面上要求的“推理”任务，相反，它们只是比以前的方法更有效地利用数据中的信息。</p><p><strong>什么是含义？</strong></p><p>首先定义两个关键术语。</p><p>“<strong>形式</strong>”是语言的任何可观察的实现，如页面上的标记、数字化文本表征中的像素或字节、发音器的运动等。</p><p>“含义”是形式与语言之外的事物之间的关系。</p><p><strong>含义与交际意图</strong></p><p>人类使用语言是有目的的，我们说话不是为了使发音器官快乐的振动，而是为了达到一些交际意图。交际意图有很多种：可能是为了向对方传递一些信息，抑或是要求对方做某件事，也可能只是为了社交。我们认为含义是M⊆E×I的关系，它包含（e，i）对，其中e为自然语言表达式，i为它们可以用来唤起的交际意图。鉴于此含义的定义，我们现在可以用“理解”来指代给定e检索i的过程。</p><p>交际意图也可以是关于抽象世界的，如银行账户、计算机文件系统，或者是说话人心中的一个纯粹的虚拟世界。</p><p>语言学家将交际意图与常规含义区分开来，对于一个单词、短语或句子的表达，其常规含义在所有可能的使用语境中是不变的。给定语言系统，常规含义是一个抽象的对象，它代表了一种形式的交际潜力。每个语言系统（比如英语）都提供了一个关系C⊆E×S，其中包含(e,s)对，其中e为表达式，s为它们的常规含义。我们假设常规含义一定有理解方式。因此，就像含义关系M一样，C将语言与语言之外的对象联系起来。</p><p>回到上面的含义关系M，它应被理解为两个对话者之间共享语言系统由关系C充当媒介。说话者有一定的交际意图i，并选择了一个适合在当前交际情况下表达意图i的、具有确定含义s的表达e。听到e后，听者会重构s并使用它们对交际情景的了解以及他们对说话者的心理状态和意图的假设，尝试推断出意图i。</p><p>总而言之，当我们努力理解NLU任务和这些任务上的系统性能与构建人类模拟自然语言理解系统的大目标之间的关系时，将形式、常规含义和交际意图明确区分开是非常有用的。此外，我们应该注意不要把交际意图和关于世界的基本事实混为一谈，因为说话者当然会犯错，也会有意地掩饰等。</p><p>我们认为纯粹根据形式训练的自然语言模型不会学习含义：如果训练数据只有形式，就没有足够的信号来学习该形式与使用语言的人的非语言意图之间的关系 M , 也没有语言系统所赋予的每个形式的形式与常规含义之间的关系C。</p><p><strong>含义与智慧</strong></p><p>长期以来，含义和理解被视为智能的关键。正如图灵提出的那样，如果一个机器在与人类进行任意的书面对话后，人们无法分辨出他是人还是机器，就可以说机器是会思考的。 然而，即使他们知道它们是人造的，人们还是很快就会把含义甚至是智慧归结到“人工智能”上。</p><p>这意味着我们在设计对机器理解的评价时必须格外小心，正如Searle在他的“中文房间”实验中所阐述的那样：他开发了一个“系统”作为隐喻，在这个系统中，一个不会说中文的人在其中按照预定的规则查阅中文书库，回答中文问题。从外界看，这个系统似乎“理解”了中文，但实际系统内部根本没有真正的理解。</p><p>Searle 的思想实验基于如下前提：系统可以较好地操控形式，使其与能理解形式含义、原因并做出适当反应的系统无法区分。我们观察到，最近在NLP方面的许多工作都声称要构建系统，但这些系统均只能访问形式。但是，语言是用来交流说话者的真实（物理、社会与心理）世界的，因此，产生有含义的反应背后的推理必须将感知输入的含义与有关该世界的信息联系起来。这反过来意味着，人类或机器要学习一种语言，就必须解决Harnad所说的符号基础问题。Harnad 对该问题如此概括：不会说中文的人不可能仅从中文字典的定义中学习中文单词的含义。</p><p>我们在这里的目的是更深入地研究，即使在现代硬件和技术背景下，连接主义模型可以扩展到接受大量数据的程度，为什么含义仍不能仅从语言形式中学习。我们认为，不管通过图灵测试是否意味着一个系统是智能的，一个只在形式上进行训练的系统将无法通过足够敏感的测试，因为它缺乏将其话语与世界联系起来的能力。</p><p><strong>章鱼测试</strong></p><p>为了说明试图仅从形式上学习含义的挑战，我们提出一个具体的场景。假设A和B都是流利的英语使用者，独立滞留于两个无人岛上。他们很快发现，以前来过这些岛屿的人留下了电报，他们可以通过水下电缆互相交流。A和B开始愉快地和对方交互信息。</p><p>与此同时，一只无法访问或观察这两个岛屿的超智能深海章鱼O，通过窃听水下电缆，监听A和B的对话。O最初对英语一无所知，但却非常擅长检测统计模式。随着时间的推移，O学会非常准确地预测B将如何回应A的每一句话。O还观察到某些单词倾向于出现在相似的上下文中，并且可能它已学会了通过假设其可以在某种程度上互换来泛化词汇形式。尽管如此，O从未观察过这些对象，因此当出现一组（现实）替代方案时，O将无法分辨出词的所指对象。</p><p>有时，O感到孤独。它切断了水下电缆，将自己插入对话中，假装成B，回复A的信息。O能成功地冒充B而不使A起疑吗？这构成了图灵测试的一种弱形式（弱是因为A没有理由怀疑她是在和一个非人类交谈）；有趣的问题是，O没有通过测试是否是因为他没有学到含义关系，而只看到了A和B的话语形式？</p><p>O能在多大程度上骗过A，取决于任务——也就是说，取决于A要谈的是什么。A和B已经花了很多时间来交换关于他们日常生活的琐事，以使岛上漫长的夜晚更加愉快。O似乎有可能生成B曾经发送的那种新句子，基本上充当了一个聊天机器人。这是因为这种对话中的话语主要具有社会功能，不需要以对话者的实际真实情况或关于现实世界的任何其他具体内容为基础。产生内部连贯的文本就足够了。</p><p>现在假设 A 发明了一种新设备，比如椰子弹射器。她兴高采烈地把制作椰子弹射器的详细说明发给B，询问B的经验和改进建议。即使O有办法在水下建造弹射器，他也不知道皮筋和椰子等词指的是什么，因此无法在物理上重现实验。他只能求助于先前关于 B 如何回应类似措辞的言论的观察。或许 O 可以将有关芒果和指甲的话语识别为“措辞相似”，因为这些词出现在与椰子和皮筋相似的上下文中。所以 O 决定简单地说“好主意，干得好！”，因为当 A 谈论绳索和钉子时，B 说了很多这句话。完全可以想象，A 接受这个回复是有含义的——但这只是因为 A将含义归因于O的回答。这并不是因为O理解了A的指示的含义，甚至这都不是他自己的回答。</p><p>最后，A面临一个紧急情况。她突然被一只愤怒的熊追赶。她抓起几根棍子，疯狂地请求B想一个办法，做一个武器来保护她。当然，O不知道A的“含义”。解决这样的任务，需要有在文字和现实世界实体之间准确映射的能力（以及推理和创造性思维）。就此而言，如果A没有在注意到欺骗行为之前被熊吃掉，那么O将无法通过图灵测试。</p><p>由于只有形式可以作为训练数据，O并未学习到含义。A和B交流的语言是他们交际意图通过含义关系投射到语言形式中。如果无法获得假设和测试潜在交流意图的手段，仅从形式上重建它们是希望渺茫的。O的语言使用最终会与能将语言建立在连贯的交际意图上的正常人的语言使用相背离。</p><p>本思想实验亦说明了我们关于听众在交流中的积极作用的观点。当O假装成B向A发送信号时，他利用了形式上的统计规律性，即他观察到的语言形式的分布。无论O学到什么，都是对A和B的交际意图和含义关系的反应。但再现这种分布不足以实现有含义的交流。O只是愚弄了A，让他相信自己是B，因为A是一个积极的听众。由于产生英文句子的人通常有交际意图，所以A假设O也有，因此她按英语常规含义对O的话语进行理解。因为她假设O是B，所以她用这个常规含义和她对B的心理和目标的其他猜测来归因交际意图。这并不是说O的话语有含义，而是说A能够理解这些话语。</p><p><strong>更多受限的思想实验</strong></p><p>章鱼的故事不仅考虑了学习包括关系M和C的完整交际系统的问题，而且还考虑了提出既连贯又对现实世界有帮助的答案所需的推理。在这里，我们提供了两个更有约束性的思想实验，以更严格地关注自然语言和编程语言中学习含义关系的问题。</p><p>由于编程语言被设计成明确的，且对执行环境相对不敏感的语言，因此与自然语言相比，常规含义与说话者含义的区别不那么重要。当Java程序e在Java虚拟机上编译执行时，可以解释为函数i，其将程序输入映射到程序输出。我们取Java的含义关系J⊆E×I来包含所有这样的(e,i)对。</p><p><strong>Java</strong>。想象一下，我们要在Github上发布的所有格式良好的Java代码上训练一个LM。输入仅为代码。它不与字节码、编译器或任何特定程序的样本输入与输出配对。我们可以使用我们喜欢的任何类型的LM，并根据需要对其进行训练。然后我们让模型执行一个样例程序，并期望正确的程序输出。</p><p><strong>英语</strong>。作为第二个例子，想象一下，在英语文本上训练一个LM（同样是任何类型的），同样没有相关的说话者的意图指示。该系统还可以访问大量未标记的照片，但文本和照片之间没有任何联系。对于文本数据，训练任务纯粹是预测形式。对于图像数据，训练任务可以是涉及图像的任何东西。在测试时，我们向模型提供由一句话和一张照片组成的输入，如，“图片上有多少只狗在跳？”或“Kim看到这张照片后说‘好可爱的狗！’”以及图1中的照片，其中适当的答案分别是数字或是照片的一个区域。</p><p><span><img class="small-img-inline" src="/2021/06/21/Climbing%20towards%20NLU%20On%20Meaning,%20Form,%20and%20Understanding%20in%20the%20Age%20of%20Data/clip_image003.png" alt></span></p><p>图1 照片激励1（左）和2（右）</p><p><strong>反思</strong>。在这两种情况下，这些测试皆是荒谬的。考虑到模型的训练内容，要求其执行这些测试显然是不公平的。但这恰恰是我们想要表达的观点：一个已经学会了某种编程语言的含义（语义）的系统知道如何执行该语言的代码。而一个学会了人类语言含义的系统可以做一些事情，如回答用该语言提出的关于世界上（或在某种情况下，在图片中的）的事物问题。</p><p>换而言之，这里有趣的并不是任务是不可能的，而是它们不可能的原因：训练数据中缺少了什么。对于一个没有观察过这些程序的输入和输出的系统来说，Java程序的形式并不包括如何执行它们的信息。同样，英语句子的形式，对于一个没有机会获得英语的含义关系C的系统来说，在没有任何交流意图的信号情况下，不包括任何关于说话者所指语言的外部实体信息。因此，一个只对Java或英语的形式进行训练的系统没有办法学习它们各自的含义关系。</p><p><strong>人类语言习得</strong></p><p>认为LMs 可能学习了含义的常见原因是人类儿童可以通过听语言来获得语言能力的主张。然而这并没有得到有关语言习得的学术工作的支持：相反，我们发现人类语言学习不仅植根于我们周围的物理世界，而且还立足于与这个世界上与其他人的互动。</p><p>同时，语言学习的关键不仅仅是互动，而是真正的共同关注，即孩子和看护人都在关注同一件事并且都意识到这一事实的情况。</p><p>总而言之，习得人类语言的过程，就像一般的人类交流一样，依赖于共同关注和主体间性：意识到另一个人在关注什么并猜测他们打算交流什么的能力。人类的孩子不会仅仅从形式上学习含义，我们也不应该期望机器也能这样做。</p><p><strong>分布式语义</strong></p><p>分布式语义学家早就意识到，将分布式表征建立在现实世界中是困难的。在文本上训练的分布式模型所学习到的词汇相似性关系本身并不能将这些词汇与世界联系起来，且词汇的分布可能与世界上事物的分布不一致。</p><p>从各类文献中，我们常见“含义即使用”的口号，其指的不是”在文本语料库中的分布”的”使用”，而是指语言在现实世界中被用来向真实的人传达交际意图。说话者将他们过去的语言使用经验提炼成我们这里所说的”含义”，并在此基础上产生新的语言使用尝试；如果听者正确推断出说话者的交际意图，这种尝试就是成功的。因此，随着时间的推移，常规含义会随着说话人的不同经验而演变，这种变化的反应可以在不断变化的文本分布中观察到。</p><p><strong>攀登正确的山峰</strong></p><p>那么，那些在非语言建模的任务上进行训练的（例如，语义分析或阅读理解测试）、使用来自BERT或其他大型LM的词嵌入作为组成部分的系统又怎么样呢？过去几年的大量论文表明，使用这种预训练的嵌入可以大幅提高下游系统的准确性，即使是与含义明显相关的任务也是如此。</p><p>我们认为当前有一个相当基本的问题要问：我们是否爬对了山？</p><p><strong>自上而下和自下而上的理论构建</strong></p><p>人们可以从两个不同的角度来看待一个领域的进展。</p><p>从自下而上的角度看，科学界的努力是通过确定具体的研究挑战来推动的。如果一项科学成果至少部分地解决了具体的挑战，那么就算得上是成功。只要这种成功经常出现并令人满意，就会形成持续进步的总体氛围。</p><p>从自上而下的角度看，人们关注的是“为整个领域提供完整、统一的理论”这一遥远的终极目标。这种观点使人对尚未完全解释的所有现象感到焦虑，并提出“自下而上的进展是否能将我们引向正确方向”的问题。</p><p>毫无疑问，NLP正处于快速爬山的过程中。每年，许多NLP任务的技术水平都在显著提高—通常是通过使用更好的预训练LMs—而不久前似乎还不可能完成的任务如今已成为旧闻。因此，当我们采取自下而上的观点时，一切都很好。但从自上而下的角度来看，我们爬得这么快的山是否是正确的山是一个大问题。我们并不知道今天任务的渐进式进展是否会带我们达到最终目标，无论是”通用语言智能”，还是通过图灵测试的系统，亦或是让语言学家满意的英语、阿拉帕霍语、泰语或豪萨语的含义捕捉系统。</p><p><strong>爬山诊断</strong></p><p>我们只有在事后才能确切地知道我们是否爬对了山，但我们提出一些最佳做法，以减少登山时的错误。</p><p>首先，最重要的是，培养对语言的谦逊态度，并提出自上而下的问题。神经方法不是NLP中第一个自下而上的成功，它们可能也不会是最后一个。</p><p>第二，要意识到任务的局限性。</p><p>第三，重视和支持精心创造新任务的工作。</p><p>第四，跨任务的评估含义模型。</p><p>最后，对错误和成功都进行彻底的分析。</p><h2><span id="总结">总结</span></h2><p>在本文中，我们认为，与目前的一些炒作相反，含义不能仅仅从形式中学习。这意味着，即使是像BERT这样的大型语言模型也不能学习“含义”；它们学习的是含义在语言形式中的一些反映，这在应用中非常有用。 我们就如何对建立在这些LM上的研究保持健康但不夸张的乐观态度提出了一些想法。特别的，本文可以被看作在是呼吁在谈论当前模型的成功时使用精确的语言，并呼吁在处理自然语言时保持谦逊。我们希望以此来鼓励人们从自上而下的角度来看待我们的领域，我们认为这将有助于我们爬上正确的山，向人类模拟NLU攀登。</p>]]></content>
    
    
    <summary type="html">&lt;img src=&quot;/2021/06/21/Climbing%20towards%20NLU%20On%20Meaning,%20Form,%20and%20Understanding%20in%20the%20Age%20of%20Data/clip_image002.jpg&quot; alt&gt;

&lt;h1 id=&quot;迈向NLU：关于数据时代的含义、形式和理解&quot;&gt;&lt;a href=&quot;#迈向NLU：关于数据时代的含义、形式和理解&quot; class=&quot;headerlink&quot; title=&quot;迈向NLU：关于数据时代的含义、形式和理解&quot;&gt;&lt;/a&gt;迈向NLU：关于数据时代的含义、形式和理解&lt;/h1&gt;&lt;h2 id=&quot;引用&quot;&gt;&lt;a href=&quot;#引用&quot; class=&quot;headerlink&quot; title=&quot;引用&quot;&gt;&lt;/a&gt;引用&lt;/h2&gt;&lt;p&gt;Bender E M, Koller A. Climbing towards NLU: On meaning, form, and understanding in the age of data[C]&amp;#x2F;&amp;#x2F;Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 2020: 5185-5198.&lt;/p&gt;
&lt;h2 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要&quot;&gt;&lt;/a&gt;摘要&lt;/h2&gt;&lt;p&gt;大型神经语言模型在许多NLP任务上的成功令人激动。然而我们发现，这些成功有时是就像是在炒作，它们将模型描述成“理解”语言或捕捉“含义”。在本文立场中，我们认为仅在形式上训练的系统先验地无法学习含义。与ACL 2020的主题“通观现状与展望未来”一致，我们认为，清楚地理解形式和含义之间的区别，将有助于推进围绕自然语言理解的科学进步。&lt;/p&gt;</summary>
    
    
    
    <category term="论文阅读" scheme="https://values.keys.moe/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="论文阅读" scheme="https://values.keys.moe/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
  </entry>
  
  <entry>
    <title>新的故事与旧的回忆 —— 2017~2021 我的本科四年</title>
    <link href="https://values.keys.moe/2021/06/21/%E6%96%B0%E7%9A%84%E6%95%85%E4%BA%8B%E4%B8%8E%E6%97%A7%E7%9A%84%E5%9B%9E%E5%BF%86%20%E2%80%94%E2%80%94%202017~2021%20%E6%88%91%E7%9A%84%E6%9C%AC%E7%A7%91%E5%9B%9B%E5%B9%B4/"/>
    <id>https://values.keys.moe/2021/06/21/%E6%96%B0%E7%9A%84%E6%95%85%E4%BA%8B%E4%B8%8E%E6%97%A7%E7%9A%84%E5%9B%9E%E5%BF%86%20%E2%80%94%E2%80%94%202017~2021%20%E6%88%91%E7%9A%84%E6%9C%AC%E7%A7%91%E5%9B%9B%E5%B9%B4/</id>
    <published>2021-06-20T16:54:45.000Z</published>
    <updated>2022-09-28T06:55:23.539Z</updated>
    
    <content type="html"><![CDATA[<img src="/2021/06/21/%E6%96%B0%E7%9A%84%E6%95%85%E4%BA%8B%E4%B8%8E%E6%97%A7%E7%9A%84%E5%9B%9E%E5%BF%86%20%E2%80%94%E2%80%94%202017~2021%20%E6%88%91%E7%9A%84%E6%9C%AC%E7%A7%91%E5%9B%9B%E5%B9%B4/新的故事与旧的回忆.jpg" alt><span id="more"></span>]]></content>
    
    
    <summary type="html">&lt;img src=&quot;/2021/06/21/%E6%96%B0%E7%9A%84%E6%95%85%E4%BA%8B%E4%B8%8E%E6%97%A7%E7%9A%84%E5%9B%9E%E5%BF%86%20%E2%80%94%E2%80%94%202017~2021%20%E6%88%91%E7%9A%84%E6%9C%AC%E7%A7%91%E5%9B%9B%E5%B9%B4/新的故事与旧的回忆.jpg&quot; alt&gt;</summary>
    
    
    
    <category term="杂谈" scheme="https://values.keys.moe/categories/%E6%9D%82%E8%B0%88/"/>
    
    
    <category term="杂谈" scheme="https://values.keys.moe/tags/%E6%9D%82%E8%B0%88/"/>
    
  </entry>
  
  <entry>
    <title>Debugging Tests for Model Explanations</title>
    <link href="https://values.keys.moe/2021/06/20/Debugging%20Tests%20for%20Model%20Explanations/"/>
    <id>https://values.keys.moe/2021/06/20/Debugging%20Tests%20for%20Model%20Explanations/</id>
    <published>2021-06-19T19:51:15.000Z</published>
    <updated>2023-10-25T16:08:00.844Z</updated>
    
    <content type="html"><![CDATA[<img src="/2021/06/20/Debugging%20Tests%20for%20Model%20Explanations/clip_image002.jpg" alt><h1><span id="模型解释的调试测试">模型解释的调试测试</span></h1><h2><span id="引用">引用</span></h2><p>Adebayo J , Muelly M , Liccardi I , et al. Debugging Tests for Model Explanations[J]. 2020.</p><h2><span id="摘要">摘要</span></h2><p>我们研究了事后模型解释对于诊断模型错误（即模型调试）是否是有效的。为了应对解释模型预测的挑战，人们提出了大量解释方法。尽管人们越来越多地使用这些方法，但仍不清楚它们是否有效。首先，我们根据bug的来源，将其分为：数据、模型和测试时污染bug。对于几个解释方法，我们评估了它们的能力：检测虚假的关系产物（数据污染），诊断错误标注的训练样本（数据污染），区分（部分）重新初始化的模型和训练好的模型（模型污染），以及检测分布外的输入（测试时污染）。我们发现，所测试的方法能诊断出一个虚假的背景错误，但并不能确凿地识别错误标注的训练样本。此外，一类修改的反向传播算法的方法对深度网络的高层参数没有影响；因此，其对诊断模型污染无效。我们进行了一项人群受试者研究来补充我们的分析，我们发现受试者未能根据归因来识别有缺陷的模型，而主要依赖于模型预测。综上，我们的结果为从业者和研究者提供了在模型测试时将解释作为工具的指导。</p><span id="more"></span><h2><span id="1介绍">1.介绍</span></h2><p>诊断与修复模型错误（即模型调试）仍是机器学习的长期挑战。模型调试越来越重要，因为带有学习组件的自动化系统正在高风险设置中测试，不经意的错误可能会导致破坏性的后果。越来越多的解释——源于训练好的模型的产物，其主要目的是向用户提供模型分析的见解——作为模型的调试工具被用于诸多专业领域中。尽管已有大量的解释方法被广泛地应用于调试中，但尚未有对方法有效性的指南。例如，一个解释对于诊断错误标注的训练样本和检测虚假的关系产物是否应该同样有效？对模型参数敏感的解释是否在检测域迁移后也同样有效？因此，我们提出并解决如下问题：</p><p>哪种解释方法对哪种模型错误有效？</p><p>为了解决此问题，我们做出如下贡献：</p><p>\1. <strong>错误分类</strong>。根据导致错误的缺陷来源，我们将监督学习流程中的错误分类（如图1）分为三类：数据、模型和测试时污染。这些污染类别捕捉训练数据缺陷、模型结构与参数缺陷与测试时输入缺陷。</p><p><span><img class="small-img-inline" src="/2021/06/20/Debugging%20Tests%20for%20Model%20Explanations/clip_image004.jpg" alt></span></p><p>图1 标准监督学习流程的调试框架。标准监督学习流程的示意图和流程每个阶段可能发生的错误样例。这样的分类捕捉到了可能发生在训练数据、模型与测试时的错误。我们称这些为：数据、模型与测试时污染测试。</p><p>\2. <strong>经验性评估</strong>。我们进行了全面的控制实验，以评估几种特征归因方法对四种错误的影响：“虚假的关系产物”，错误标注的训练样本，重新初始化的权重和外分布（OOD）迁移。</p><p>\3. <strong>见解</strong>。我们发现，测试的特征归因方法可以识别虚假背景错误，但不能确凿地区分正常和错误标注的训练样本。此外，通过“正向聚合”修改的反向传播计算得出相关性的归因方法对于深度神经网络（DNN）模型的高层参数是不变的。最后，我们发现，在特定设置中，分布外的样本的归因在视觉上与“域内”模型的样本相似，这表明了仅基于视觉检查的调试可能产生误导。</p><p>\4. <strong>人群受试者研究</strong>。我们进行了一项54人的IRB批准的研究，以评估终端用户是否能通过归因识别有缺陷的模型。我们发现，即使在有归因的情况下，用户仍主要依靠模型预测来确定一个模型是否是有缺陷的。</p><h2><span id="2错误特征-解释方法与用户研究">2.错误特征、解释方法与用户研究</span></h2><h3><span id="21确定模型错误">2.1确定模型错误</span></h3><p>我们将模型错误定义为学习和&#x2F;或预测流程中的污染，导致模型产生错误的预测或导致模型学习错误的关系。我们将注意力集中在标准监督学习设置上，并根据其来源对错误进行分类。给定输入标签{xi,yi}i n，其中x∈X、y∈Y，分类器的目标是学习泛化函数fθ：X→Y。fθ被用于预测测试样本，xtest∈X，则ytest&#x3D;fθ(xtest)。给定一个损失函数L和模型参数θ，则对于一系列模型，我们提供了错误的分类，即模型、数据与测试时污染：</p><p><span><img class="small-img-inline" src="/2021/06/20/Debugging%20Tests%20for%20Model%20Explanations/clip_image005.png" alt></span></p><p><strong>数据污染错误</strong>是由训练数据中的缺陷引起的，要么是输入特征，要么是标签，要么两者皆有。例如，一些不正确的标签会导致模型学习到错误的关系。另一个错误是虚假的关系训练信号。例如，在一个物体分类任务中，所有的鸟都出现在蓝天背景下。在这个数据集上训练的模型会将蓝天背景与鸟类相关联；这样的数据集偏差在实践中常常发生。</p><p><strong>模型污染错误</strong>是由模型参数的缺陷引起的。例如，代码中的错误可以导致模型权重的意外初始化。</p><p><strong>测试时污染的错误</strong>是由测试输入的缺陷引起的，如测试时的域迁移或不匹配的预处理。</p><p>上述错误分类使我们能针对具体错误类别评估解释，并能让我们阐述解释方法何时能对特定错误类别有效。如表1所示，我们评估了一系列应用于具有每个错误特定实例的模型的解释方法。</p><p><span><img class="small-img-inline" src="/2021/06/20/Debugging%20Tests%20for%20Model%20Explanations/clip_image007.jpg" alt></span></p><p>表1 我们为每个错误类别测试的错误样本及它们的公式化描述</p><h3><span id="22解释方法">2.2解释方法</span></h3><p>我们着力于为模型输出的对应输入维度提供“相关性”分数的特征归因方法。对于在图像数据上训练的深度神经网络（DNN），可以将特征相关性可视化为热力图，如图2所示。</p><p><span><img class="small-img-inline" src="/2021/06/20/Debugging%20Tests%20for%20Model%20Explanations/clip_image009.jpg" alt></span></p><p>图2 考虑的归因方法。图中展示了为了区分鸟和狗而训练的CNN模型的两个输入的特征归因。</p><p>归因函数<span><img class="small-img-inline" src="/2021/06/20/Debugging%20Tests%20for%20Model%20Explanations/clip_image011.png" alt></span>、输出Fk(x)映射到归因图Mxi∈Rd上。</p><p><strong>梯度（Grad）与变量</strong>。我们考虑1）Gtadient(Grad)图，|▽xiFi(xi)|；2）SmoothGrad（SGrad），<span><img class="small-img-inline" src="/2021/06/20/Debugging%20Tests%20for%20Model%20Explanations/clip_image015.png" alt></span>；7）Expected Gradients（EGrad），其计算IntGrad，但其有一个对训练集期望的基线输入。</p><p><strong>替代方法</strong>。LIME和SHAP使用一个简单函数g对xi周围的F进行局部近似。SHAP提供了一个对Shapley值的可行近似。</p><p><strong>修改的反向传播</strong>。这类方法给输出分配“相关性”分数，对每个输入维度使用反向传播。DConvNet和Guided Back-propagation（GBP）修改ReLU单元的梯度。Layer-wise relevance propagation （LRP）方法指定了修改反向传播的“相关性”规则。我们考虑LRPEPS和LRP sequential preset-a-flat （LRP-SPAF）。PatternNet（PNet） 和Pattern Attribution （PAttribution）将输入分解为信号和噪声部分，并反向传播信号部分的相关性。</p><p><strong>归因比较</strong>。我们分别用结构相似性指数（SSIM）和Spearman等级相关度指标来衡量视觉和特征排名相似度。</p><h3><span id="23人群受试者研究概述">2.3人群受试者研究概述</span></h3><p><strong>任务与设置</strong>。我们设计了一项研究，其用以测量终端用户使用特征归因来评估分类模型可靠性的能力。参与者被要求作为一家销售动物分类模型的假定企业的质保（QA）测试员，并每次向他们展示原始图像、模型预测和4个狗品种的归因图。然后，他们用5点里克特量表对他们向外部客户推荐销售该模型的可能性进行评分，并说明他们决定的理由。参与者从4个预先创建的答案中选择（图5-b），或填写一个自由格式的答案。参与者自我报告了他们的机器学习专业知识水平，这一点通过3个问题得到了验证。</p><p><strong>方法</strong>。我们将重点放在研究一个有代表性的方法子集上：Gradient、Integrated Gradients、SmoothGrad。</p><p><strong>错误</strong>。我们在没有错误的模型上测试了表1中描述的错误。</p><h2><span id="3调试数据污染">3.调试数据污染</span></h2><p><strong>概述</strong>。我们评估了特征归因是否能检测出虚假的训练产物和错误标注的训练样本。虚假的产物是指在训练集中编码或受其标签影响的信号，但它们不能为数据生成过程提供有意义的联系。我们在输入背景中诱发一个虚假的关系，并测试特征归因能否诊断出这种影响。我们发现，所考虑的方法确实将重要性归因于具有虚假信号的输入图像背景。然而，尽管归因中有可视化证据，但人群受试者研究中的参与者不确定虚假模型条件下模型的可靠性。因此，参与者并未完全否定该模型。</p><p><span><img class="small-img-inline" src="/2021/06/20/Debugging%20Tests%20for%20Model%20Explanations/clip_image017.jpg" alt></span></p><p>图3 虚假关系错误的特征归因。图中显示了在虚假数据上训练的BVD-CNN的4个输入的归因。A和B显示了两个狗的样本，C和D是鸟的样本。第一行显示了带有虚假背景的输入（狗或鸟）的归因。第二行显示的是只有虚假背景的归因。值得注意的是，我们观察到特征归因方法都强调了背景。衡量标准见表2。</p><p>对于错误标注的样本，我们比较来自以下训练输入的归因：1）训练时输入具有正确标签的模型；2）相同的模型设置，但使用错误的标签进行训练。如果这两种设置下的归因相似，那么这种方法不太可能用于识别错误标注的样本。我们观察到，所有方法中错误标注样本的归因都显示出视觉相似性。</p><p><strong>数据与模型设置</strong>。我们考虑了一个鸟与狗的二元分类任务。我们使用来自Cats-v-Dogs数据集的狗与来自Caltech-UCSD数据集的鸟。在此数据集上，我们训练了一个具有5个卷积层和3个全连接层的CNN（从这里开始我们把这个架构称为BVD-CNN），其使用ReLU函数，但最后一层是sigmoid。该模型达到了94%的测试准确度。</p><h3><span id="31虚假关系训练产物">3.1虚假关系训练产物</span></h3><p><strong>虚假错误实现</strong>。我们通过将所有的鸟放在地点数据集中的一个天空背景上，并将所有的狗放在竹林背景上（见图3）来引入虚假相关性。对这些数据进行训练的BVD-CNN在天空与竹林测试集（没有鸟或狗）上达到了97%的准确率，表明该模型确实学习到了虚假关系。</p><p><strong>结果</strong>。为了定量地衡量归隐方法是否反应了虚假的背景，我们将归因与两个准确标注的遮罩（GT-1和GT-2）进行比较。如图4所示，我们考虑一个理想的遮罩，其将所有的相关性都分配给了背景，而不分配给物体部分。接下来，我们考虑一个宽松的版本，其通过没有对象的虚假背景归因来加权准确标注的遮罩。在表2中，我们报告了所有方法对这两种准确标注的遮罩的SSIM比较得分。对于GT-2，分数范围从最低的0.78到最高的0.98；这提供证据表明归因识别了虚假的背景信号。我们为GT-1找到了类似的证据。</p><p><strong>来自人群受试者研究的启示</strong>：用户是不确定的。图5展示了人群受试者研究的结果，我们评估了终端用户可靠地使用归因来识别依赖虚假训练集信号的模型的能力。对于一个正常的模型，Gradient、SmoothGrad和Integrated Gradients的李克特分数中值分别为4、4、3。选择1的李克特分数表示用户“肯定不会”推荐这个模型，而5意味着用户“肯定”推荐这个模型。因此，用户对一个正常的模型有充分的评价。此外，对于对Gradient和SmoothGrad，分别有30%和40%的参与者（见图5-右）表示，正常模型的归因“突出了他们希望它关注的图像部分”。</p><p><span><img class="small-img-inline" src="/2021/06/20/Debugging%20Tests%20for%20Model%20Explanations/clip_image019.jpg" alt></span></p><p>表2 具有虚假背景的输入和真实遮罩的输入两者的归因遮罩之间的相似性。SSIM-GT1测量理想虚假输入遮罩和GT-1之间的视觉相似性，如图4所示。SSIM-GT2测量GT-2的视觉相似性。我们亦涵盖了每个指标的平均值标准误差（SEM），这是跨190个输入计算的。为了校准该指标，随机采样的高斯归因和虚假归因之间的平均SSIM为3e-06。</p><p><span><img class="small-img-inline" src="/2021/06/20/Debugging%20Tests%20for%20Model%20Explanations/clip_image021.jpg" alt></span></p><p>图5 A：用户研究中参与者的反应。参与者对3种归因方法（Gradient, SmoothGrad, and Integrated Gradients）的反应箱线图和测试的5个模型条件。纵轴上是李克特量表，从1（肯定不），到5（肯定）。参与者被告知，如果他们认为狗与鸟的分类模型可以被出售给客户，则选择“肯定”。B：选择的动机。参与者选择推荐的动机（%）。如图例所示，用户可以从4个选项中选择一个，或插入一个开放式的回答。</p><p>对于“虚假模型”，李克特分数显示的数据分布范围更广。虽然Gradient、SmoothGrad和Integrated Gradients的中位数分别为2、2、3，但一些终端用户仍然推荐这种模式。对于每一种归因类型，大多数终端用户表示，归因“没有突出我期望它关注的图像部分”。尽管如此，终端用户并没有像对其他错误条件那样直接否定虚假模型。这些结果表明，归因方法诊断虚假相关性的能力可能无法用于可靠决策中。</p><h3><span id="32错误标注的训练样本">3.2错误标注的训练样本</span></h3><p><strong>错误实现</strong>。我们在鸟类和狗类的数据集上训练了一个BVD-CNN模型，其中10%的训练样本标签被交换。该模型在训练、验证和测试集上达到了93.2%、91.7%和88%的准确率。</p><p><strong>结论</strong>。我们发现，对于一个有缺陷的模型而言，来自错误标注的样本的归因在视觉上与来自正确输入标签的这些相同样本的归因相似（如图6中的样本）。我们发现，在所有测试方法中，正确标注的实例和相应错误标注的实例之间的SSIM都在0.73-0.99之间。这些结果表明，所测试的归因方法对于识别错误标注的样本可能是无效的。</p><p><strong>来自人群受试者研究的启示</strong>：用户使用预测标签，而非归因方法。与虚假设置相反，参与者否定了错误标注的样本，对Gradient、SmoothGrad和Integrated Gradients的李克特评分中位数分别为1、2和1。然而，我们发现，这些参与者绝大多数都是依靠模型的预测来做出决定。</p><p><span><img class="small-img-inline" src="/2021/06/20/Debugging%20Tests%20for%20Model%20Explanations/clip_image023.jpg" alt></span></p><p>图6 诊断错误标注的训练样本。该图显示了两个训练输入以及每种方法的特征归因。正确标签行对应于基于正确标签的训练集训练的模型所得出的特征归因。错误标签行显示了使用带有错误标签的训练集训练的模型所得出的特征归因。我们看到，两种设置下的归因在视觉上看是相似的。</p><h2><span id="4调试模型污染">4.调试模型污染</span></h2><p>我们接下来评估与模型参数有关的错误。具体而言，我们考虑的是模型权重在预测前被意外重新初始化的情况。我们发现，修改过的反向传播算法，如Guided Back-Propapagtion （GBP）、DConvNet和LRP相关变体，包括Pattern Net（PNet）和Pattern Attribution（PAttribution），对深度网络的更高层权重是不变的。</p><p><strong>错误实现</strong>。我们在ImageNet上的预训练的VGG-16模型上实例化了这个错误。我们从顶层开始重新初始化模型权重，直到第一层。然后，我们将这些部分重新初始化的模型的归因与原始模型得出的归因进行比较。</p><p><span><img class="small-img-inline" src="/2021/06/20/Debugging%20Tests%20for%20Model%20Explanations/clip_image025.jpg" alt></span></p><p>图7 在ImageNet上训练的VGG-16模型的连续权重更新的重新初始化对应的模型归因的演变。定性结果（左）与定量结果（右）。定性结果的最后一列对应完全重新初始化权重的网络。</p><p><strong>结论</strong>：修改的反向传播方法是参数不变的。从图7可以看出，包括Guided BackProp、Deconvnet、DeepTaylor、PatternNet、Pattern Attribution和LRP-SPAF在内的一类修改的反向传播方法在视觉上和数量上对VGG-16模型的高层参数都是不变的。</p><p><strong>来自人群受试者研究的启示</strong>：用户使用预测标签，而非归因方法。我们观察到，参与者断然否定一个顶层已被重新初始化的模型，而该判断大多纯粹基于分类标签，而很少基于错误的归因（图5）。</p><h2><span id="5调试测试时污染">5.调试测试时污染</span></h2><p>当输入的分布特征与训练集不同时，模型可能提供错误的预测。为了评估特征归因，诊断域迁移的能力，对于给定输入，我们比较了从域内模型和从域外模型得出的归因。例如，我们分别在MNIST上训练的模型、Fashion MNIST上训练的模型、ImageNet上训练的模型和鸟-狗分类模型上，比较了同一个MNIST数字所得出的归因。我们发现在某些情况下有视觉上的相似性：如相同的Fashion MNIST输入样本，从ImageNet上训练的VGG-16模型得出的特征归因与在Fashion MNIST上训练的模型得出的特征归因在视觉上相似。然而，输入维度的定量排名却大不相同。</p><p><span><img class="small-img-inline" src="/2021/06/20/Debugging%20Tests%20for%20Model%20Explanations/clip_image027.jpg" alt></span></p><p>图8 在多个模型上的Fashion MNIST OOD。第一行显示了在Fashion MNIST上训练的模型的特征归因。在接下来几行中，我们展示了同一输入在MNIST模型上的特征归因、在鸟-狗分类上训练的BVD-CNN模型的特征归因以及在ImageNet上预训练的VGG-16模型的特征归因。</p><p><span><img class="small-img-inline" src="/2021/06/20/Debugging%20Tests%20for%20Model%20Explanations/clip_image029.jpg" alt></span></p><p>表3 测试时解释相似度度量。我们可以观察到视觉上的相似度，但无排名上的相似度。我们将每个指标与对190个样本计算出的平均值的标准误差一起展示。FMNIST-&gt;MNIST模型是指对一个FMNIST模型的FMNIST归因与从MNIST模型得到的FMNIST归因的比较。我们同时展示了SSIM和Rank的相关度量。</p><p><strong>错误实现</strong>。我们考虑了4个数据集-模型对：在MNIST上训练的BVD-CNN、Fashion MNIST、鸟-狗数据集和在ImageNet上训练的VGG-16模型。</p><p><strong>结论</strong>。如图8所示，我们观察到域内Fashion MNIST归因，以及这些样本在其他模型上的归因之间的视觉相似性。如表3所示，我们观察到视觉上的相似性，尤其是ImageNet上的VGG-16模型，但在特征排名上基本不相关。</p><p><strong>来自人群受试者研究的启示</strong>：用户使用预测标签，而非归因。对于域迁移的研究，我们向参与者展示了在训练过程中没有使用的狗的归因，每个训练好的模型对其品种的预测皆不同。我们发现，在这种情况下，由于错误的预测标签，用户不会推荐该模型（图5）。</p><h2><span id="6讨论与结论">6.讨论与结论</span></h2><p>调试机器学习模型仍然是一项具有挑战性的工作，而模型解释可能是这项探索中的一个有用工具。尽管从业者和研究者可能可以使用很多解释方法，但仍不清楚哪些方法对什么类型的错误有用。本工作旨在解决此问题，首先，将模型错误分为：数据、模型和测试时污染错误，然后针对每种错误类型测试特征归因方法。总的来说，我们发现特征归因方法能诊断测试出空间的虚假关系错误，但不能确凿地区分错误标注样本与正常样本。在模型污染的情况下，我们发现某些使用正向聚合的修改后的反向传播的计算特征归因方法，对深层模型的高层参数是不变的。这表明，这些方法可能对诊断模型污染的错误并不有效。我们还发现，域外输入的归因与域内模型上这些输入的归因相似，这表明在视觉上检查这些解释时要谨慎，尤其是对于图像任务。我们还进行了人群受试者研究，以评估终端用户能多好地使用归因来评估模型可靠性。在此，我们发现最终用户主要靠模型预测来诊断模型错误。</p><p>我们的发现有一定的局限性与注意事项。所提出的错误描述仅覆盖了标准的监督学习流程，可能无法完整地捕捉到各类因素组合而成的错误。我们仅关注特征归因：然而，其他方法，如基于“概念”激活、模型表征剖析、训练点排序等方法可能更适合这里研究的调试任务。对于人群受试者研究，我们发现参与者大多依赖标签而不是特征归因，这可能是狗品种分类任务的结果。对于那些他们没有专业知识或先验知识的任务，参与者是否还会依赖模型的预测目前尚不清楚。</p><p>本工作的目的是为研究人员和从业者提供在模型调试时使用特征归因的指导。希望我们的发现可以作为第一步，为评估解释方法的效用提供更为严格的方法。</p>]]></content>
    
    
    <summary type="html">&lt;img src=&quot;/2021/06/20/Debugging%20Tests%20for%20Model%20Explanations/clip_image002.jpg&quot; alt&gt;

&lt;h1 id=&quot;模型解释的调试测试&quot;&gt;&lt;a href=&quot;#模型解释的调试测试&quot; class=&quot;headerlink&quot; title=&quot;模型解释的调试测试&quot;&gt;&lt;/a&gt;模型解释的调试测试&lt;/h1&gt;&lt;h2 id=&quot;引用&quot;&gt;&lt;a href=&quot;#引用&quot; class=&quot;headerlink&quot; title=&quot;引用&quot;&gt;&lt;/a&gt;引用&lt;/h2&gt;&lt;p&gt;Adebayo J , Muelly M , Liccardi I , et al. Debugging Tests for Model Explanations[J]. 2020.&lt;/p&gt;
&lt;h2 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要&quot;&gt;&lt;/a&gt;摘要&lt;/h2&gt;&lt;p&gt;我们研究了事后模型解释对于诊断模型错误（即模型调试）是否是有效的。为了应对解释模型预测的挑战，人们提出了大量解释方法。尽管人们越来越多地使用这些方法，但仍不清楚它们是否有效。首先，我们根据bug的来源，将其分为：数据、模型和测试时污染bug。对于几个解释方法，我们评估了它们的能力：检测虚假的关系产物（数据污染），诊断错误标注的训练样本（数据污染），区分（部分）重新初始化的模型和训练好的模型（模型污染），以及检测分布外的输入（测试时污染）。我们发现，所测试的方法能诊断出一个虚假的背景错误，但并不能确凿地识别错误标注的训练样本。此外，一类修改的反向传播算法的方法对深度网络的高层参数没有影响；因此，其对诊断模型污染无效。我们进行了一项人群受试者研究来补充我们的分析，我们发现受试者未能根据归因来识别有缺陷的模型，而主要依赖于模型预测。综上，我们的结果为从业者和研究者提供了在模型测试时将解释作为工具的指导。&lt;/p&gt;</summary>
    
    
    
    <category term="论文阅读" scheme="https://values.keys.moe/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="论文阅读" scheme="https://values.keys.moe/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
  </entry>
  
  <entry>
    <title>Prioritize Crowdsourced Test Reports via Deep Screenshot Understanding</title>
    <link href="https://values.keys.moe/2021/04/20/Prioritize%20Crowdsourced%20Test%20Reports%20via%20Deep%20Screenshot%20Understanding/"/>
    <id>https://values.keys.moe/2021/04/20/Prioritize%20Crowdsourced%20Test%20Reports%20via%20Deep%20Screenshot%20Understanding/</id>
    <published>2021-04-20T09:24:05.000Z</published>
    <updated>2023-10-25T15:05:16.658Z</updated>
    
    <content type="html"><![CDATA[<img src="/2021/04/20/Prioritize%20Crowdsourced%20Test%20Reports%20via%20Deep%20Screenshot%20Understanding/title.png" alt><h1><span id="基于深度截图理解的众包测试报告优先级排序">基于深度截图理解的众包测试报告优先级排序</span></h1><h2><span id="摘要">摘要</span></h2><p>​众包测试在移动应用程序测试中日益占据主导地位，但对于应用开发者来说，审查数量过多的测试报告是很大的负担。已有许多学者提出基于文本和简单图片特征的测试报告处理方法。然而，在移动应用测试中，测试报告所包含的文本较为精简且信息不够充分，图片则能够提供更丰富的信息。这一趋势促使我们在深度截图理解的基础上，对众包测试报告的优先级进行排序。<br>​本文中，我们提出了一种新的众包测试报告优先级排序方法，即DEEPPRIOR。我们首先引入一个新的特征来代表众包测试报告，即DEEPFEATURE，它基于对应用程序截图的深度分析，涵盖了所有组件（widget）及它们的文本、坐标、类型甚至是意图。DEEPFEATURE包括直接描述bug的bug特征（Bug Feature），和刻画bug完整上下文的上下文特征（Context Feature）。DEEPFEATURE的相似度用于表示测试报告的相似度，并被用于对众包测试报告进行优先级排序。我们形式上将相似度定义为DEEPSIMILARITY。我们还进行了一个实证实验，以评估所提技术在大型数据中的有效性。结果表明，DEEPPRIOR性能最佳，以不足一半的成本获得优于其它方法的结果。</p><p>索引词 众包测试，移动应用测试，深度截图理解</p><span id="more"></span><h2><span id="i-介绍">I. 介绍</span></h2><p>​众包已成为许多领域的主流技术之一。众包的开放性带来了许多优势。例如，可以在多个不同的实际环境中模拟对众包活动的操作。这样的优势有助于缓解移动应用（app）测试中严重的“碎片化问题”。成千上万种不同品牌、不同操作系统（OS）版本、不同硬件传感器就是安卓测试中众所周知的“碎片化问题”[1]。众包测试是解决这一问题的最佳方案之一。应用开发者可以将它们的应用程序分发给拥有不同移动设备的众包工人，并要求他们提交包含应用截图和文本描述的测试报告。这有助于应用开发者尽可能多的发现问题。<br>​然而，众包测试的报告审查效率低是一个严重的问题。众包的开放性会导致有大量的报告被提交，而几乎82%的提交的报告都是重复的[2]。由于报告的复杂性，自动审查报告是一项艰巨的工作。在文字部分，自然语言的复杂性可能导致歧义，并且众包工人可能使用不同的词来描述相同的对象，或使用相同的词来描述不同的场景。在图像部分，由于许多应用使用相似的UI构建函数，截图的相似度也几乎没有帮助。因此，对于应用程序开发人员来说，及时发现报告中的bug是困难但重要的。<br>​在近期的研究中，测试报告的处理通常分为两部分：应用截图和文本描述。现有的研究分别对这两部分进行分析以提取特征。对于文本描述，现有的方法是提取关键词，并根据预定义的词汇对关键词进行标准化处理。对于应用截图，它们将每个截图作为一个整体，提取用数字向量表示的图像特征。在获得了这两部分的结果之后，目前大多数的研究都以文本为依托，将截图作为补充材料，或者简单地将图像信息和文本信息进行拼接。然而，我们认为这样的处理方式会导致许多有价值的信息丢失。文本描述和应用截图之间的关系会被遗漏，报告的去重与确定优先级效果也可能更差。<br>​在本文中，我们提出了一种新颖的方法，即DEEPPRIOR，通过对截图的深入理解来确定众包测试报告的优先级。DEEPPRIOR详细顾及了对应用截图和文本描述的深入理解。对于一份被提交的测试报告，我们会从截图和文本两者中提取信息。在截图中，我们通过计算机视觉（CV）技术收集所有widget，并根据文本描述定位问题widget（表示为WP）。其余的widget则被视为上下文widget（表示为WC）。本研究以自然语言处理（NLP）技术对文本进行处理，并分为两部分：复现步骤（以R表示）和bug描述（以P表示）。复现步骤被进一步标准化为“操作-对象”序列。bug描述也被进一步处理以提取对问题widget的描述，从而进行WP的定位。<br>​我们不对应用截图和文本描述进行单独处理，而是将它们作为一个整体，并将所有信息收集起来作为报告的DEEPFEATURE。根据bug本身的关联性，DEEPFEATURE包括bug特征（Bug Feature，BFT）和上下文特征（Context Feature，CFT）。bug特征由WP和P组成，它表示报告中揭示的和bug直接相关的信息。上下文特征由WC和R组成，其代表的是上下文信息，包括触发bug的操作轨迹和bug发生时的activity的信息。<br>​将上述特征整合到DEEPFEATURE中后，DEEPPRIOR将计算报告中的DEEPSIMILARITY以进行优先级排序。对于bug特征和上下文特征，我们分别计算DEEPSIMILARITY。<br>​对于bug特征，为了计算报告中的WP的DEEPSIMILARITY，我们利用CV技术对特征点进行提取和匹配。P是一个简短的文本描述，因此我们使用NLP技术，在自建词汇表的基础上提取与bug有关的关键词，比较关键词的频率作为DEEPSIMILARITY。<br>​对于上下文特征，WC被输入到一个预先训练好的深度学习分类器中，以识别每个widget的类型，每个类型的数字向量作为WC DEEPSIMILARITY。R由一系列操作和对应的widget组成，代表了从应用启动到bug发生的序列。因此，我们按照R的顺序，利用NLP技术提取操作和对象。我们将“操作-对象”序列作为行为轨迹，并计算DEEPSIMILARITY。<br>​之后进行优先级排序。我们首先构建一个NULL 报告（如章节III-D中定义），并将其添加到优先级报告池中。之后，我们反复计算每个未确定优先级的报告和报告池中所有报告之间的DEEPSIMILARITY。具有与优先级报告池中相比最低的“最小DEEPSIMILARITY”的报告会被放入优先级报告池中。<br>​我们还设计了一个实证实验，使用一个大型活跃的众包测试平台的大规模数据集组。我们将DEEPPRIOR与其他两种方法进行了比较，结果表明，DEEPPRIOR是有效的。<br>​本文的重要贡献如下：<br>​* 我们提出了一个新颖的方法，通过对截图的深入理解和详细的文本分析，对众包测试报告进行优先级排序。我们从截图中提取所有widget，将文本信息分类到不同的类别，并构建DEEPFEATURE。<br>​* 我们构建了一个用于对截图进行深入理解的集成数据集组，包括大规模的widget图像数据集、大规模的测试报告关键词词汇、大规模的文本分类数据集和大规模的众包测试报告数据集。<br>​* 基于数据集，我们对提出的方法DEEPPRIOR进行了实证评估，结果表明，DEEPPRIOR以不到一半的开销胜过了当前最新的方法。</p><h2><span id="ii-背景与动机">II. 背景与动机</span></h2><p>​众包测试在移动应用测试中得到了广泛的应用，其优点是显而易见的，但其弊端也是不可忽视的。在大多数主流的众测平台上，众测工人都需要提交一份报告来描述自己所遇到的bug。报告的主题是对bug的截图和文本描述。应用的截图和文本描述也同样是对众测报告进行优先级排序的主要依据。<br>​目前考虑截图的众测报告处理方案，如[2][3]主要是分析应用截图特征和文本描述信息来衡量所有报告之间的相似度。虽然他们考虑了应用截图，但只是将图片简单地处理为宽x高xRGB的矩阵。然而，这些方法忽略了丰富而有价值的信息，我们认为应该把应用截图看作是有意义的widget的集合，而非无意义的像素的集合。这是因为在回顾众测报告数据集的时候，我们发现了一些生动的例子，现有的方法难以处理它们，因为这些方法只是做了简单的特征提取，而没有对截图进行深度理解。</p><p>A. 样例1：不同的应用主题<br>        现在的应用都支持不同的主题，用户可以根据自己的喜好定制应用的外观（图1）。此外，“深色模式”使得配色方案更加复杂。图像特征提取算法很难处理这样的复杂问题，会出现错误。从这些样本中我们可以发现，三份报告中的应用截图分别为蓝色、白色和绿色主题。这三份报告都是报告音乐资源文件加载失败的。然而，根据文献[2]，图像颜色特征是报告替代的重要组成部分之一。不同颜色的应用截图将被识别为不同的截图。<br><span><img class="small-img-inline" src="/2021/04/20/Prioritize%20Crowdsourced%20Test%20Reports%20via%20Deep%20Screenshot%20Understanding/图1.png" alt></span><br>图1 样例1：不同的应用主题<br><span><img class="small-img-inline" src="/2021/04/20/Prioritize%20Crowdsourced%20Test%20Reports%20via%20Deep%20Screenshot%20Understanding/图1-1.png" alt></span></p><p>B. 样例2：相同截图上的不同bug<br>        如图2所示，两份报告使用的是相同的应用activity的截图，图像特征提取算法会在这两张截图之间给出一个较高的相似度。然而，根据bug描述，这两份报告描述的是完全不同的bug。在DEEPPRIOR中，对于报告#1128，我们可以提取出“未找到媒体”的文字；对于报告#1127，除了提示信息之外，我们还可以提取出音量widget，DEEPPRIOR可以识别出不同的问题。<br>        <span><img class="small-img-inline" src="/2021/04/20/Prioritize%20Crowdsourced%20Test%20Reports%20via%20Deep%20Screenshot%20Understanding/图2.png" alt></span><br>图2 样例2：相同截图上的不同bug<br><span><img class="small-img-inline" src="/2021/04/20/Prioritize%20Crowdsourced%20Test%20Reports%20via%20Deep%20Screenshot%20Understanding/图2-2.png" alt></span></p><p>C. 样例3：不同截图上的相同bug<br>        如图3所示，顶部的ImageView widget的内容不同，且其占据了整个页面的很大比例。并且，由于测试时间不同，评论也不同。因此，现有的方法会认为两张截图的相似度低，即使文本描述的相似度很高，也会降低整体相似度。而通过DEEPPRIOR，我们可以提取底部的弹出信息，为“发表评论失败”，并给两个报告分配一个很高的相似度。这样的弹窗被认为是相当重要的、包含bug的widget。<br><span><img class="small-img-inline" src="/2021/04/20/Prioritize%20Crowdsourced%20Test%20Reports%20via%20Deep%20Screenshot%20Understanding/图3.png" alt></span><br>图3 样例3：不同截图上的相同bug<br><span><img class="small-img-inline" src="/2021/04/20/Prioritize%20Crowdsourced%20Test%20Reports%20via%20Deep%20Screenshot%20Understanding/图3-3.png" alt></span></p><h2><span id="iii-方法">III. 方法</span></h2><p>​本节介绍DEEPPRIOR的详细内容，即通过深度截图理解，对众测报告进行优先级排序。DEEPPRIOR由4个阶段组成，包括特征提取、特征聚合、DEEPSIMILARITY的计算与报告优先级排序。我们从应用截图和文本描述两者中收集4种不同类型的报告特征。然后，我们将提取的特征汇总成一个DEEPFEATURE，其包括bug特征和上下文特征。基于DEEPFEATURE，我们设计了一个算法来计算每两份测试报告之间的DEEPSIMILARITY。基于预先定义的规则（详见章节III-D），我们根据DEEPSIMILARITY对测试报告进行优先级排序。DEEPPRIOR方法的总体框架可参考图4。<br><span><img class="small-img-inline" src="/2021/04/20/Prioritize%20Crowdsourced%20Test%20Reports%20via%20Deep%20Screenshot%20Understanding/图4.png" alt></span></p><p>图4 DeepPrior框架</p><h3><span id="a-特征提取">A. 特征提取</span></h3><p>​第一步也是最重要的一步，为特征提取。我们分别对众测报告的应用截图和文本描述进行分析。<br>​\1) 应用截图中的特征。应用截图在众测报告中至关重要。众测人员需要在bug发生时进行截图，以便更好地阐述bug。如文献[2]所描述，文本描述仅能提供有限的信息，可能不足以清晰地描述bug。因此，除了文本描述之外，还要考虑截图来提供更多的信息。在一张截图中，存在许多不同的widget，有些widget可以提示bug信息。因此，对截图的深度理解主要依赖于widget。在DEEPPRIOR中，我们使用CV技术和深度学习（DL）技术来提取所有的widget并分析其信息。DL技术功能强大，CV技术可以处理更多种类的任务[4]。<br>​问题widget。一个应用activity可以被看作是一个有组织的widget集。一般来说，在众测任务中，众测人员能够发现的bug都会通过widget显露出来。因此，找到引发bug的widget，并将这个widget与其他widget区分开来是很重要的，而该widget就是我们所定义的问题widget（WP）。为了区分问题widget，我们分析文本描述。在众测报告中，众测人员会指出在bug发生前操作了哪个widget。如章节III-A2所示，我们可以从文本描述中提取出问题widget，并且为了定位问题widget，我们针对不同的情况可采取两种不同的策略。<br>​* 如果提取的widget包含文本，我们将widget的截图和文本描述中的文本进行匹配。被匹配的widget被认为是问题widget。<br>​* 如果widget上没有文本或文本匹配失败，我们将提取的widget送入深度神经网络，以识别简单的widget意图。该深度神经网络是在Xiao等人[5]的研究基础上修改而来的。该模型通过卷积神经网络（CNN）将widget截图编码为特征向量。输出是使用循环神经网络（RNN）从特征向量解码而得的短文本片段，该短文本片段描述了widget意图。<br>​上下文widget。除了问题widget以外，应用截图中展现的widget集还包含了更多组成上下文的widget，而它们对深度图像理解也是至关重要的。在早期的调查中，我们发现，即使问题widget、复现步骤（activity启动路径）和bug描述相同，应用的activity也可能全然不同（如章节II-C中的启发性样例）。在这个情况下，上下文widget对于识别差异而言十分重要。因此我们将其余的widget收集为上下文widget（WC）。对于每个上下文widget，我们将widget截图输入卷积神经网络，以识别其类型。每个类型由14维向量构成。<br>​卷积神经网络能够识别14种不同类型的、最为广泛使用的widget，包括Button（BTN）、CheckBox（CHB）、CheckTextView（CTV）、EditText（EDT）、ImageButton（IMB）、ImageView（IMV）、ProgressBarHorizontal（PBH）、ProgressBarVertical（PBV）、RadioButton（RBU）、RatingBar（RBA）、SeekBar（SKB）、Switch（SWC）、Spinner（SPN）、TextView（TXV）。为了训练神经网络，我们收集了36573张均匀分布在这14种类型上的widget截图。训练集、验证集和测试集的比例为7:1:2，这是图像分类任务的常见做法。神经网络由多个卷积层、最大池化层和全连接层组成。采用AdaDelta算法作为优化器，模型采用categorical_crossentropy作为损失函数。<br>​\2) 来自文本描述的特征。除了应用截图，文本描述可以更直观、更直接地提供bug信息。同时，文本描述也可以作为应用截图的正向补充。在DEEPPRIOR中，我们采用NLP技术，特别是DL算法，对测试报告中的文本描述进行处理。<br>​在文本描述中，众测人员需要对截图中的bug进行描述，并提供复现步骤，即从应用启动到bug发生的操作顺序。然而，在大多数众测平台上，bug描述和复现步骤是混在一起的，并且由于专业能力不同，众包工人并不被要求遵循特定的模式[6]。因此，将bug描述与复现步骤区分开是很复杂的。为了解决这个问题，我们采用了TextCNN模型[7]。<br>​TextCNN模型可以通过预先训练的词向量完成句子级分类任务。在将文本输入模型之前，我们对数据进行预处理。将测试报告的文本描述分割成句子，再使用jieba库将句子分割成单词，根据停顿词列表过滤掉停顿词。预处理后，我们将文本送入词向量层。在该层中，利用Word2Vec模型[8]将文本转化为128维的向量。之后，我们采用多个卷积层和最大池化层来提取文本特征。在最后一层，我们使用SoftMax激活函数，得到每个句子是bug描述还是复现步骤的概率。最后，我们将所有分类为bug描述或复现步骤的句子进行合并。为了训练TextCNN模型，我们构建了一个大规模的文本分类数据集，由2252个bug描述和2088个复现步骤组成。我们按照惯例，将训练集、验证集和测试集的比例设置为6:2:2。<br>​bug描述。bug描述总是以短句的形式出现。因此，我们用一个向量来表示句子，其也用Word2Vec模型编码。大多数的bug描述都遵循某种特定的模式，如“对某些widget进行了某些操作，发生了某些非预期的行为”，所以即使具体的词语会有所不同，但提取这种特征仍是有效的。<br>​另一个重要的过程是提取问题widget的描述，以帮助对问题widget进行定位。为了实现这一目标，我们采用基于HMM（Hidden Markov Model，隐马尔科夫模型）模型的文本分割算法[9]，并在文本分割之后分析bug描述的各个部分的词性。然后，我们提取对象部分作为问题widget定位的依据，句子中这样的对象部分就是触发bug的widget。在获取对象之后，我们使用上文所述的策略对问题widget进行定位。<br>​复现步骤。除了bug描述外，文本描述的另一个重要部分是复现步骤。复现步骤是一系列的操作，描述了用户从应用启动到bug发生的操作。对于分类到复现步骤的句子，我们按照报告中的初始顺序进行处理。我们使用相同的NLP算法进行文本分割，并对每个句子的每个文本段进行词性分析。然后，收集操作部分和对象部分，形成“操作-对象”对。然后，我们将“操作-对象”对连接成一个“操作-对象”序列。另外，除了操作词和对象，我们还为一些特定的操作添加一些补充信息。比如，假设有一个操作是键入操作，我们会添加输入内容作为补充信息，因为不同的测试输入可能导致不同的处理结果，使应用定向到不同的activity上。最后，经过形式化处理后，我们就可以从文本描述中获得复现步骤。</p><h3><span id="b-特征聚合">B. 特征聚合</span></h3><p>​在从应用截图和文本描述中获取所有特征后，我们将其汇总为两个特征类别。bug特征（Bug Feature, BFT）和上下文特征（Context Feature, CFT）。bug特征指的是众测报告中直接反映或描述bug的特征，而上下文特征是由bug出现时提供环境描述的特征聚合而成。<br>​\1) bug特征（BFT）：bug特征可以直接提供bug的信息。由于众测报告是由应用截图和文本描述组成，这两部分都包含了发生bug的关键信息。在应用截图中，我们提取了问题widget，是widget截图。DEEPPRIOR可以自动提取这样的信息。在文本描述中，bug描述部分直接描述了bug。因此，在平衡考虑应用截图和文本描述的情况下，我们将问题widget和bug描述汇总为bug特征。<br>​\2) 上下文特征（CFT）：上下文特征包括了为bug的发生构建完整上下文的特征。在应用截图中，上下文widget由问题widget以外的所有widget组成。在文本描述中，之所以考虑到复现步骤信息是因为其提供了从应用程序启动到bug发生时的完整操作路径，可以帮助识别两个测试报告的bug是否在同一个应用activity上。因此，将上下文widget和复现步骤汇总在一起，构成上下文特征。<br>​\3) 特征聚合：借助bug特征和上下文特征，我们可以将众测报告中的应用截图和文本描述中所获得的所有特征聚合到最终的DEEPFEATURE中。我们并非直接将应用截图转化为简单的特征向量，而是对应用截图进行深度理解。同时，我们对应用截图和文本描述之间的结合也更加紧密。此外，我们将应用截图和文本描述作为一个整体，根据它们在bug反馈中的作用进行划分。bug特征非常重要，我们认为上下文特征在众包测试报告的优先级确定中也应起到至关重要的作用，bug相似度的计算很大程度上依赖于整个上下文。</p><h3><span id="c-deepsimilarity的计算">C. DEEPSIMILARITY的计算</span></h3><p>​要对众测报告进行优先级排序，一个主要的步骤就是计算所有报告间的相似度。由于我们是第一个将深度截图理解引入到报告的优先级排序中的,我们将这个相似度命名为DEEPSIMILARITY。之前的研究[2][3]普遍采用合并不同特征的做法，我们分别计算不同特征的DEEPSIMILARITY，并为不同特征的结果分配不同的权重。形式化表达如下：<br>​<span><img class="small-img-inline" src="/2021/04/20/Prioritize%20Crowdsourced%20Test%20Reports%20via%20Deep%20Screenshot%20Understanding/公式1.png" alt></span><br>​\1) bug特征：我们分别计算问题widget和bug描述的DEEPSIMILARITY，并使用参数α来整合他们。<br>​问题widget。问题widget是根据章节III-A1中介绍的策略，从应用截图中提取的widget截图。为了计算问题widget的DEEPSIMILARITY，我们提取了widget截图的图像特征。为了提取图像特征，我们采用了最先进的SIFT（Scale-Invariant Feature Transform）算法[10]。因此，每个widget由一个特征点集来表示。SIFT算法的优点是可以处理不同尺寸、位置和旋转角度的图像，而这种图像在这样一个移动设备有成千上万种不同型号的时代是相当普遍的。为了对不同众测报告中的问题widget进行对比和匹配，我们使用FLANN库[11]。经过计算，我们可以得到一个0到1间的分数，0表示完全不同，1表示完全相同。这个分数可以看作是问题widget的DEEPSIMILARITY。<br>​bug描述。bug描述是简短的语句，简要描述了众测报告中的bug。因此，我们使用NLP技术对bug描述进行编码。遵循以往研究中的方法，我们使用Word2Vec模型作为编码器。为了提高Word2Vec模型的性能，我们构建了一个测试报告关键词数据库。测试报告关键词 数据库包含了8647个与软件测试、移动应用、测试报告相关的关键词，包括了标注的同义词、反义词和多义词。编码后的bug描述是一个100维的向量。之后，仍参考前人的研究，如[2][3]，我们采用广泛使用的欧氏度量算法来成对地计算不同测试报告中的bug描述的DEEPSIMILARITY。为了统一不同尺度的数值，我们使用函数 (x-min)&#x2F;(max-min)，以将每个结果x归一化到[0,1]区间，其中max是所有结果的最大值，min是所有结果的最小值。<br>​\2) 上下文特征：我们还分别计算上下文widget和复现步骤的DEEPSIMILARITY，并使用参数β来整合他们。<br>​上下文widget。上下文widget也是bug发生时整个上下文的重要组成部分。为了对应用截图有深度理解，特别是应用截图上的widget，我们使用卷积神经网络来识别每个提取的widget截图的widget类型，并形成一个包含14种widget类型数量的向量。之后，我们使用欧氏度量算法来计算获取的14维向量的距离。我们考虑了每个类型的widget的绝对数量和所有widget的分布情况。欧氏度量算法的结果（从0到1）即上下文widget的DEEPSIMILARITY。<br>​复现步骤。在特征提取过程中，复现步骤被转化为“操作-对象”序列。为了计算“操作-对象”序列的DEEPSIMILARITY，我们采用动态时间规整（Dynamic Time Warping, DTW）算法处理待比较的“操作-对象”序列。DTW算法在自动语音识别方面表现出色。在本文中，我们调整了DTW算法来处理相应的众测报告中触发bug的操作路径。DTW算法可以测量时空序列的相似度，尤其是可能存在“速度”变化的时空序列。具体而言，我们任务中的“速度”是指不同的用户操作可以通过不同的路径到达同一个应用activity的情况。与其他轨迹相似度算法相比，DTW由于可以处理不同长度的序列，所以匹配的效果更好，适合处理“操作-对象”序列。</p><h3><span id="d-报告的优先次序">D. 报告的优先次序</span></h3><p>​在聚合了DEEPFEATURE，并定义了DEEPSIMILARITY的计算规则后，我们开始对众测报告进行优先级排序。首先，我们构建两个空报告池：非优先级报告池和优先级报告池。所有的众测报告最初都会被放入非优先级报告池。<br>​与[3]中采用的随机选择一个报告作为初始报告的策略不同，我们认为应该平等的对待所有报告，随机选择报告可能影响最终的优先级。因此，为了使优先级算法形式化、统一化，我们引入了NULL报告的概念，它也包含了四个特点。<br>​* 问题widget：问题widget的截图本质上是一个三维矩阵，分别代表宽度、高度和三个颜色通道。因此，我们将问题widget构造为一个零矩阵。零矩阵的宽度和高度设置为所有实际众测报告的平均大小。直观地说，它是一个全黑的图像。<br>​* bug描述：NULL报告的bug描述直接设置为空字符串，由于字符串长度为0，显然不包含任何单词，经过Word2Vec处理后，特征向量将是一个100维的全部为“0”的向量。<br>​* 上下文widget：对于NULL报告的上下文widget，我们直接构造出代表数量为14种的不同类型widget的向量，并且所有元素均为0。这表示众包的应用截图上“没有”widget。<br>​* 复现步骤：NULL报告的复现步骤也设置为空字符串，“操作-对象”序列的长度也为0。<br>​优先级划分的主要共识是，在某些报告会重复描述bug的情况下，尽早发现所有的bug[3][13][14]。<br>​因此，要尽早为开发者提供尽可能多的描述不同bug的报告。基于这一思想，我们设计了如下的优先级策略，形式化表达式见算法1。<br>​<span><img class="small-img-inline" src="/2021/04/20/Prioritize%20Crowdsourced%20Test%20Reports%20via%20Deep%20Screenshot%20Understanding/算法1.png" alt></span><br>​首先，我们根据上述规则构建NULL报告，并将NULL报告追加到空的优先级报告池中。接下来进行一个迭代的过程。我们计算每个未确定优先级的报告与整个优先级报告池的DEEPSIMILARITY，此DEEPSIMILARITY定义为未确定优先级的报告与优先级报告池中的所有报告的最小DEEPSIMILARITY。与优先级报告池中DEEPSIMILARITY最低的报告将被移至优先报告池中。</p><h2><span id="iv-评估">IV. 评估</span></h2><h3><span id="a-实验设置">A. 实验设置</span></h3><p>​为了评估我们提出的DEEPPRIOR，我们设计了一个实证实验。为了完成实验，我们收集了10个不同移动应用的536份众包测试报告（详见表I）。A1到A10代表10个应用，不同应用的测试报告数量在10到152之间。我们还邀请软件测试专家根据测试报告所描述的bug进行人工分类，平均一个bug类别的报告数量为8.06份。</p><p>表I 实验应用<br><span><img class="small-img-inline" src="/2021/04/20/Prioritize%20Crowdsourced%20Test%20Reports%20via%20Deep%20Screenshot%20Understanding/表1.png" alt></span></p><p>​在众包测试报告数据集的基础上，我们建立了3个具体的数据集来更好地支持评估，包括1) 大规模widget图像数据集，2) 大规模测试报告关键词集，3) 大规模文本分类数据集。这4个数据集构成了综合数据集。<br>​我们共设计了三个研究问题（RQ）来评估所提出的测试报告优先级确定方法DEEPPRIOR。</p><p>​* RQ1：DEEPPRIOR如何有效识别从应用截图中提取的widget类型？<br>​* RQ2：DEEPPRIOR对众测报告中的文本描述的分类效果如何？<br>​* RQ3：DEEPPRIOR能多有效地确定众测报告的优先级？</p><h3><span id="b-rq1widget类型分类">B. RQ1：widget类型分类</span></h3><p>​第一个研究问题的设定是评估我们对应用截图的处理效果。在应用截图处理中，最重要的部分是widget的提取和分类。因此，我们评估了widget类型分类的CNN的准确性。我们共收集了36573张不同的widget图像，这些图像在14个类别中均匀分布。<br>​CNN的具体介绍详见章节III-A1。数据集按惯例，按照7:2:1的比例划分为训练集、验证集和测试集。在CNN模型训练完成后，我们对测试集的准确性进行评估。widget类型分类的总体准确率达到89.98%。具体而言，我们用精确率（precision）、召回率（recall）和F值（F-Measure）评估网络。计算公式如下，其中TP表示真正例样本，FP表示假正例样本，TN表示真负例样本，FN表示假负例样本。</p><p><span><img class="small-img-inline" src="/2021/04/20/Prioritize%20Crowdsourced%20Test%20Reports%20via%20Deep%20Screenshot%20Understanding/公式23.png" alt></span></p><p>​评估结果如表II所示，精确率平均值达90.05%，最低精确率为74.36%，最高为99.81%。对于衡量实际检索到的实例总量的召回率方面，平均值为89.98%，召回率值在70.83%至100%之间。F值是精确率和召回率的调和平均数，其平均值达到89.92%。以上结果反映了所提出的分类器的突出能力。</p><p>表II widget类型分类<br><span><img class="small-img-inline" src="/2021/04/20/Prioritize%20Crowdsourced%20Test%20Reports%20via%20Deep%20Screenshot%20Understanding/表2.png" alt></span></p><p>​我们也对结果进行了深入的观察。我们发现，有两组widget容易被混淆。第一组包括ImageButton和ImageView。不难理解，从视觉上看，这两种类型几乎是无法识别的。这两种类型之间的唯一区别是，ImageButton可以触发一个操作，而ImageView只是一个简单的图像。不过，有一点很重要的是，在应用设计中，开发者可以在ImageView widget上添加一个超链接来实现等效的效果。第二组包括Button、EditText和TextView。这三个widget都是一个固定的区域，里面包含一个文本片段，从视觉上看也很相似，即使是人类也难以辨别。此外，一些特殊的渲染方式也让这些widget更加难以识别。根据我们的调查，我们发现这两组易混淆的widget不会有太大影响，无论是从视觉角度还是从功能角度，都可以把这些widget当作为等价的widget。<br>​RQ1的结论：CNN对widget类型进行分类的总体准确率达到89.98%，对于每个具体类型，平均精确率达90.05%，最低精确率为74.36%，F值为89.92%。另外，根据我们对实测报告的调查，即使一些精确率较低的模型，其视觉和功能特征也不会对DEEPPRIOR造成负面影响。</p><h3><span id="c-rq2文本描述分类">C. RQ2：文本描述分类</span></h3><p>​在文本描述的处理中，我们将其分为两类：bug描述和复现步骤。不同的文本描述被认为是不同的报告特征。为了对文本描述进行分类，我们将文本描述分割成句子。然后，我们将这些句子输入到TextCNN模型中来完成任务，详细介绍参见章节III-A2。同时，为了更好地训练和评估网络，我们建立了一个大规模的文本分类数据集。该数据集包含了4340个标记的文本片段，其中包括2252个bug描述和2088个复现步骤。数据集按照7:2:1的比例分为训练集、验证集和测试集。</p><p>表III 文本分类<br><span><img class="small-img-inline" src="/2021/04/20/Prioritize%20Crowdsourced%20Test%20Reports%20via%20Deep%20Screenshot%20Understanding/表3.png" alt></span></p><p>​RQ2的结论：文本分类的整体准确率达到96.65%。精确率、召回率和F值都在98%以上。这样的结果显示了DEEPPRIOR对文本描述的优秀分析能力，这也为众测报告的优先级的排序打下了坚实的基础。</p><h3><span id="d-rq3众包测试报告的优先级排序">D. RQ3：众包测试报告的优先级排序</span></h3><p>​在本研究问题中，我们评估DEEPPRIOR的测试报告优先级排序效果。我们使用的指标是APFD（Average Percentage of Fault Detected）指标[15]，Feng等人也使用该指标对众测报告进行优先级排序的评估[3]。在公式中，表示最先发现bugi的报告的索引，<span><img class="small-img-inline" src="/2021/04/20/Prioritize%20Crowdsourced%20Test%20Reports%20via%20Deep%20Screenshot%20Understanding/clip_image006.png" alt></span>n是报告总数，M是暴露的bug的总数。</p><p><span><img class="small-img-inline" src="/2021/04/20/Prioritize%20Crowdsourced%20Test%20Reports%20via%20Deep%20Screenshot%20Understanding/公式4.png" alt></span></p><p>​为了更好地说明DEEPPRIOR的优势，我们将DEEPPRIOR与以下优先级策略进行比较。<br>​* IDEAL：这种策略理论上是具有最好的优先级排序，意味着开发人员可以在最短的时间内审查所有报告中的bug。<br>​* IMAGE：这种策略只使用DEEPPRIOR的深度图像理解结果来对测试报告进行排序，因为深度图像理解是我们研究的重要组成部分。<br>​* BDDIV：这种策略参考了Feng等的工作[3]的算法，这也是众包测试报告优先级排序的最先进的方法。<br>​* RANDOM：RANDOM策略指的是没有任何优先级策略的情况。<br>​对于DEEPPRIOR和IMAGE策略，因为我们的方法很稳定，所以我们运行一次，训练后的模型不会因为不同的尝试而产生不同的结果；对于IDEAL策略，我们手动计算APFD，因为对于固定的报告簇而言，它是一个定值；对于BDDIV策略，我们运行30次，然后像原文[3]一样计算平均值；对于RANDOM策略，我们运行100次，以消除偶然情况的影响。<br>​首先，我们将DEEPPRIOR与RANDOM策略进行比较。如表IV所示，我们发现DEEPPRIOR策略比RANDOM策略效果好了许多，从15.15%至38.93%之间不等，并且平均提升幅度达27.04%。由此可见DEEPPRIOR的优越性。</p><p>表IV DEEPPRIOR报告优先级排序结果与比较<br><span><img class="small-img-inline" src="/2021/04/20/Prioritize%20Crowdsourced%20Test%20Reports%20via%20Deep%20Screenshot%20Understanding/表4.png" alt></span></p><p>​然后，我们将DEEPPRIOR与单一IMAGE策略的结果进行对比。DEEPPRIOR的平均提升幅度为4.54%，在2个应用（A3和A4）中，DEEPPRIOR的表现远超过了IMAGE策略。对于A8来说，DEEPPRIOR的策略弱于IMAGE策略。我们对A8的报告进行检查，发现是因为文本描述写的不好，不能对报告的优先级有积极的帮助。总的而言，结果证明了文本分析和深度图像理解两者结合的必要性，单一策略可以弥补彼此的缺点，提高确定优先级的准确性。<br>​同时，我们还对DEEPPRIOR和BDDIV策略（即最先进的方法）进行了比较。根据实验结果，DEEPPRIOR的表现优于BDDIV，平均提升幅度为3.81%。在一些应用中的提升尤其明显。此外，我们还记录了从读取报告簇到输出优先级排序好的报告的总时间开销。结果显示，DEEPPRIOR所使用的时间不到BDDIV的一半，表现出极大的性能优势。<br>​DEEPPRIOR相对于BDDIV的另一个优势是，DEEPPRIOR可以输出稳定的结果，而BDDIV的结果会浮动。根据BDDIV策略的详细结果（于在线包中），我们发现BDDIV的波动性很大。<br>​在不同的应用上与基线策略相比的提升情况不同，可以用某些原因解释这一点。首先，“报告-种类”的比率是不同的，所以在一个应用的有限的activity集中，同样的activity重复出现的频率会变得很高。其次，不同的应用有不同的内容。比如A1是一款儿童教育类应用，它由大量的图片、视频、变体文字组成。在这种情况下，想要提取有用的文本信息，并对应用截图有一个全面的理解，就会变得更为复杂。因此，预估值会降低。<br>​RQ3的结论：DEEPPRIOR对众测报告进行优先级排序的能力是非常优秀的，它的性能优于最先进方法BDDIV，但开销不到其一半。同时，IMAGE策略的具体实验也表明了我们深度截图理解算法的有效性。与最先进的方法相比，DEEPPRIOR的表现更加稳定。</p><h3><span id="e-有效性风险">E. 有效性风险</span></h3><p>​本次实验中应用的类别是有限的。我们的15个实验应用涵盖了8个不同的类别（根据应用商店分类法），类别覆盖有限。但是，我们要强调的是，由于我们对应用截图的深度理解涉及到对应用活动的布局特征，因此DEEPPRIOR仅适用于分析具有网格布局或列表布局的应用程序。我们所表述的也仅限于此类布局的应用中。<br>​众包工人的注册不受限制。众包工人的能力不受限制，可能会出现低质量的报告。然而，即使一些报告的质量很低，如果它真的包含一个bug，DEEPPRIOR也可以识别它所描述的bug。如果没有，DEEPPRIOR会将该报告归类为单一类别，不会影响到其他报告的优先级。<br>​我们构建的数据集是中文的。数据集的语言可能是一个威胁，但NLP和OCR技术相当强大。如果我们把文本处理引擎换成其它语言的引擎，文本处理也会很好地完成，不会对DEEPPRIOR产生负面影响。此外，机器翻译的成熟[16]也使其具备了处理跨语言文本信息的强大能力。</p><h2><span id="v-相关工作">V. 相关工作</span></h2><h3><span id="a-众包测试">A. 众包测试</span></h3><p>​众包测试已是一种主流的测试方法。它与传统测试有很大的不同。测试任务被分配给大量来自不同地点、具备不同专业能力的众包工人。众包测试最显著的优势是能够模拟不同的使用条件，经济成本相对较低[17][18]。然而，众包测试的开放性导致了大量的冗余报告。关键问题是如何提高开发人员审核测试报告的效率。一些研究从选择有技能的众包工人来完成任务[19][20][21]。这样的策略是有效的，但同时它仍然难以控制，因为即使是熟练的众包工人也会在任务中偷懒。因此，我们认为在众包测试中，更重要的是处理测试报告，而非其他因素。Liu等[22]和Yu[6]分别提出了由测试报告的截图自动生成描述的方法，这些方法都基于这样的一个共识，即对所有众包工人而言，应用程序的截图容易获得，但文本描述很难编写。这一想法启发了我们对截图的深度理解，以帮助更好地确定测试报告的优先级。</p><h3><span id="b-众包测试报告处理">B. 众包测试报告处理</span></h3><p>​为了更好地帮助开发人员审查报告和修复bug，当前已有许多研究来处理众包测试报告。基本策略包括报告分类、重复检测和报告优先级确定。在本节中，我们将介绍基于不同策略的相关工作。<br>​Banerjee等人提出了FactorLCS[23]，其使用常见的序列匹配，该方法在开放的bug跟踪库上是有效的。他们还提出了一种多标签分类器的方法[24]，以找到报告集群中具有高度相似性的“主要”报告。同样，Jiang等人提出了TERFUR[14]，这是一个使用NLP技术对测试报告进行聚类的工具，他们还过滤掉了低质量的报告。Wang等[25]将众包工人的特征作为测试报告的特征考虑进去，然后进行聚类。Wang等提出了LOAF[26]，这是第一个将操作步骤和结果描述分开处理的报告特征提取方法。<br>​更多的研究专注于检测重复测试报告。Sun等[27]采用信息检索模型，比较准确地检测重复的bug报告。Sureka等[28]采用基于字符n-gram的模型来完成重复检测任务。Prifti等[29]对大规模的开源项目测试报告进行了调查，提出了一种可以将重复报告的搜索集中在整个存储库的特定部分的方法。Sun等人提出了一个衡量相似度的检索函数REP[30]，并且该函数包括部件、版本等非文本字段的相似度。Nguyen等提出了DBTM [31]，该工具同时利用了基于IR的特征和基于主题的特征，并根据技术问题检测重复的bug报告。Alipour等[32]对测试报告上下文进行了较为全面的分析，提高了检测准确率。Hindle[33]通过结合上下文质量属性、架构术语和系统开发主题进行改进，提高了bug重复检测的能力。<br>​上述方法，包括报告分类和重复检测，都是选择部分测试报告来代表所有的测试报告。但是，我们认为，即使存在重复的报告，所有的报告都包含有价值的信息。而且，在检测到重复的报告后，开发人员仍需要对报告进行审查，以推进bug的处理。因此，我们认为报告优先级是一个更好的选择。<br>​当前也已有许多关于报告优先级的研究。Zhou等提出了BugSim[34]，其考虑了文本和统计特征以对测试报告进行排序。Tian等[35]提出的DRONE是一种基于机器学习的方法，其考虑测试报告的不同因素来预测测试报告的优先级。Feng等人提出了一系列方法，DivRisk[36]和BDDiv[3]，以对测试报告进行优先级排序，他们首先考虑了测试报告的截图。随后，Wang等[2]进一步研究并探索出了一种更加完善的测试报告优先级排序方法，并更提高了对截图的关注度。<br>​在以上的所有研究中，只有少数研究，如[2]和[3]，考虑了应用程序的截图，我们认为这是一个在提取特征来处理测试报告方面相当有价值的因素。但这些研究仅将截图作为简单的矩阵，而非有意义的内容。</p><h3><span id="c-深度图像理解">C. 深度图像理解</span></h3><p>​图像理解是计算机视觉（CV）领域的一个热点问题。本节主要介绍在软件测试中利用图像理解的研究。<br>​Lowe[10]提出了SIFT算法，其利用一系列新的图像局部特征，这些特征对于图像本身而言是恒定的，包括平移、缩放和旋转，以匹配目标图像上的特征点，并计算相似度。光学字符识别（Optical Character Recognition，OCR）是一种应用广泛的文字识别工具，它有助于根据图像上丰富的文字信息更好地理解图像。Nguyen等[37]提出了REMAUI，其使用CV技术来识别应用截图中的widget、文本、图像甚至容器。Moran等[38]在REMAUI的基础上提出了REDRAW，更精确地识别widget，并能自动生成应用UI的代码。同样，Chen等[39]也提出了一种结合CV技术和机器学习的工具，以根据应用截图生成GUI骨架。Yu等[1]提出了一种名为LIRAT的工具，可以在透彻了解应用截图的情况下跨平台记录和重新运行移动应用测试脚本。</p><h2><span id="vi-结论">VI. 结论</span></h2><p>​本文通过深度截图理解，提出了一种众包测试报告优先级排序方法DEEPPRIOR。DEEPPRIOR将应用截图和文本描述转化为四个不同的特征，包括问题widget、上下文widget、bug描述和复现步骤。然后，将特征汇总到DEEPFEATURE中，这些特征根据与bug的相关性，包括bug特征和上下文特征。之后，我们根据特征计算DEEPSIMILARITY。最后，根据DEEPSIMILARITY，按照预先设定的规则对报告进行优先级排序。我们还进行了一个实验来评估所提出的方法，结果显示，DEEPPRIOR的表现优于目前的最优方法，且开销不到它的一半。</p><h2><span id="感谢">感谢</span></h2><p>​本工作得到国家重点研发计划（2018AAA0102302）、国家自然科学基金（61802171、61772014、61690201）、中央高校基本科研基金（14380021）、国家大学生创新创业训练计划（202010284073Z）的部分支持。</p><h2><span id="参考文献">参考文献</span></h2><p>[1] S. Yu, C. Fang, Y. Feng, W. Zhao, and Z. Chen, “Lirat: Layout and image recognition driving automated mobile testing of cross-platform,” in 2019 34th IEEE&#x2F;ACM International Conference on Automated Software Engineering (ASE). IEEE, 2019, pp. 1066–1069. </p><p>[2] J. Wang, M. Li, S. Wang, T. Menzies, and Q. Wang, “Images don’t lie: Duplicate crowdtesting reports detection with screenshot information,” Information and Software Technology, vol. 110, pp. 139–155, 2019. </p><p>[3] Y. Feng, J. A. Jones, Z. Chen, and C. Fang, “Multi-objective test report prioritization using image understanding,” in 2016 31st IEEE&#x2F;ACM International Conference on Automated Software Engineering (ASE). IEEE, 2016, pp. 202–213. </p><p>[4] N. O’Mahony, S. Campbell, A. Carvalho, S. Harapanahalli, G. V. Hernandez, L. Krpalkova, D. Riordan, and J. Walsh, “Deep learning vs. traditional computer vision,” in Science and Information Conference. Springer, 2019, pp. 128–144. </p><p>[5] X. Xiao, X. Wang, Z. Cao, H. Wang, and P. Gao, “Iconintent: automatic identification of sensitive ui widgets based on icon classification for android apps,” in 2019 IEEE&#x2F;ACM 41st International Conference on Software Engineering (ICSE). IEEE, 2019, pp. 257–268. </p><p>[6] S. Yu, “Crowdsourced report generation via bug screenshot understanding,” in 2019 34th IEEE&#x2F;ACM International Conference on Automated Software Engineering (ASE). IEEE, 2019, pp. 1277–1279. </p><p>[7] Y. Kim, “Convolutional neural networks for sentence classification,” arXiv preprint arXiv:1408.5882, 2014. </p><p>[8] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, “Distributed representations of words and phrases and their compositionality,” in Advances in neural information processing systems, 2013, pp. 3111–3119. </p><p>[9] L. R. Rabiner, “A tutorial on hidden markov models and selected applications in speech recognition,” Proceedings of the IEEE, vol. 77, no. 2, pp. 257–286, 1989. </p><p>[10] D. G. Lowe et al., “Object recognition from local scale-invariant features.” in iccv, vol. 99, no. 2, 1999, pp. 1150–1157. </p><p>[11] M. Muja and D. G. Lowe, “Fast approximate nearest neighbors with automatic algorithm configuration.” VISAPP (1), vol. 2, no. 331-340, p. 2, 2009. </p><p>[12] D. F. Silva and G. E. Batista, “Speeding up all-pairwise dynamic time warping matrix calculation,” in Proceedings of the 2016 SIAM International Conference on Data Mining. SIAM, 2016, pp. 837–845. </p><p>[13] T. Y. Chen, F.-C. Kuo, R. G. Merkel, and T. Tse, “Adaptive random testing: The art of test case diversity,” Journal of Systems and Software, vol. 83, no. 1, pp. 60–66, 2010. </p><p>[14] B. Jiang, Z. Zhang, W. K. Chan, and T. Tse, “Adaptive random test case prioritization,” in 2009 IEEE&#x2F;ACM International Conference on Automated Software Engineering. IEEE, 2009, pp. 233–244. </p><p>[15] G. Rothermel, R. H. Untch, C. Chu, and M. J. Harrold, “Prioritizing test cases for regression testing,” IEEE Transactions on software engineering, vol. 27, no. 10, pp. 929–948, 2001. </p><p>[16] S. Karimi, F. Scholer, and A. Turpin, “Machine transliteration survey,” ACM Computing Surveys (CSUR), vol. 43, no. 3, pp. 1–46, 2011. </p><p>[17] R. Gao, Y. Wang, Y. Feng, Z. Chen, and W. E. Wong, “Successes, challenges, and rethinking–an industrial investigation on crowdsourced mobile application testing,” Empirical Software Engineering, vol. 24, no. 2, pp. 537–561, 2019. </p><p>[18] K. Mao, L. Capra, M. Harman, and Y. Jia, “A survey of the use of crowdsourcing in software engineering,” Journal of Systems and Software, vol. 126, pp. 57–84, 2017. </p><p>[19] Q. Cui, S. Wang, J. Wang, Y. Hu, Q. Wang, and M. Li, “Multi-objective crowd worker selection in crowdsourced testing.” </p><p>[20] Q. Cui, J. Wang, G. Yang, M. Xie, Q. Wang, and M. Li, “Who should be selected to perform a task in crowdsourced testing?” in 2017 IEEE 41st Annual Computer Software and Applications Conference (COMPSAC), vol. 1. IEEE, 2017, pp. 75–84. </p><p>[21] M. Xie, Q. Wang, G. Yang, and M. Li, “Cocoon: Crowdsourced testing quality maximization under context coverage constraint,” in 2017 IEEE 28th International Symposium on Software Reliability Engineering (ISSRE). IEEE, 2017, pp. 316–327. </p><p>[22] D. Liu, X. Zhang, Y. Feng, and J. A. Jones, “Generating descriptions for screenshots to assist crowdsourced testing,” in 2018 IEEE 25th International Conference on Software Analysis, Evolution and Reengineering (SANER). IEEE, 2018, pp. 492–496. </p><p>[23] S. Banerjee, B. Cukic, and D. Adjeroh, “Automated duplicate bug report classification using subsequence matching,” in 2012 IEEE 14th International Symposium on High-Assurance Systems Engineering. IEEE, 2012, pp. 74–81. </p><p>[24] S. Banerjee, Z. Syed, J. Helmick, and B. Cukic, “A fusion approach for classifying duplicate problem reports,” in 2013 IEEE 24th International Symposium on Software Reliability Engineering (ISSRE). IEEE, 2013, pp. 208–217. </p><p>[25] J. Wang, Q. Cui, Q. Wang, and S. Wang, “Towards effectively test report classification to assist crowdsourced testing,” in Proceedings of the 10th ACM&#x2F;IEEE International Symposium on Empirical Software Engineering and Measurement, 2016, pp. 1–10. </p><p>[26] J. Wang, S. Wang, Q. Cui, and Q. Wang, “Local-based active classification of test report to assist crowdsourced testing,” in Proceedings of the 31st IEEE&#x2F;ACM International Conference on Automated Software Engineering, 2016, pp. 190–201. </p><p>[27] C. Sun, D. Lo, X. Wang, J. Jiang, and S.-C. Khoo, “A discriminative model approach for accurate duplicate bug report retrieval,” in Proceedings of the 32nd ACM&#x2F;IEEE International Conference on Software Engineering-Volume 1, 2010, pp. 45–54. </p><p>[28] A. Sureka and P. Jalote, “Detecting duplicate bug report using character n-gram-based features,” in 2010 Asia Pacific Software Engineering Conference. IEEE, 2010, pp. 366–374. </p><p>[29] T. Prifti, S. Banerjee, and B. Cukic, “Detecting bug duplicate reports through local references,” in Proceedings of the 7th International Conference on Predictive Models in Software Engineering, 2011. </p><p>[30] C. Sun, D. Lo, S.-C. Khoo, and J. Jiang, “Towards more accurate retrieval of duplicate bug reports,” in 2011 26th IEEE&#x2F;ACM International Conference on Automated Software Engineering (ASE 2011). IEEE, 2011, pp. 253–262. </p><p>[31] A. T. Nguyen, T. T. Nguyen, T. N. Nguyen, D. Lo, and C. Sun, “Duplicate bug report detection with a combination of information retrieval and topic modeling,” in 2012 Proceedings of the 27th IEEE&#x2F;ACM International Conference on Automated Software Engineering. IEEE, 2012, pp. 70–79. </p><p>[32] A. Alipour, A. Hindle, and E. Stroulia, “A contextual approach towards more accurate duplicate bug report detection,” in 2013 10th Working Conference on Mining Software Repositories (MSR). IEEE, 2013, pp. 183–192. </p><p>[33] A. Hindle, A. Alipour, and E. Stroulia, “A contextual approach towards more accurate duplicate bug report detection and ranking,” Empirical Software Engineering, vol. 21, no. 2, pp. 368–410, 2016. </p><p>[34] J. Zhou and H. Zhang, “Learning to rank duplicate bug reports,” in Proceedings of the 21st ACM international conference on Information and knowledge management, 2012, pp. 852–861. </p><p>[35] Y. Tian, D. Lo, and C. Sun, “Drone: Predicting priority of reported bugs by multi-factor analysis,” in 2013 IEEE International Conference on Software Maintenance. IEEE, 2013, pp. 200–209. </p><p>[36] Y. Feng, Z. Chen, J. A. Jones, C. Fang, and B. Xu, “Test report prioritization to assist crowdsourced testing,” in Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering, 2015, pp. 225–236. </p><p>[37] T. A. Nguyen and C. Csallner, “Reverse engineering mobile application user interfaces with remaui (t),” in IEEE&#x2F;ACM International Conference on Automated Software Engineering, 2016. </p><p>[38] K. Moran, C. Bernal-Cardenas, M. Curcio, R. Bonett, and D. Poshy- ´ vanyk, “Machine learning-based prototyping of graphical user interfaces for mobile apps,” arXiv preprint arXiv:1802.02312, 2018. </p><p>[39] C. Chen, T. Su, G. Meng, Z. Xing, and Y. Liu, “From ui design image to gui skeleton: a neural machine translator to bootstrap mobile gui implementation,” in Proceedings of the 40th International Conference on Software Engineering. ACM, 2018, pp. 665–676.</p>]]></content>
    
    
    <summary type="html">&lt;img src=&quot;/2021/04/20/Prioritize%20Crowdsourced%20Test%20Reports%20via%20Deep%20Screenshot%20Understanding/title.png&quot; alt&gt;

&lt;h1 id=&quot;基于深度截图理解的众包测试报告优先级排序&quot;&gt;&lt;a href=&quot;#基于深度截图理解的众包测试报告优先级排序&quot; class=&quot;headerlink&quot; title=&quot;基于深度截图理解的众包测试报告优先级排序&quot;&gt;&lt;/a&gt;基于深度截图理解的众包测试报告优先级排序&lt;/h1&gt;&lt;h2 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要&quot;&gt;&lt;/a&gt;摘要&lt;/h2&gt;&lt;p&gt;​		众包测试在移动应用程序测试中日益占据主导地位，但对于应用开发者来说，审查数量过多的测试报告是很大的负担。已有许多学者提出基于文本和简单图片特征的测试报告处理方法。然而，在移动应用测试中，测试报告所包含的文本较为精简且信息不够充分，图片则能够提供更丰富的信息。这一趋势促使我们在深度截图理解的基础上，对众包测试报告的优先级进行排序。&lt;br&gt;​		本文中，我们提出了一种新的众包测试报告优先级排序方法，即DEEPPRIOR。我们首先引入一个新的特征来代表众包测试报告，即DEEPFEATURE，它基于对应用程序截图的深度分析，涵盖了所有组件（widget）及它们的文本、坐标、类型甚至是意图。DEEPFEATURE包括直接描述bug的bug特征（Bug Feature），和刻画bug完整上下文的上下文特征（Context Feature）。DEEPFEATURE的相似度用于表示测试报告的相似度，并被用于对众包测试报告进行优先级排序。我们形式上将相似度定义为DEEPSIMILARITY。我们还进行了一个实证实验，以评估所提技术在大型数据中的有效性。结果表明，DEEPPRIOR性能最佳，以不足一半的成本获得优于其它方法的结果。&lt;/p&gt;
&lt;p&gt;索引词 众包测试，移动应用测试，深度截图理解&lt;/p&gt;</summary>
    
    
    
    <category term="论文阅读" scheme="https://values.keys.moe/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="论文阅读" scheme="https://values.keys.moe/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
  </entry>
  
  <entry>
    <title>Syntactic Data Augmentation Increases Robustness to Inference Heuristics</title>
    <link href="https://values.keys.moe/2020/09/16/Syntactic%20Data%20Augmentation%20Increases%20Robustness%20to%20Inference%20Heuristics/"/>
    <id>https://values.keys.moe/2020/09/16/Syntactic%20Data%20Augmentation%20Increases%20Robustness%20to%20Inference%20Heuristics/</id>
    <published>2020-09-16T08:52:33.000Z</published>
    <updated>2022-10-05T09:40:33.660Z</updated>
    
    <content type="html"><![CDATA[<img src="/2020/09/16/Syntactic%20Data%20Augmentation%20Increases%20Robustness%20to%20Inference%20Heuristics/title.png" alt><h1><span id="通过语法数据扩增提升推理启发法的鲁棒性">通过语法数据扩增提升推理启发法的鲁棒性</span></h1><h2><span id="摘要">摘要</span></h2><p>​诸如BERT的预训练的神经模型在微调以执行自然语言推理（NLI）时，常常在标准数据集上展现出了高度准确性，但在受控的挑战集上，它们却表现出对语序敏感度的出奇缺乏。我们假设此问题并不主要因为预训练模型的局限性引起，而是由于缺乏众包的NLI样例引起的，而这些样例可能在微调阶段传递了语法结构的重要性。我们探索了几种方法来扩增标准训练集中语法丰富的实例，这些实例是通过对MNLI语料库的句子应用语法转换而生成的。而表现最好的扩增方法，主语&#x2F;宾语倒置法，可以在不影响BERT对MNLI测试集性能的情况下，将BERT对受控实例的词序敏感度诊断从0.28提升至0.73。这种改进全面超过了用于数据扩增的特定结构，这表明了扩增可以使BERT学习到抽象语法的表现形式。</p><span id="more"></span><h2><span id="1-介绍">1. 介绍</span></h2><p>​在NLP里常见的监督学习范例中，特定分类任务的大量标记实例被随机地分为训练集和测试集。系统在训练集上进行训练，然后在测试集上进行评估。神经网络，尤其是对单词预测对象的进行预训练的系统，如ELMo(Peters et al.,2018)或BERT(Devlin et al.,2019)——在这种范例中表现出色：在具有足够大的预训练语料库的情况下，这些模型在许多测试集上所表现出的准确性达到甚至超过了未经训练的人类标注者(Raffel et al.,2019)。</p><p>​同时，越来越多的证据表明，从与训练集相同的范围中提取的测试集上的高精度并不表示模型已经掌握了该任务。当模型应用于表示相同任务的不同数据集中，这种差异可能表现为准确性的急剧下降(Talmor and Berant, 2019;Yogatama et al., 2019)，或者表现为对输入语言无关扰动的过度敏感(Jia and Liang, 2017; Wallace et al., 2019)。</p><p>​在自然语言推理（NLI）任务中，McCoy等人(2019b)记录了这样的一种差异，即模型在标准测试集上的出色性能并不对应表明它能像人类定义的那样精通于此任务。在这个任务中，系统将获得两个句子，其被期望确定一个句子（前提）是否蕴含另一个句子（假设）。即使不是所有人，大多数人也都会同意NLI需要对语法结构敏感。例如，以下句子即使包含了相同的单词，但它们并不相互蕴含：</p><p>​(1)  演员看到了律师 (The lawyer saw the actor.)</p><p>​(2)  律师看到了演员 (The actor saw the lawyer.)</p><p>​McCoy等人构造了HANS挑战集，其包含了一系列此类构造的例子，并且其被用来表明，当BERT在MNLI语料库进行微调时，该微调模型在从该语料库提取的测试集上取得了较高的准确率，但其对语法几乎没有敏感性；该模型会错误地得出结论，如（1）蕴含（2）。</p><p>​我们考虑用两种解释来说明为什么在MNLI上微调的BERT会在HANS上失败。在代表性不足假设下，BERT在HANS上失败，是因为它的预训练表现形式缺失了一些必要的语法信息。而在缺失连接的假设下，BERT从输入中提取相关语法信息(参见 Goldberg 2019;Tenney et al. 2019)，但是它无法在HANS上使用这个信息，因为很少有MNLI训练实例可以表明语法应该如何支持NLI的(McCoy et al., 2019b)。这两种假设都有可能是正确的：部分语法方面BERT可能根本未学习到，还有部分方面已经学过了，但并没有应用被用于进行推理。</p><p>​缺失连接假设预测，从一个语法结构中使用少量的实例进行训练集的扩增将使BERT知道任务需要它使用它的语法表现形式。这不仅将使得用于数据扩增的结构的改进，并且也可推广到其他结构上。相反，代表性不足假设预测，模型想要在HANS上具有更好的表现，BERT必须从头开始学习每种语法结构是如何影响NLI的。这预计需要有更大的数据扩增集来获得足够的性能，并且整个结构几乎不能泛化。</p><p>​本文旨在验证这些假设。我们通过对MNLI的少量实例进行语法转换以构造扩增集。尽管在MNLI上只扩增了约400个主语与宾语互换的实例（大约是MNLI训练集大小的0.1%），但模型对语法上具有挑战性的案例的准确率得到了显著的提高。更关键的是，即使在扩增中仅使用了一个单一的变换，但在一系列的结构中，准确度都得到了提高。例如，BERT在涉及关系从句的实例（例如，演员给游客看到的银行家打电话(The actors called the banker who the tourists saw) 无法推导出银行家打电话给游客(The banker called the tourists)）在未扩增的实例中准确性为0.33，而扩增后为0.83。这表明我们的方法并不会过度适应于一个结构，而是利用了BERT现有的语法表示形式，从而为缺失连接假设提供了支持。同时，我们还观察到了泛化的局限性，在这些情况下，它们支持了代表性不足的假设。</p><h2><span id="2-背景">2. 背景</span></h2><p>​HANS是一个模板生成的挑战集，旨在测试NLI模型是否采用了三种语法启发法。首先是词汇重叠启发法，其假定所有时间内所有在假设中的单词也都在前提中，且标签需是蕴含标签。在MNLI训练集中，这种启发法通常会做出正确的预测，几乎从不做出错误的预测。这可能是由于MNLI生成的过程所致：众包工作者被给予一个前提，并被要求生成与该前提相矛盾，或是蕴含这个前提的句子。为了最大程度地减少工作量，工作人员可能过度地使用了词汇重叠，将其作为一种生成含义假设的捷径。当然，词汇重叠启发法并不是一种普遍有效的推理策略，并且在许多HANS实例中都是失败的。例如上文所述，律师看到了演员(the lawyer saw the actor)，并不意味着演员看到了律师(the actor saw the lawyer)。</p><p>​HANS还包括诊断子序列启发法（假定前提蕴含任何与其相邻的子序列的假设）和成分启发法（假设前提蕴含其自身所有构成要素）的实例情形。当我们专注于对抗词汇重叠启发法时，我们还将测试其他启发法的泛化情况，这可以看作是词汇重叠中特别具有挑战性的案例。表A.5，A.6，A.7给出了用于诊断这三种启发法的所有结构的实例。</p><p>​数据扩增通常用于增强视觉的鲁棒性(Perez and Wang, 2017)与语言的鲁棒性(Belinkov and Bisk, 2018; Wei and Zou, 2019)，包括了NLI (Minervini and Riedel, 2018; Yanaka et al., 2019)。在许多情况下，使用一种实例进行扩增可以提高特定情况下的准确性，但不能泛化到其他情况，这表明模型过拟合于扩增集(Jia and Liang, 2017; Ribeiro et al., 2018; Iyyer et al., 2018; Liu et al., 2019)。特别的，McCoy等人(2019b)发现，HANS的实例的扩增可以泛化推广到不同的单词重叠挑战集(Dasgupta et al., 2018)，但这仅适用于长度与HANS实例相似的实例。我们通过生成各种基于语料库的实例来减轻对表面属性的过度拟合，这些实例与挑战集上的词法与语法均不同。最后，Kim等人(2018)使用了与我们相似的数据扩增方法，但没有研究对不在扩增集中的实例类型的泛化。</p><h2><span id="3-生成扩增数据">3. 生成扩增数据</span></h2><p>​我们使用两种语法转换从MNLI生成扩增实例：倒置INVERSION（互换原句的主语与宾语）和被动化PASSIVIZATION。对于每个转换，我们都有两个系列的扩增集。原始前提(ORIGINAL PREMISE)策略保留了原有的MNLI前提，并对假设进行了转换；转换假设(TRANSFORMED HYPOTHESIS)使用原始MNLI假设作为新前提，转换后的假设作为新假设（实例见表1，具体请参见§A.2）。我们尝试了三种扩增集的大小：小型（101个实例），中型（405个实例），大型（1215个实例）。所有的扩增集都比MNLI训练集（297k）小得多。</p><img src="/2020/09/16/Syntactic%20Data%20Augmentation%20Increases%20Robustness%20to%20Inference%20Heuristics/表1.png" alt><p>​我们没有试图确保生成实例的自然性；例如，在倒置转换中，车厢造成了大量噪音(The carriage made a lot of noise)被转换成大量噪音造成了车厢(A lot of noise made the carriage)。此外，扩增数据集的标签存在一些噪音；例如，我们假设倒置将正确的标签从蕴含改为中性，但是也并非必然如此（如果买方遇到卖方(The buyer met the seller)，那么卖方遇到买方(The seller met the buyer)是有可能的）。如下所示，这种噪声不会损害MNLI的准确性。</p><p>​最后，我们包括一个随机的打乱条件，其中MNLI前提及其假设都被随机打乱。我们使用这个情况来测试语法上不知情的方法是否能教会这个模型：当忽略单词顺序时，就无法做出可靠的推论。</p><h2><span id="4-试验设置">4. 试验设置</span></h2><p>​我们将每个扩增集分别添加到MNLI的训练集中，并对每个生成的训练集进行微调BERT的训练。微调的更多细节在附录A.1中。我们为扩增策略与扩增集大小的每种组合重复了五个随机种子的过程，但最成功的策略（倒置+转换假设（INVERSION+TRANSFORMED HYPOTHESIS））除外。且对于每个扩增的范围，均进行了15次运行。参照McCoy等人(2019b)，在对HANS进行评估时，我们将模型产生的中性与矛盾标签合并为一个单一的非蕴含(non-entailment)标签。</p><p>​对于原始前提(ORIGINAL PREMISE)与转换假设(TRANSFORMED HYPOTHESIS)，我们尝试了分别使用每一种转换，并使用了包含倒置与被动化的数据集进行了实验。我们还分别对仅使用带有蕴含标签的被动化例子和仅使用带有非蕴含标签的被动化例子进行了单独的实验。作为基线，我们使用了100次在未进行数据扩增的MNLI上训练出的微调的BERT模型(McCoy et al., 2019a)。</p><p>​我们会报告模型在HANS上的准确性和在MNLI的开发集上的准确性（MNLI测试集的标签不公开）。我们没有调整这个开发集的任何参数。我们下面讨论的所有比较都在p&lt;0.01的水平上，比较结果都是十分显著的（基于双向t检验）。</p><h2><span id="5-结果">5. 结果</span></h2><p>​MNLI的准确性在不同的扩增策略中都非常相似，并且与未经扩增的基线(0.84)相匹配，这表明最多有1215个实例的语法扩增不会损害数据集的整体表现。相比之下，HANS的准确度差异很大，大多数模型在非蕴含的实例中的表现得比置信准确度差（在HANS上为0.5），这表明了它们采用了启发法（图1）。很大程度上，最有效的扩增策略是倒置结合转换假设。HANS在单词重叠案例（其中正确的标签都是非蕴含的，例如：医生看了律师(the doctor saw the lawyer)↛律师看了医生(the lawyer saw the doctor)）的准确度在没有数据扩增的情况下为0.28，在大型扩增集上为0.73。同时，在启发法做出正确预测的情况下（如演员旁的游客们给作家们打电话(The tourists by the actor called the authors) → 游客们给作家们打电话(The tourists called the authors)），这种策略降低了BERT的准确性；实际上，在词汇重叠做出正确与不正确的预测情况下，最佳模型的准确度都是相似的，这表明了这种干预阻止了模型采用启发法。</p><img src="/2020/09/16/Syntactic%20Data%20Augmentation%20Increases%20Robustness%20to%20Inference%20Heuristics/图1.png" alt><p>​随机打乱的方法并未使模型在未经扩增的基线上得到了改善，表明关注语法的转换是必要的(表A.2)。被动化比倒置的收益要小得多，这可能是由于存在显式的标记引起的（如单词by），这可能导致模型仅在这些单词出现时才考虑词序。有趣的是，即使是在HANS的被动实例中，倒置也仍比被动化更有效（大型倒置扩增：0.13；大型被动化扩增：0.01）。最后，自身倒置比倒置与被动化的结合更有效。</p><p>​现在，我们来更详细地分析最有效的策略，即倒置结合转换假设。首先，该策略在抽象层面上与HANS的主语&#x2F;宾语交换类别相似，但是两者的词汇与语法特性均有不同。尽管存在这些差异，但模型在HANS类别上的表现在中型与大型扩增条件下都是完美的（1.00），这表明BERT能从转换的高级语法结构中受益。对于小的扩增集，此类别的准确性为0.53，表明有101个实例不足以使BERT知道不能随意的交换主语与宾语的对象。相反，将扩增大小从中型变至大型，能在HANS的子案例中产生适度且易变的效果（见附录A.3了解具体个案的结果）；为了更清楚的了解扩增大小的影响，可能还需要对该参数进行更密集的采样。</p><img src="/2020/09/16/Syntactic%20Data%20Augmentation%20Increases%20Robustness%20to%20Inference%20Heuristics/图2.png" alt><p>​尽管倒置是该扩充集中的唯一转换，但是除了主语&#x2F;宾语交换之外，其他结构的性能也得到了显著的提高（图2）；例如，模型能够更好地处理包含介词短语的实例，例如，经理背后的法官看医生(The judge behind the manager saw the doctors)并不蕴含医生看经理(The doctors saw the manager)，（未扩增：0.41；大型扩增：0.89）。在以子序列启发法方法为目标的案例中，有一个更平缓，但仍然十分明显的提升；这种较小程度的提升表明，对连续子序列从词汇重叠中分离处理更具泛化性。一个例外是对“NP&#x2F;S”推论的准确性，例如经理们听说秘书辞职了(the managers heard the secretary resigned) ↛经理们听见了秘书说话(The managers heard the secretary)，这一准确度从0.02（未扩增）大幅提升至0.5（大型扩增）。因此，对子序列案例的进一步改进可能需要涉及子序列的数据扩增。</p><p>​在过去的一年中，人们提出了一系列技术来提高HANS的性能。这些模型包括语法感知模型(Moradshahi et al., 2019; Pang et al., 2019)，旨在捕获预定义浅层启发法的辅助模型，以使主模型可以专注于稳健策略(Clark et al., 2019; He et al., 2019; Mahabadi and Henderson, 2019)以及提高难度训练实例权重的方法(Yaghoobzadeh et al., 2019)。尽管其中的一些方法在HANS上比我们的方法具有更高的准确性，包括更好的泛化了成分和子序列的情况（参见表A.4），但它们并不具有直接的可比性：我们的目标是在不修改模型或训练程序的情况下，评估训练集中的具有语法挑战性的实例是如何影响BERT的NLI的表现的。</p><h2><span id="6-讨论">6. 讨论</span></h2><p>​我们最佳效果的策略涉及通过对MNLI实例的主语&#x2F;宾语倒置转换而生成的少量MNLI实例来扩增MNLI训练集。这产生了可观的泛化能力：既是对另一个域而言的（HANS挑战集），更重要的是，其也适用于其他结构，如关系从句和介词短语。这支持了缺失连接假设：对一个结构进行少量扩增会引起抽象的语法敏感性，而不是仅仅通过为模型建立来自同一分布的案例样本来“接种(inoculating)”模型，以防止在挑战集上失败(Liu et al., 2019)。</p><p>​同时，倒置转换并未完全抵消启发法，特别是，这些模型在被动句上的表现较差。因此，对于这些结构，BERT的预训练可能在通过一个较小的扩增后，也仍无法产生强有力的语法表现形式；换句话说，这可能是我们的代表性不足假设成立的情况。该假设预测，作为单词预测模型的预训练BERT对于被动词处理困难，并且可能需要专门针对NLI任务学习其对应结构的特性；这可能需要大量的扩增实例。</p><p>​表现最佳的扩增策略是从一个单独原句生成前提&#x2F;假设对，这意味着该策略不依赖于NLI语料库。我们可以从任何语料库生成扩增实例，这使得我们有可能测试非常大的扩增集是否有效（当然，请注意，来自不同领域的扩增语句可能会影响在MNLI上本身的表现）。</p><p>​最终，我们希望有一个能在跨语言理解任务中也能在使用语法方面具有强归纳偏差的模型，即使在重叠启发法导致其在训练集上的高精度时也是如此。事实上，很难想象人们在理解一个句子时会完全忽略语法。另一种可选的方法是创建足以代表各种语言现象的训练集；众包工作者（理性的人）偏爱使用尽可能简单的生成策略，这可以通过对抗性过滤等方法来抵消(Nie et al., 2019)。然而，在此期间，我们得出结论，数据扩增是一个简单而有效的策略，其可以缓解BERT等模型中已知的推理启发法。</p><h2><span id="a-附录">A. 附录</span></h2><h3><span id="a1-微调细节">A.1 微调细节</span></h3><p>​我们在所有实验中都使用了bert-base-uncased版本的模型。按照标准，我们通过在MNLI上训练线性分类器从CLS标记的最终层嵌入预测标签，同时继续更新BERT参数，来对该预训练模型进行微调(Devlin et al., 2019)。每个模型的训练实例顺序都进行了打乱。所有模型都经过了三个epoch的训练。</p><h3><span id="a2-生成扩增实例">A.2 生成扩增实例</span></h3><p>​以下列表描述了我们使用的扩增策略。表A.1说明了应用于特定原句的所有策略。注意，倒置通常会改变句子的含义（侦探跟随着嫌疑犯(the detective followed the suspect)与嫌疑犯跟随着侦探(the suspect followed the detective)描述的并不是一个场景），但是被动语句并不会（侦探跟随着嫌疑犯(the detective followed the suspect)与嫌疑犯被侦探跟随(the suspect was followed by the detective)描述的是同一个场景）。</p><ul><li><p>倒置（原始前提）：对于原实例(p,h, →)，生成(p,INV(h), ↛)，其中INV返回主语与宾语调换了的原句，忽略原实例中标签为↛的实例。</p></li><li><p>倒置（转换假设）：对于原实例(p,h) （带有任意标签），丢弃前提p并且生成(h,INV(h), ↛)</p></li><li><p>被动（原始前提）：对于原实例(p,h) （带有任意标签），生成(p,PASS(h))，保持标签不变，其中PASS返回原句的被动语态版本（在不改变原意的情况下）</p></li><li><p>被动（转换假设）：对于原实例(p,h)，丢弃前提p，并且生成两个实例，一个携带蕴含标签——(h,PASS(h), →)，一个携带非蕴含标签——(h,PASS(h), ↛)</p></li></ul><p>​我们使用MNLI提供的选区分析，识别出MNLI中可以作为原句的及物句，但噪声较大的TELEPHONE类型除外。为此，我们搜索了恰好带有VP的一个NP子节点的矩阵S节点，其中主宾都是完整的名词短语（即，都不是像me这样的人称代词），而动词词缀不是be或have。我们保留了动词的原始时态，并在必要时修改了他们的一致性特征。（如：电影是Matt Dillon和Gary Sinise 出演(the movie stars Matt Dillon and Gary Sinise)改为Matt Dillon和Gary Sinise 出演了这部电影(Matt Dillon and Gary Sinise star the movie)）。</p><p>​所有策略中，最大的扩增集大小为1215。这个大小是我们可以从MNLI生成的最大扩增数据集确定的，该数据集是使用了上述的倒置结合原始前提方法。为了公平比较，即使对于可以生成更大数据集的策略，我们仍保持相同的大小。我们还通过随机抽样405个使用上述过程识别的案例创造了中型数据集，以及包含了101个实例的小数据集。我们对于每种策略仅执行一次这个过程：因此，运行仅在分类器的权重初始化和实例顺序方面有所不同，而在训练中包含的扩增实例没有变化。</p><p>​为了创建组合的扩增数据集，我们将倒置和被动化的数据集进行串联，然后随即丢掉一半实例（以使组合数据集的大小与其他数据集相匹配）。与其他数据集一样，我们只执行了一次：合并的扩增集在每次运行中相同。该过程的一个结果是，倒置和被动化的实例数量不完全相同。</p><h3><span id="a3-详细结果">A.3 详细结果</span></h3><p>​下列图表提供了我们试验的详细结果。</p><p>​表A.2显示了每种策略在MNLI上的平均准确率，以及在诊断三种启发法（词汇重叠启发法，子序列启发法和成分启发法）中的每一种的HANS案例的平均准确率，正确的标签是非蕴含（↛）。表A.3探究了最佳扩增策略—具有转换假设的主宾倒置，包括了在正确的标签是蕴含（→）和非蕴含（↛）两种情况。</p><p>​最后，末尾三个表格详细说明了通过倒置结合转换假设来对30个HANS子案例进行扩增的效果，并按设计按照诊断启发法来细分成表：词汇重叠启发法（表A.5）；子序列启发法（表A.6）和成分启发法（表A.7）。</p><img src="/2020/09/16/Syntactic%20Data%20Augmentation%20Increases%20Robustness%20to%20Inference%20Heuristics/表A.1.png" alt><p>表A.1：语法扩充策略（完整表格）</p><img src="/2020/09/16/Syntactic%20Data%20Augmentation%20Increases%20Robustness%20to%20Inference%20Heuristics/表A.2.png" alt><img src="/2020/09/16/Syntactic%20Data%20Augmentation%20Increases%20Robustness%20to%20Inference%20Heuristics/表A.3.png" alt><p>表A.3：使用主语&#x2F;宾语倒置结合转换假设的数据扩增对HANS准确性的影响。图表展示了在使用三种扩增集大小（101，405，1215个实例）来扩增的MNLI训练集后BERT的微调结果，也展示了在未经扩增的MNLI训练集上BERT的微调结果。</p><img src="/2020/09/16/Syntactic%20Data%20Augmentation%20Increases%20Robustness%20to%20Inference%20Heuristics/表A.4.png" alt><p>表A.4：不同的架构与训练方式所得到的HANS准确率，其被按照实例诊断的启发法与实例明确的标签拆分开来。除了MT-DNN+LF外，其余均采用了BERT作为基本模型。L，S与C分别代表词汇重叠、子序列和成分启发法。扩增集的大小为n&#x3D;101（小型），n&#x3D;405（中型），n&#x3D;1215（大型）。</p><img src="/2020/09/16/Syntactic%20Data%20Augmentation%20Increases%20Robustness%20to%20Inference%20Heuristics/表A.5.png" alt><p>表A.5：主语&#x2F;宾语倒置结合转换假设：图表展示了HANS子案例在诊断为词法重叠启发法时的结果，这些案例包括四种训练方案-未经扩增（只在MNLI上进行训练），和小型（n&#x3D;101），中型（n&#x3D;405），大型（n&#x3D;1215）扩增集的情况。置信准确度为0.5。图表上半部分：标签是非蕴含的案例情况。图表下半部分：标签是蕴含的案例情况。</p><img src="/2020/09/16/Syntactic%20Data%20Augmentation%20Increases%20Robustness%20to%20Inference%20Heuristics/表A.6.png" alt><p>表A.6：主语&#x2F;宾语倒置结合转换假设：图表展示了HANS子案例在诊断为子序列启发法时的结果，这些案例包括四种训练方案-未经扩增（只在MNLI上进行训练），和小型（n&#x3D;101），中型（n&#x3D;405），大型（n&#x3D;1215）扩增集的情况。图表上半部分：标签是非蕴含的案例情况。图表下半部分：标签是蕴含的案例情况。</p><img src="/2020/09/16/Syntactic%20Data%20Augmentation%20Increases%20Robustness%20to%20Inference%20Heuristics/表A.7.png" alt><p>表A.7：主语&#x2F;宾语倒置结合转换假设：图表展示了HANS子案例在诊断为成分启发法时的结果，这些案例包括四种训练方案-未经扩增（只在MNLI上进行训练），和小型（n&#x3D;101），中型（n&#x3D;405），大型（n&#x3D;1215）扩增集的情况。置信准确度为0.5。图表上半部分：标签是非蕴含的案例情况。图表下半部分：标签是蕴含的案例情况。</p>]]></content>
    
    
    <summary type="html">&lt;img src=&quot;/2020/09/16/Syntactic%20Data%20Augmentation%20Increases%20Robustness%20to%20Inference%20Heuristics/title.png&quot; alt&gt;

&lt;h1 id=&quot;通过语法数据扩增提升推理启发法的鲁棒性&quot;&gt;&lt;a href=&quot;#通过语法数据扩增提升推理启发法的鲁棒性&quot; class=&quot;headerlink&quot; title=&quot;通过语法数据扩增提升推理启发法的鲁棒性&quot;&gt;&lt;/a&gt;通过语法数据扩增提升推理启发法的鲁棒性&lt;/h1&gt;&lt;h2 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要&quot;&gt;&lt;/a&gt;摘要&lt;/h2&gt;&lt;p&gt;​	诸如BERT的预训练的神经模型在微调以执行自然语言推理（NLI）时，常常在标准数据集上展现出了高度准确性，但在受控的挑战集上，它们却表现出对语序敏感度的出奇缺乏。我们假设此问题并不主要因为预训练模型的局限性引起，而是由于缺乏众包的NLI样例引起的，而这些样例可能在微调阶段传递了语法结构的重要性。我们探索了几种方法来扩增标准训练集中语法丰富的实例，这些实例是通过对MNLI语料库的句子应用语法转换而生成的。而表现最好的扩增方法，主语&amp;#x2F;宾语倒置法，可以在不影响BERT对MNLI测试集性能的情况下，将BERT对受控实例的词序敏感度诊断从0.28提升至0.73。这种改进全面超过了用于数据扩增的特定结构，这表明了扩增可以使BERT学习到抽象语法的表现形式。&lt;/p&gt;</summary>
    
    
    
    <category term="论文阅读" scheme="https://values.keys.moe/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="论文阅读" scheme="https://values.keys.moe/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
  </entry>
  
  <entry>
    <title>Adversarial Robustness through Disentangled Representations</title>
    <link href="https://values.keys.moe/2020/06/19/Adversarial%20Robustness%20through%20Disentangled%20Representations/"/>
    <id>https://values.keys.moe/2020/06/19/Adversarial%20Robustness%20through%20Disentangled%20Representations/</id>
    <published>2020-06-18T16:38:38.000Z</published>
    <updated>2023-10-25T15:08:35.798Z</updated>
    
    <content type="html"><![CDATA[<img src="/2020/06/19/Adversarial%20Robustness%20through%20Disentangled%20Representations/封面.png" alt><h1><span id="通过解耦表征实现对抗鲁棒性">通过解耦表征实现对抗鲁棒性</span></h1><h2><span id="摘要">摘要</span></h2><p>​尽管深度学习模型具有优秀的实证性能，但诸多研究已发现了它们对对抗样本的脆弱性。它们易对具有难以察觉的对抗扰动的输入做出易受其影响的预测。尽管最近的研究已明显提升了在对抗训练策略下模型的鲁棒性，但自然准确性与对抗鲁棒性之间的差距依旧显著。为缓解此问题，本文中，我们假设鲁棒性与非鲁棒性表征是整体表征中相互耦合的两个基本成分。为实现对抗鲁棒性，自然与对抗样本的鲁棒表征应与非鲁棒部分分离，鲁棒表征的一致化可弥补准确性与鲁棒性之间的差距。受此启发，我们提出了一种称为深度鲁棒解耦表征网络（Deep Robust Representation Disentanglement Network, DRRDN）的新防御方法。具体而言，DRRDN使用解耦器从对抗样本与自然样本中提取并一致化鲁棒表征。理论分析保证了我们的方法能在具有良好解耦与一致化性能的情况下平衡鲁棒性与准确性。在基准数据集上的实验结果证明了我们方法的实证优势。</p><span id="more"></span><h2><span id="1-介绍">1. 介绍</span></h2><p>​我们提出了一种新的防御方法，称为深度鲁棒解耦表征网络（DRRDN）。具体而言，DRRDN遵循自动编码器（Auto-Encoder, AE）框架，通过编码器提取隐藏表征，使用解码器恢复原始输入。与典型的AE不同，DRRDN独特地使用两个解耦器将隐藏表征解耦为两个分支，一个是为了提取类特定的表征以进行分类，另一个是提取与类型无关的表征，以确定此表征是来自自然域还是对抗域。为了获得更好的解耦性能，我们使用互信息最小化以进一步正则化解耦器。与传统的AE类似，为保证一致性，我们亦使用了一个重构器以恢复解耦之前的整体表征。在推理阶段，我们使用特定类的表征进行预测。因此，无论输入的样本来源于哪个域，用于分类而提取的表征都能对对抗攻击有始终与一的鲁棒性。此外，我们从理论上证明，在满足解耦和一致化性能的前提下，自然表征与对抗表征之间的分布差异较小，所以自然准确度和对抗准确度应更接近。最后，我们在MNIST和CIFAR10数据集上实证了DRRDN在各种对抗攻击下的有效性。本工作主要贡献如下：</p><ul><li><p>我们提出了一种新的对抗防御方法DRRDN，它通过解耦与一致化来源于自然与对抗样本的鲁棒表征来消除对抗扰动的影响。</p></li><li><p>我们所提出的DRRDN是模型无关的，因此其可适用于任何模型架构。此外，DRRDN利用扰动的信息以实现完整的解耦能力。</p></li><li><p>我们通过理论分析，证明了我们方法的自然准确度和对抗鲁棒性之间的差距更小。</p></li><li><p>基准数据集的实验结果证明了DRRDN的优越性。</p></li></ul><h2><span id="2-方法">2. 方法</span></h2><p>​我们提出的DRRDN方法是在对抗训练的框架下更新的。为简单起见，我们在训练阶段使用PGD方法来生成对抗样本。本节先对对抗训练与PGD攻击流程做简单回顾。</p><h3><span id="准备工作">准备工作</span></h3><p>​考虑一个深度神经网络f(·)&#x3D;c(g(·; θ);w)∈Rk，其中g(·; θ)是以θ为参数的表征编码器，c(·;w)是以w为参数的分类器，k是分类的类型数量。对抗样本x’∈R HxWxC是由自然样本x∈R HxWxC通过添加对抗扰动δ生成的，而该扰动被限定在一个半径为ε的小球中，即：</p><p><span><img class="small-img-inline" src="/2020/06/19/Adversarial%20Robustness%20through%20Disentangled%20Representations/公式0.png" alt></span></p><p>​除此之外，x’应导致网络的预测发生改变。对抗训练的基本思想是交替地生成对抗样本与使用它们训练。总目标如下：</p><p><span><img class="small-img-inline" src="/2020/06/19/Adversarial%20Robustness%20through%20Disentangled%20Representations/公式1.png" alt></span></p><p>​然而，对于复杂的高维数据，内部最大化通常难以实现。因此，PGD使用以下T步投影来近似生成ε球内的最强对抗样本：</p><p><span><img class="small-img-inline" src="/2020/06/19/Adversarial%20Robustness%20through%20Disentangled%20Representations/公式2.png" alt></span></p><p>​其中，Proj Bεp (x)(·)表示投影操作，其将扰动样本投影至ε半径的球Bεp (x)中，以确保对抗样本的相似度。t∈[1,T]，是投影步数的数量，α是单步幅度，sign(·)返回梯度中每个元素的符号。注意，如果总的更新步数T为1，则PGD方法就会退化为较弱的FGSM攻击。在通过公式(2)得到x’后，把它喂给模型，像公式(1)那样优化外部最小化，以加强模型鲁棒性。</p><h3><span id="深度鲁棒解耦表征网络drrdn">深度鲁棒解耦表征网络（DRRDN）</span></h3><p>​DRRDN的基础理论假设整体表征z&#x3D;g(x;θ)∼pθ(z)（相似的，z’&#x3D;g(x’;θ)∼pθ(z’)）由两个耦合的分支组成，分别为特定类表征zs与类型无关表征zi，其中pθ(z)是原始数据分布p(x)的前推度量。</p><p>​具体而言，纯zs被指定为分类任务，因此，无论是zs还是zs’，他们都应能被正确分类。在DRRDN中，我们使用一个解耦器ds(·; φs)来分离z和z’的特定类表征：</p><p><span><img class="small-img-inline" src="/2020/06/19/Adversarial%20Robustness%20through%20Disentangled%20Representations/公式3.png" alt></span></p><p>​回顾特定类表征的工作，我们将分类损失（如交叉熵）应用于ds(·; φs)上。</p><p><span><img class="small-img-inline" src="/2020/06/19/Adversarial%20Robustness%20through%20Disentangled%20Representations/公式4.png" alt></span></p><p>​其中，cs(·, ws) ∈ Rk是以ws为参数的特定类分类器。</p><p>​然而，仅分离特定类的表征可能并不足以实现完全解耦。捕获扰动噪声信息的类型无关的部分也应被建模，并从整体表征中去除。与ds类似，我们设计了一个类型无关解耦器di(·; φi)来建模类型无关的信息：</p><p>​<span><img class="small-img-inline" src="/2020/06/19/Adversarial%20Robustness%20through%20Disentangled%20Representations/公式5.png" alt></span></p><p>​请注意，类型无关的表征应捕捉自然表征与对抗表征的特点，即zi应是可与zi’解耦的。因此，我们在di上应用二元分类损失，为：</p><p>​<span><img class="small-img-inline" src="/2020/06/19/Adversarial%20Robustness%20through%20Disentangled%20Representations/公式6.png" alt></span></p><p>​其中ci(·,wi)∈[0,1]是以wi为参数的类型无关分类器，其试图将不管是特定类的，还是类型无关的自然表征与对抗表征都进行区分。</p><p>​一个好的解耦应产生相互独立的部分，这些部分不共享彼此之间的信息。为了实现这一目标，我们考虑以下两个方面：功能排他性与互信息最小化。首先，功能排他性是指被解耦的分支应有且仅有一项特定任务来胜任，而对其余任务无能为力。公式(4)与(6)已确保每个解耦的表征都能执行相应的任务，但并未对对应表征施加任何约束，这可能导致扰动信息的残留，或特定类表征中鲁棒信息的损失。因此，受生成对抗网络的启发，我们通过使用对抗损失来更新ds，以欺骗训练好的ci*：</p><p>​<span><img class="small-img-inline" src="/2020/06/19/Adversarial%20Robustness%20through%20Disentangled%20Representations/公式7.png" alt></span></p><p>​其中ci*(·,wi*)是公式6中训练好的类型无关的分类器。除此之外，我们正则化优化好的cs*对di的预测熵，使其尽可能大，以保证di不能被正确分类：</p><p><span><img class="small-img-inline" src="/2020/06/19/Adversarial%20Robustness%20through%20Disentangled%20Representations/公式8.png" alt></span></p><p>​其中，cs*(·,ws*)使用公式(4)优化，公式(8)被用来最小化cs*(·,ws*)对di的负信息熵。</p><p>​其次，对于完全解耦，ds与di的输出之间的依赖性应尽可能小。在我们的方法中，我们对ds与di进行了互信息最小化（MI）操作，以更好地解耦对抗扰动的影响。然而，MI的计算通常是难以实现的。因此，在我们的模型中，我们采用互信息神经估计器（Mutual Information Neural Estimator, MINE）来进行MI的无偏估计。具体而言，给定联合分布的特定类与类型无关表征对<span><img class="small-img-inline" src="/2020/06/19/Adversarial%20Robustness%20through%20Disentangled%20Representations/公式8.1.png" alt></span>的n个样本，以及来源于边缘分布的<span><img class="small-img-inline" src="/2020/06/19/Adversarial%20Robustness%20through%20Disentangled%20Representations/公式8.2.png" alt></span>的n对样本（请注意，被解耦的表征可能来源于自然或对抗表征），MINE通过蒙特卡洛积分经验地估计ds与di之间的MI，如下所示：</p><p><span><img class="small-img-inline" src="/2020/06/19/Adversarial%20Robustness%20through%20Disentangled%20Representations/公式9.png" alt></span></p><p>​其中，T(·; ψ)是以ψ为参数的神经网络。</p><p>​最后，遵循标准的AE架构，我们使用了一个重构器以恢复完整表征，以保持解耦表征的跨周期一致性。本文中，为简单起见，我们使用了一个L2重构器：</p><p><span><img class="small-img-inline" src="/2020/06/19/Adversarial%20Robustness%20through%20Disentangled%20Representations/公式10.png" alt></span></p><p>​其中，r(·,·; θrec)是以θrec为参数的重构器。</p><p><span><img class="small-img-inline" src="/2020/06/19/Adversarial%20Robustness%20through%20Disentangled%20Representations/算法1.png" alt></span></p><p>​请注意，在推理阶段与生成对抗样本时，我们仅使用基本的表征提取器g(·; φ)，特定类解耦器ds(·; φ)与特定类分类器cs(·;ws)。为了更好的理解DRRDN的结构，我们在图1中给出了说明。我们还在算法1中总结了DRRDN的具体训练步骤。具体来说，DRRDN的总体目标可总结如下：</p><p>​<span><img class="small-img-inline" src="/2020/06/19/Adversarial%20Robustness%20through%20Disentangled%20Representations/公式11.png" alt></span></p><p><span><img class="small-img-inline" src="/2020/06/19/Adversarial%20Robustness%20through%20Disentangled%20Representations/图1.png" alt></span></p><p>图1 DRRDN架构示意图。g(·;θ) 表示以θ为参数的特征提取器，ds(·; φs)和di(·; φi)分别是以φs和φi为参数的特定类和类型无关表征的解耦器，cs是预测输入表征类别的分类器，ci是区分表征来源于哪个域的鉴别器，r是重构解码器，z表示每个模块的输出表征。</p><h2><span id="3-理论分析">3. 理论分析</span></h2><p>​本节中，我们提供对所提方法的理论理解。Zhang等人(2019)指出，模型在自然样本和对对抗样本的鲁棒性之间存在一种权衡。我们假设这种权衡的一个可能原因是分类器不能泛化来自自然与对抗分布的表征。在公式(6)和(7)中，我们定义了一个对抗损失以正则化特定类的解耦器。在下面的命题中，我们证明，通过优化(6)和(7)中的目标，来自自然和对抗样本的特定类表征可以相互一致。因为我们只使用特定类表征进行推理，因此可以减少模型准确性与鲁棒性之间的差距。</p><p><strong>命题1</strong> 当达到全局最小值<span><img class="small-img-inline" src="/2020/06/19/Adversarial%20Robustness%20through%20Disentangled%20Representations/命题1.png" alt></span>和<span><img class="small-img-inline" src="/2020/06/19/Adversarial%20Robustness%20through%20Disentangled%20Representations/命题2.png" alt></span>时，pφs(zs)等于pφs (zs’)。</p><p>证明：根据Goodfellow等的研究，给定自然与对抗的特定类表征zs∼ pφs(zs)，zs’ ∼ pφs(zs’)，最优类型无关分类器ci*(·,wi*)满足：</p><p><span><img class="small-img-inline" src="/2020/06/19/Adversarial%20Robustness%20through%20Disentangled%20Representations/公式12.png" alt></span></p><p>​使用固定的最优类型无关分类器ci*(·,wi*)，公式(7)可被转换为：</p><p><span><img class="small-img-inline" src="/2020/06/19/Adversarial%20Robustness%20through%20Disentangled%20Representations/公式13.png" alt></span></p><p>因此，当pφs(zs)等于pφs (zs’)时，可达到最优<span><img class="small-img-inline" src="/2020/06/19/Adversarial%20Robustness%20through%20Disentangled%20Representations/命题3.png" alt></span>，命题证明完毕。</p><h2><span id="4-实验结果">4. 实验结果</span></h2><p>​本节中，我们通过各种实验验证我们方法的有效性。我们首先进行白盒与黑盒攻击，以比较DRRDN与最先进方法的鲁棒性。本节还对解耦的影响进行了进一步的分析。</p><h3><span id="设置">设置</span></h3><p><strong>数据集</strong>。我们使用MNIST和CIFAR10作为评估防御方法的基准数据集。我们保持默认训练&#x2F;测试集的分割比例。</p><p><strong>实现</strong>。我们使用一个四层卷积网络作为MNIST数据集的基本表征提取器。DRRDN的解耦器和分类器是全连接层，为公平比较，DRRDN分类器的容量与基准模型相同。对于CIFAR10数据集，我们采用WRN-28-10作为表征编码器。在设计解耦器和分类器时，我们遵循了同对MNIST数据集相同的原则。在MINE和重构器模块中，为简单起见，我们亦采用了全连接层。针对优化方面，我们使用SGD优化器，初始学习率在CIFAR10上为1e-2和1e-1。训练总epoch为100次，在此期间，分别在第55、75和90个epoch上衰减0.01的学习率。</p><p><strong>防御设置</strong>。我们在对抗训练框架下训练DRRDN。对于训练攻击，我们设置对MNIST的扰动ε&#x3D;0.3，步长α&#x3D;0.01；对CIFAR10的扰动ε&#x3D;0.031，步长α&#x3D;0.007。对于MNIST和CIFAR10，生成训练攻击的更新迭代次数分别为40和10。为简单起见，我们仅验证l∞范数攻击。</p><h3><span id="不同攻击下对抗防御的表现">不同攻击下对抗防御的表现</span></h3><p><strong>白盒攻击</strong>。白盒攻击假定攻击者可以访问目标模型的结构与参数，并可直接根据梯度生成对抗样本。因此，标准训练模型的鲁棒性非常脆弱。我们采用三种类型的攻击进行验证，分别为FGSM、PGD和CW。PGD和CW对MNIST和CIFAR产生的攻击步长分别为0.01和0.003，FGSM的步长放大了10倍。我们遵循Carlini和Wagner的模式进行CW攻击。我们将DRRDN的性能与标准方式下训练的模型（即只用自然数据训练），以及在Madry和TRADES下训练的模型进行鲁棒性的比较。表1总结了自然准确度和鲁棒准确度。</p><p><span><img class="small-img-inline" src="/2020/06/19/Adversarial%20Robustness%20through%20Disentangled%20Representations/表1.png" alt></span></p><p>表1 在不同对抗攻击下模型对(a)MNIST和(b)CIFAR10的表现</p><p>​从表1我们可以得到，标准模型几乎无法防御任何方式的攻击。当用对抗样本进行测试时，除如FGSM这种稍弱的攻击外，标准模型的鲁棒准确性会突然下降到几乎为0。Madry和TRADES证明了它们在防御对抗攻击方面的有效性，然而，它们自然准确度和鲁棒准确度之间的差距依旧较大。我们归因该现象为自然样本与对抗样本间表征分布不一致，所以模型难以同时泛化自然样本与对抗样本。相比之下，我们提出的DRRDN不仅在对抗分类准确度上取得了更高的效果，且在自然和对抗准确度之间保持了较小的差距。这表明，通过解耦和一致化特定类表征，可以让自然准确度和对抗鲁棒性之间的联系更加紧密。</p><p><strong>黑盒攻击</strong>。我们进一步用黑盒攻击验证我们的模型。顾名思义，黑盒攻击假设攻击者并不知道目标模型防御的细节。尽管黑盒攻击通常比白盒弱得多，但在现实应用中，黑盒攻击设置更自然，因模型的具体设计往往是并不明确的，且其可证明模型对不同攻击的模型鲁棒性的可迁移性。</p><p>​对于黑盒攻击，我们先独立地训练代用模型，包括Standard、Madry、TRADES和DRRDN，根据他们的梯度使用PGD方法来产生对抗样本。之后，由代用模型产生的对抗样本将用于攻击其他目标模型。具体而言，在生成黑盒攻击时，我们先使用与白盒攻击相同的模型结构。为方便起见，PGD方法也使用同一组参数（即MINIST ε&#x3D;0.3，α&#x3D;0.01，40次迭代；CIFAR10 ε&#x3D;0.31，α&#x3D;0.003，20次迭代）。黑盒攻击下的分类结果如表2所示。</p><p><span><img class="small-img-inline" src="/2020/06/19/Adversarial%20Robustness%20through%20Disentangled%20Representations/表2.png" alt></span></p><p>表2 在不同对抗攻击下模型对(a)MNIST和(b)CIFAR10的表现</p><p>​垂直观察表2，我们可以看到DRRDN赋予了模型对黑盒攻击最强的鲁棒性，这意味着DRRDN的鲁棒性并非针对单种攻击，而是普适的。这可能是因为类型无关的表征可以很好地区分自然分布与对抗分布，并将不同的攻击引导至非自然分布上。而有一个有趣的现象是，当我们横向观察表2，DRRDN提供了最弱的攻击。事实上，对这一现象存在自然且直观的解释。我们使用DRRDN生成基于特定类分支的梯度的对抗扰动。特定类表征可捕捉到输入的基本特征（理想情况下，比只用自然数据训练的标准模型所提取出的表征更纯粹，因为自然分布的冗余信息也被消除了）。因此，DRRDN产生的对抗扰动应比标准模型产生的扰动更弱。</p><h3><span id="解耦的影响">解耦的影响</span></h3><p>​上述实验表明，DRRDN能使DNN模型对白盒与黑盒对抗攻击具有更强的鲁棒性。我们想知道，我们精心设计的解耦机制是否真的有利于模型的鲁棒性。本节中，我们进行两个探索性实验来验证解耦的有效性。</p><p><strong>混合重构表征</strong>。在方法部分，我们详细介绍了特定类表征和分类无关表征的功能。特定类解耦器目标是仅保留分类信息的表征，而分类无关表征的目的是吸收扰动的信息。当然，从自然样本和对抗样本中解耦出的特定类的表征应能很好地逐个泛化，因为它们的分布基本相同。因此，如果DRRDN的解耦机制真的有作用，不难想象，若将具有不同分布标签（即自然分布和对抗分布）的特定类和类型无关的表征结合起来，重建混合表征，我们将在标准模型的、在混合表征上微调的分类器上得到不同的分类结果。因为混合表征实际上是从两个分布中还原了原始表征。在与MNIST数据集上的白盒攻击相同的设置下，我们使用标准训练模型进行基于混合重构表征的分类实验。结果如图3所示。请注意，通过排列组合，我们可以重建四种混合表征，分别为zs+zi，zs‘+zi，zs+zi’和zs’+zi’。</p><p><span><img class="small-img-inline" src="/2020/06/19/Adversarial%20Robustness%20through%20Disentangled%20Representations/表3.png" alt></span></p><p>表3 混合表征的分类能力</p><p>​表3中，非常明显，与zi’混合的重建表征（即来自对抗样本的类型无关表征）通常更难被标准模型正确分类。相反，包含zi的混合表征（即来自自然样本的类型无关表征）可以被很好地区分。这意味着分布特征依被成功分解至类型无关表征中去了。</p><p><strong>解耦表征的可视化</strong>。为更好展示拆分效果，我们对拆分后的表征进行了可视化实验。可视化实验亦基于MNIST数据集的白盒攻击的实验设置。在得到用DRRDN训练的鲁棒模型后，我们首先从自然与对抗测试数据中提取特定类和类型无关的表征，然后通过t-SNE将高维表征压缩成一个二维特征。我们在图2展示了可视化效果。不同类别的压缩特征用不同颜色表示。除了特定类和类型无关的表征外，我们亦对标准模型的表征进行了可视化比较。</p><p><span><img class="small-img-inline" src="/2020/06/19/Adversarial%20Robustness%20through%20Disentangled%20Representations/图2.png" alt></span></p><p>图2：MNIST上的特定类、类型无关和标准表征</p><p>​图2 的第一和第三行分别展示了来自自然分布和对抗分布的压缩表征，中间一行为两个分布的压缩表征的组合，其用来表明分布的一致性。我们对图2的压缩表征有如下三个重要发现：1）自然与对抗样本中。只有特定类的表征是可被分类的；2）只有类型无关表征可区分自然与对抗分布；3）在中间一行，自然与对抗样本中提取的特定类表征比标准表征更紧凑与吻合。上述特点亦证明解耦的效果。</p><p><strong>消融实验</strong>。我们进一步进行了消融实验，分别从整个目标函数中去除互信息项（公式(9)）或重构项（公式(10)）。如表4所示，我们发现在没有互信息的情况下，DRRDN的准确度下降比没有重构项的情况下要严重得多，这也展示了完全解耦的重要性。</p><p><span><img class="small-img-inline" src="/2020/06/19/Adversarial%20Robustness%20through%20Disentangled%20Representations/表4.png" alt></span></p><p>表4 对MNIST数据集的消融实验。DRRDN（最终算法）、DRRDN-MI（无互信息项）与DRRDN-REC（无重构项）。</p><h2><span id="5-总结">5. 总结</span></h2><p>​本文中，我们研究了针对对抗攻击的防御问题。我们提出了DRRDN模型来拆分特定类表征，其中关于自然与对抗领域特征的信息已被完全去除。我们亦通过使用GAN理论为我们提出的模型提供了理论保证。基于基准数据集的实证评估进一步证明了DRRDN与最先进防御方法相比的优越性。</p>]]></content>
    
    
    <summary type="html">&lt;img src=&quot;/2020/06/19/Adversarial%20Robustness%20through%20Disentangled%20Representations/封面.png&quot; alt&gt;

&lt;h1 id=&quot;通过解耦表征实现对抗鲁棒性&quot;&gt;&lt;a href=&quot;#通过解耦表征实现对抗鲁棒性&quot; class=&quot;headerlink&quot; title=&quot;通过解耦表征实现对抗鲁棒性&quot;&gt;&lt;/a&gt;通过解耦表征实现对抗鲁棒性&lt;/h1&gt;&lt;h2 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要&quot;&gt;&lt;/a&gt;摘要&lt;/h2&gt;&lt;p&gt;​	尽管深度学习模型具有优秀的实证性能，但诸多研究已发现了它们对对抗样本的脆弱性。它们易对具有难以察觉的对抗扰动的输入做出易受其影响的预测。尽管最近的研究已明显提升了在对抗训练策略下模型的鲁棒性，但自然准确性与对抗鲁棒性之间的差距依旧显著。为缓解此问题，本文中，我们假设鲁棒性与非鲁棒性表征是整体表征中相互耦合的两个基本成分。为实现对抗鲁棒性，自然与对抗样本的鲁棒表征应与非鲁棒部分分离，鲁棒表征的一致化可弥补准确性与鲁棒性之间的差距。受此启发，我们提出了一种称为深度鲁棒解耦表征网络（Deep Robust Representation Disentanglement Network, DRRDN）的新防御方法。具体而言，DRRDN使用解耦器从对抗样本与自然样本中提取并一致化鲁棒表征。理论分析保证了我们的方法能在具有良好解耦与一致化性能的情况下平衡鲁棒性与准确性。在基准数据集上的实验结果证明了我们方法的实证优势。&lt;/p&gt;</summary>
    
    
    
    <category term="论文阅读" scheme="https://values.keys.moe/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="论文阅读" scheme="https://values.keys.moe/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
  </entry>
  
  <entry>
    <title>通过Xrdp实现Windows远程访问Ubuntu 16.04</title>
    <link href="https://values.keys.moe/2019/03/05/%E9%80%9A%E8%BF%87Xrdp%E5%AE%9E%E7%8E%B0Windows%E8%BF%9C%E7%A8%8B%E8%AE%BF%E9%97%AEUbuntu-16-04/"/>
    <id>https://values.keys.moe/2019/03/05/%E9%80%9A%E8%BF%87Xrdp%E5%AE%9E%E7%8E%B0Windows%E8%BF%9C%E7%A8%8B%E8%AE%BF%E9%97%AEUbuntu-16-04/</id>
    <published>2019-03-05T03:25:45.000Z</published>
    <updated>2022-09-28T06:24:41.208Z</updated>
    
    <content type="html"><![CDATA[<p>​目前网上的大量教程都是需要安装xfac4或者xubuntu桌面系统才能实现远程连接。因为xrdp支持在13.10之后版本就已经不支持的Gnome了和原生Unity桌面，所以网上很多方法都是安装能够被xdrp支持的第三方xfac4或者xubuntu桌面系统间接达到远程控制Ubuntu。<br>​本文提供如何使用Xrdp访问原生Ubuntu桌面。</p><span id="more"></span><h2><span id="step1-下载tigervnc-server软件包">Step.1 下载TigerVNC Server软件包</span></h2><p>下载地址：<br><a href="http://www.c-nergy.be/downloads/tigervncserver_1.6.80-4_amd64.zip">http://www.c-nergy.be/downloads/tigervncserver_1.6.80-4_amd64.zip</a></p><h2><span id="step2-安装tigervnc-server">Step.2 安装TigerVNC Server</span></h2><p>1.打开终端，进入到刚刚你你下载TigerVNC Server的存放目录</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">cd</span> 下载</span><br></pre></td></tr></table></figure><p>2.执行安装指令</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo dpkg -i tigervncserver_1.<span class="number">6.80</span>-<span class="number">4</span>_amd64.<span class="keyword">deb</span></span><br><span class="line">或者</span><br><span class="line">sudo apt-<span class="built_in">get</span> install tightvncserver</span><br></pre></td></tr></table></figure><p>过程中如果出现警告信息和错误信息，原因是没有相对应的依赖包。<br>执行</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-<span class="built_in">get</span> install -<span class="keyword">f</span></span><br></pre></td></tr></table></figure><p>然后在执行之前的安装命令。</p><h2><span id="step3-安装xrdp">Step.3 安装Xrdp</span></h2><p>终端输入安装命令</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-<span class="built_in">get</span> install xrdp -<span class="keyword">y</span></span><br></pre></td></tr></table></figure><h2><span id="step4-配置xrdp">Step.4 配置Xrdp</span></h2><p>需要通过xrdp连接到桌面，<br>需要正确配置相关信息并填充到.xsession文件（针对每个用户）<br>或&#x2F;etc&#x2F;startwm.sh（针对所有用户）<br>命令如下：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">echo</span> unity&gt;~/.xsession</span><br><span class="line">或者</span><br><span class="line">sudo sed -i.bak <span class="string">&#x27;/fi/a #xrdp multi-users \n unity \n&#x27;</span> /etc/xrdp/startwm.<span class="keyword">sh</span></span><br></pre></td></tr></table></figure><h2><span id="step5-重启xrdp服务">Step.5 重启Xrdp服务</span></h2><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo service xrdp restart</span><br></pre></td></tr></table></figure><h2><span id="step6-开启桌面共享功能">Step.6 开启桌面共享功能</span></h2><img src="/2019/03/05/%E9%80%9A%E8%BF%87Xrdp%E5%AE%9E%E7%8E%B0Windows%E8%BF%9C%E7%A8%8B%E8%AE%BF%E9%97%AEUbuntu-16-04/桌面共享.jpg" alt><p>系统-&gt;首选项-&gt;桌面共享，或者直接搜索桌面共享功能<br>进入后<br>将【允许其他人查看您的桌面】勾上，<br>【自动配置UPnP路由器开放和转发接口】勾上，如图所示</p><img src="/2019/03/05/%E9%80%9A%E8%BF%87Xrdp%E5%AE%9E%E7%8E%B0Windows%E8%BF%9C%E7%A8%8B%E8%AE%BF%E9%97%AEUbuntu-16-04/桌面共享2.jpg" alt><p>之后配置基本结束。Windows可以通过mstc直接通过IP访问Ubuntu。</p><hr><p>注：Ubuntu18尝试后似乎没有桌面共享功能的选项。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;​		目前网上的大量教程都是需要安装xfac4或者xubuntu桌面系统才能实现远程连接。因为xrdp支持在13.10之后版本就已经不支持的Gnome了和原生Unity桌面，所以网上很多方法都是安装能够被xdrp支持的第三方xfac4或者xubuntu桌面系统间接达到远程控制Ubuntu。&lt;br&gt;​		本文提供如何使用Xrdp访问原生Ubuntu桌面。&lt;/p&gt;</summary>
    
    
    
    <category term="Ubuntu" scheme="https://values.keys.moe/categories/Ubuntu/"/>
    
    
    <category term="Ubuntu" scheme="https://values.keys.moe/tags/Ubuntu/"/>
    
  </entry>
  
  <entry>
    <title>树莓派人脸识别-face-recognition的安装与应用</title>
    <link href="https://values.keys.moe/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB-face-recognition%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E5%BA%94%E7%94%A8/"/>
    <id>https://values.keys.moe/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB-face-recognition%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E5%BA%94%E7%94%A8/</id>
    <published>2019-03-02T03:06:57.000Z</published>
    <updated>2022-09-27T17:08:54.800Z</updated>
    
    <content type="html"><![CDATA[<img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB-face-recognition%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E5%BA%94%E7%94%A8/人脸识别.gif" alt><span id="more"></span><img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB-face-recognition%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E5%BA%94%E7%94%A8/face_recongnition.png" alt>该库可以通过python或者命令行即可实现人脸识别的功能。使用dlib深度学习人脸识别技术构建，在户外脸部检测数据库基准（Labeled Faces in the Wild）上的准确率为99.38%。<h2><span id="安装过程"><strong>安装过程</strong></span></h2><p>先在终端下安装一大堆需要的库：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-<span class="built_in">get</span> <span class="keyword">update</span> sudo apt-<span class="built_in">get</span> install build-essential \</span><br><span class="line">cmake \</span><br><span class="line">gfortran \</span><br><span class="line">git \</span><br><span class="line">wget \</span><br><span class="line">curl \</span><br><span class="line">graphicsmagick \</span><br><span class="line">libgraphicsmagick1-dev \</span><br><span class="line">libatlas-dev \</span><br><span class="line">libavcodec-dev \</span><br><span class="line">libavformat-dev \</span><br><span class="line">libboost-<span class="keyword">all</span>-dev \</span><br><span class="line">libgtk2.<span class="number">0</span>-dev \</span><br><span class="line">libjpeg-dev \</span><br><span class="line">liblapack-dev \</span><br><span class="line">libswscale-dev \</span><br><span class="line">pkg-config \</span><br><span class="line"><span class="keyword">python3</span>-dev \</span><br><span class="line"><span class="keyword">python3</span>-numpy \</span><br><span class="line"><span class="keyword">python3</span>-pip \</span><br><span class="line">zip</span><br></pre></td></tr></table></figure><p>如果使用树莓派的摄像头（CSI接口），执行下面的命令：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-<span class="built_in">get</span> install <span class="keyword">python3</span>-picamera</span><br><span class="line">sudo pip3 install --upgrade picamera[array]</span><br></pre></td></tr></table></figure><p>下载安装dlib：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mkdir</span> -<span class="keyword">p</span> dlib </span><br><span class="line">git clone -<span class="keyword">b</span> <span class="string">&#x27;v19.6&#x27;</span> --single-branch https://github.<span class="keyword">com</span>/davisking/dlib.git dlib/ </span><br><span class="line"><span class="keyword">cd</span> ./dlib</span><br><span class="line">sudo <span class="keyword">python3</span> setup.<span class="keyword">py</span> install --<span class="keyword">compiler</span>-flags <span class="string">&quot;-mfpu=neon&quot;</span></span><br></pre></td></tr></table></figure><p>安装face_recognition：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo pip3 install face_recognition</span><br></pre></td></tr></table></figure><p>下载示例代码并尝试运行（可选）：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git clone --single-branch https://github.<span class="keyword">com</span>/ageitgey/face_recognition.git</span><br><span class="line"><span class="keyword">cd</span> ./face_recognition/examples</span><br><span class="line"><span class="keyword">python3</span> facerec_on_raspberry_pi.<span class="keyword">py</span></span><br></pre></td></tr></table></figure><p><strong>注</strong>：过程缺少库时，使用</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">apt-cache <span class="built_in">search</span> 库名</span><br></pre></td></tr></table></figure><p>来搜索到那个库的安装包，然后用</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-<span class="built_in">get</span> install 包名</span><br></pre></td></tr></table></figure><p>来安装。</p><p><strong>例如</strong>：缺少了libatlas.so.3，那我们就用<strong>apt-cache search libatlas</strong>来搜索，发现它的包名叫libatlas3-base，所以我们用<strong>sudo apt-get install libatlas3-base</strong>来安装。<br>后面测试摄像头的时候也会遇到这样的问题，解决办法是一样的。 </p><p>待把CSI接口树莓派摄像头装上后，在raspi-config中启用摄像头，然后重启。<br>（详见博客内关于CSI摄像头的另一篇文章）</p><p>运行一下实时人脸识别的代码（可选）：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">python3</span> facerec_from_webcam_faster.<span class="keyword">py</span></span><br></pre></td></tr></table></figure><p>过程可能报错，import cv2的时候缺少库，然后根据提示用之前安装方法安装就好了。装完一个库再运行的时候，发现又提示缺少别的库，然后再安装缺少的库。反复个多次，把缺少的库都装好即可。</p><p>再次运行的时候，会报别的错误，出错的代码是</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">small_frame = cv2.<span class="keyword">resize</span>(frame, (<span class="number">0</span>, <span class="number">0</span>), fx=<span class="number">0.25</span>, fy=<span class="number">0.25</span>)</span><br></pre></td></tr></table></figure><p>这是因为video_capture.read()没有读到图片。<br>树莓派中的camera module是放在&#x2F;boot&#x2F;目录中以固件形式加载的，不是一个标准的V4L2的摄像头驱动，所以加载起来之后会找不到&#x2F;dev&#x2F;video0的设备节点。<br><strong>解决方法</strong>：转载<a href="https://blog.csdn.net/deiki/article/details/71123947">https://blog.csdn.net/deiki/article/details/71123947</a></p><p>之后可以使用下面的命令来加载驱动模块：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo modprobe bcm2835-v4l2</span><br></pre></td></tr></table></figure><p>如果想开机自动加载，我们可以修改&#x2F;etc&#x2F;modules文件，添加一行：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bcm2835-v4l2</span><br></pre></td></tr></table></figure><p>如下图所示：<br><img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB-face-recognition%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E5%BA%94%E7%94%A8/bcm2835-v4l2.png" alt></p><p>安装过程基本完毕。</p><h2><span id="个人提供的样例代码"><strong>个人提供的样例代码：</strong></span></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env </span></span><br><span class="line"><span class="comment">#coding=utf-8</span></span><br><span class="line"><span class="comment">#从目录中读取一堆文件</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> face_recognition</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> RPi.GPIO</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">RPi.GPIO.setmode(RPi.GPIO.BCM)</span><br><span class="line"></span><br><span class="line">RPi.GPIO.setwarnings(<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">buzzer=<span class="number">4</span></span><br><span class="line">RPi.GPIO.setup(buzzer,RPi.GPIO.OUT)</span><br><span class="line"></span><br><span class="line">RPi.GPIO.output(buzzer,<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">strangerAppear=<span class="literal">False</span></span><br><span class="line">strangerNum=<span class="number">0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">demo_filelist=[]</span><br><span class="line">demo_face_encodings=[]</span><br><span class="line">demo_face_names=[]</span><br><span class="line"><span class="keyword">for</span> f <span class="keyword">in</span> os.listdir(<span class="string">&#x27;/home/pi/demo/face_recognition/jpg//&#x27;</span>):</span><br><span class="line">    demo_filelist.append(f)</span><br><span class="line">    demo_face_names.append(f[:-<span class="number">4</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> filename <span class="keyword">in</span> demo_filelist:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">u&#x27;正在加载....&#x27;</span>+filename)</span><br><span class="line">    demo_image = face_recognition.load_image_file(<span class="string">&#x27;/home/pi/demo/face_recognition/jpg//&#x27;</span>+filename)</span><br><span class="line">    face_encoding = face_recognition.face_encodings(demo_image)[<span class="number">0</span>]</span><br><span class="line">    demo_face_encodings.append(face_encoding)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize some variables</span></span><br><span class="line">face_locations = []</span><br><span class="line">face_encodings = []</span><br><span class="line">face_names = []</span><br><span class="line">process_this_frame = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Get a reference to webcam #0 (the default one)</span></span><br><span class="line">video_capture = cv2.VideoCapture(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    <span class="comment"># Grab a single frame of video</span></span><br><span class="line">    ret, frame = video_capture.read()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Resize frame of video to 1/4 size for faster face recognition processing</span></span><br><span class="line">    small_frame = cv2.resize(frame, (<span class="number">0</span>, <span class="number">0</span>), fx=<span class="number">0.3</span>, fy=<span class="number">0.3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Only process every other frame of video to save time</span></span><br><span class="line">    <span class="keyword">if</span> process_this_frame==<span class="number">1</span>:</span><br><span class="line">        <span class="comment"># Find all the faces and face encodings in the current frame of video</span></span><br><span class="line">        face_locations = face_recognition.face_locations(small_frame)</span><br><span class="line">        face_encodings = face_recognition.face_encodings(small_frame, face_locations)</span><br><span class="line">   <span class="comment">#     print(u&quot;我检测到了&#123;&#125;张脸。&quot;.format(len(face_locations)))</span></span><br><span class="line">        face_names = []</span><br><span class="line">        <span class="keyword">for</span> face_encoding <span class="keyword">in</span> face_encodings:</span><br><span class="line">        <span class="comment"># See if the face is a match for the known face(s)</span></span><br><span class="line">            <span class="keyword">match</span> = face_recognition.compare_faces(demo_face_encodings, face_encoding,tolerance=<span class="number">0.6</span>)<span class="comment">#</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="keyword">match</span>)</span><br><span class="line">            name = <span class="string">&quot;Unknown&quot;</span></span><br><span class="line">            i=-<span class="number">1</span></span><br><span class="line">            <span class="keyword">for</span> m <span class="keyword">in</span> <span class="keyword">match</span>:</span><br><span class="line">                i+=<span class="number">1</span></span><br><span class="line">                <span class="keyword">if</span> m:</span><br><span class="line">                    name= demo_face_names[i]</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span>(name==<span class="string">&quot;Unknown&quot;</span>):</span><br><span class="line">                RPi.GPIO.output(buzzer,<span class="literal">False</span>)</span><br><span class="line">                frame_1=cv2.flip(frame,<span class="number">1</span>)</span><br><span class="line">                path=<span class="string">&quot;/home/pi/demo/face_recognition/&quot;</span>+<span class="built_in">str</span>(strangerNum)+<span class="string">&quot;.png&quot;</span></span><br><span class="line">                cv2.imwrite(path,frame_1)</span><br><span class="line">                strangerNum=strangerNum+<span class="number">1</span></span><br><span class="line">                strangerAppear=<span class="literal">True</span></span><br><span class="line">            face_names.append(name)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Display the results</span></span><br><span class="line">    <span class="keyword">for</span> (top, right, bottom, left), name <span class="keyword">in</span> <span class="built_in">zip</span>(face_locations, face_names):</span><br><span class="line">        <span class="comment"># Draw a box around the face</span></span><br><span class="line">        <span class="comment">#cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Draw a label with a name below the face</span></span><br><span class="line"><span class="comment">#        cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 0, 255), 5)</span></span><br><span class="line">        font = cv2.FONT_HERSHEY_DUPLEX<span class="comment">#FONT_HERSHEY_DUPLEX</span></span><br><span class="line"><span class="comment">#        print(name)</span></span><br><span class="line"><span class="comment">#        boxsize, _ = cv2.getTextSize(fs.string, fs.face, fs.fsize, fs.thick)</span></span><br><span class="line"> <span class="comment">#       locx = int((right+left)/2-25 - 14*len(name)/2)</span></span><br><span class="line">        cv2.putText(frame, name, (<span class="number">20</span>,<span class="number">20</span>), font, <span class="number">1.0</span>, (<span class="number">0</span>, <span class="number">0</span>, <span class="number">255</span>), <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    process_this_frame += <span class="number">1</span><span class="comment"># not process_this_frame</span></span><br><span class="line">    <span class="keyword">if</span> process_this_frame&gt;<span class="number">1</span>:</span><br><span class="line">        process_this_frame=<span class="number">1</span></span><br><span class="line">    <span class="comment"># Display the resulting image</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">############</span></span><br><span class="line">    cv2.imshow(<span class="string">&#x27;Video&#x27;</span>, frame)</span><br><span class="line">    cv2.moveWindow(<span class="string">&#x27;Video&#x27;</span>,<span class="number">600</span>,<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Hit &#x27;q&#x27; on the keyboard to quit!</span></span><br><span class="line">    <span class="keyword">if</span> cv2.waitKey(<span class="number">1</span>) &amp; <span class="number">0xFF</span> == <span class="built_in">ord</span>(<span class="string">&#x27;q&#x27;</span>):</span><br><span class="line">        RPi.GPIO.output(buzzer,<span class="literal">True</span>)</span><br><span class="line">        strangerAppear=<span class="literal">False</span></span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Release handle to the webcam</span></span><br><span class="line">video_capture.release()</span><br><span class="line">cv2.destroyAllWindows()</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">入门介绍</summary>
    
    
    
    <category term="树莓派" scheme="https://values.keys.moe/categories/%E6%A0%91%E8%8E%93%E6%B4%BE/"/>
    
    
    <category term="树莓派" scheme="https://values.keys.moe/tags/%E6%A0%91%E8%8E%93%E6%B4%BE/"/>
    
    <category term="Python" scheme="https://values.keys.moe/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>树莓派CSI摄像头的连接与常用指令</title>
    <link href="https://values.keys.moe/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BECSI%E6%91%84%E5%83%8F%E5%A4%B4%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%B8%8E%E5%B8%B8%E7%94%A8%E6%8C%87%E4%BB%A4/"/>
    <id>https://values.keys.moe/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BECSI%E6%91%84%E5%83%8F%E5%A4%B4%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%B8%8E%E5%B8%B8%E7%94%A8%E6%8C%87%E4%BB%A4/</id>
    <published>2019-03-02T01:28:19.000Z</published>
    <updated>2022-09-28T06:03:44.256Z</updated>
    
    <content type="html"><![CDATA[<img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BECSI%E6%91%84%E5%83%8F%E5%A4%B4%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%B8%8E%E5%B8%B8%E7%94%A8%E6%8C%87%E4%BB%A4/CSI摄像头.jpg" alt><span id="more"></span><h1><span id="安装树莓派摄像头模块">安装树莓派摄像头模块</span></h1><p>​1、找到 CSI 接口(CSI接口在以太网接口旁边)，掀起深色胶带。<br>​2、拉起 CSI 接口挡板。<br>​3、拿起你的摄像头模块，将贴在镜头上的塑料保护膜撕掉。确保黄色部分的PCB(有字的一面)是安装完美的（可以轻轻按一下黄色的部分来保证安装完美）。<br>​4、将排线插入CSI接口。记住，有蓝色胶带的一面应该面向以太网接口方向。同样，这时也确认一下排线安装好了之后，将挡板拉下。</p><img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BECSI%E6%91%84%E5%83%8F%E5%A4%B4%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%B8%8E%E5%B8%B8%E7%94%A8%E6%8C%87%E4%BB%A4/安装图.jpg" alt><h1><span id="在树莓派上启用摄像头模块">在树莓派上启用摄像头模块</span></h1><p>在安装完摄像头模块之后，首先要确认你已经升级了树莓派系统并应用了最新的固件。<br>终端下可以输入以下命令来操作：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-<span class="built_in">get</span> <span class="keyword">update</span>   </span><br><span class="line">sudo apt-<span class="built_in">get</span> upgrade </span><br></pre></td></tr></table></figure><p>运行树莓派配置工具来激活摄像头模块：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo raspi-config</span><br></pre></td></tr></table></figure><p>移动光标至菜单中的 “Enable Camera（启用摄像头）”，将其设为Enable（启用状态）。完成之后重启树莓派。</p><h1><span id="安装驱动使能树莓派的相关模块">安装驱动使能树莓派的相关模块</span></h1><h2><span id="1-添加驱动程序文件进来">1、添加驱动程序文件进来：</span></h2><p>终端输入指令</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo <span class="keyword">vim</span> /etc/modules</span><br></pre></td></tr></table></figure><p>在最后添加如下的代码：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bcm2835-v4l2</span><br></pre></td></tr></table></figure><p>这样就完成了在启动过程中加载camera驱动的前提。</p><h2><span id="2-修改raspberry的启动配置使能项">2、修改Raspberry的启动配置使能项：</span></h2><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo raspi-config</span><br></pre></td></tr></table></figure> <img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BECSI%E6%91%84%E5%83%8F%E5%A4%B4%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%B8%8E%E5%B8%B8%E7%94%A8%E6%8C%87%E4%BB%A4/配置1.jpg" alt><p>(由于系统版本，树莓派版本不同，显示设置可能不同，但基本大同小异。)<br>选择Interfacing Option，选中Select然后Enter进入，如下图所示： </p><img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BECSI%E6%91%84%E5%83%8F%E5%A4%B4%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%B8%8E%E5%B8%B8%E7%94%A8%E6%8C%87%E4%BB%A4/配置2.jpg" alt><h2><span id="3-检查x2fdev下面是否存在摄像头设备">3、检查&#x2F;dev下面是否存在摄像头设备</span></h2><p>重启完之后，我们的基本的操作就完成了，下来来看看&#x2F;dev下面是否存在摄像头设备的问题：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">ls</span> -al /dev/ | grep video</span><br></pre></td></tr></table></figure><img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BECSI%E6%91%84%E5%83%8F%E5%A4%B4%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%B8%8E%E5%B8%B8%E7%94%A8%E6%8C%87%E4%BB%A4/检查.jpg" alt><h1><span id="使用操作树莓派的摄像头">使用操作树莓派的摄像头</span></h1><p>下面简单的使用操作树莓派的摄像头：<br>1、我们使用rapistill指令来截图</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">raspistill -<span class="keyword">o</span> image.jpg</span><br></pre></td></tr></table></figure><p>raspistill命令的相关参数和实验的具体效果：<br>-v：调试信息查看<br>-w：图像宽度<br>-h：图像高度<br>-rot：图像旋转角度，只支持 0、90、180、270 度（这里说明一下，测试发现其他角度的输入都会被转换到这四个角度之上）<br>-o：图像输出地址，例如image.jpg，如果文件名为“-”，将输出发送至标准输出设备<br>-t：获取图像前等待时间，默认为5000，即5秒<br>-tl：多久执行一次图像抓取<br>使用raspivid指令来生成.h246的文件</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">raspivid -<span class="keyword">o</span> mykeychain.h264 -t <span class="number">10000</span> -<span class="keyword">w</span> <span class="number">1280</span> -h <span class="number">720</span> </span><br></pre></td></tr></table></figure><p>如果你想改变拍摄时长，只要通过 “-t” 选项来设置你想要的长度就行了（单位是毫秒）。<br>如果你想改变图像的分辨率，使用 “-w” 和 “-h” 选项将分辨率降为 1280x720等等。</p>]]></content>
    
    
    <summary type="html">入门介绍</summary>
    
    
    
    <category term="树莓派" scheme="https://values.keys.moe/categories/%E6%A0%91%E8%8E%93%E6%B4%BE/"/>
    
    
    <category term="树莓派" scheme="https://values.keys.moe/tags/%E6%A0%91%E8%8E%93%E6%B4%BE/"/>
    
    <category term="Python" scheme="https://values.keys.moe/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>树莓派常见问题与解决方案</title>
    <link href="https://values.keys.moe/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%E4%B8%8E%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/"/>
    <id>https://values.keys.moe/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%E4%B8%8E%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/</id>
    <published>2019-03-02T01:26:05.000Z</published>
    <updated>2022-09-28T06:28:27.634Z</updated>
    
    <content type="html"><![CDATA[<p>又是老问题？（笑</p><span id="more"></span><h1><span id="树莓派有密码的网络联网失败的问题与解决">树莓派有密码的网络联网失败的问题与解决</span></h1><p><strong>在保证系统无任何修改的情况下：</strong><br>树莓派出现无法连接有密码的网络，而可以连接无密码网络的情况。<br>问题出现原因：<strong>电池电源电流供应不足</strong>。<br>解决方案：给树莓派micrioUSB处通以<strong>正常电源</strong>，问题得以解决。</p><h1><span id="修改树莓派热点的名称和密码">修改树莓派热点的名称和密码</span></h1><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo nano /etc/hostapd/hostapd.<span class="keyword">conf</span> </span><br></pre></td></tr></table></figure><p>其中ssid为热点的名称、wpa_passphrase为热点的密码<br><img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%E4%B8%8E%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/修改树莓派热点的名称和密码.jpg" alt></p><h1><span id="树莓派更换国内可用镜像源">树莓派更换国内可用镜像源</span></h1><p>***最新系统不用折腾换源了，亲测官方源可以使用。</p><p><strong>查看树莓派的镜像列表</strong><br><a href="http://www.raspbian.org/RaspbianMirrors">http://www.raspbian.org/RaspbianMirrors</a><br>操作</p><h2><span id="1-编辑sourceslist">1. 编辑sources.list</span></h2><p>打开终端 输入</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo nano /etc/apt/sources.<span class="keyword">list</span></span><br></pre></td></tr></table></figure><p>用#注释或直接删除原有的内容，新增两条：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">deb</span> http://mirrors.tuna.tsinghua.edu.<span class="keyword">cn</span>/raspbian/raspbian/ stretch main contrib non-free rpi</span><br><span class="line">#deb-src http://mirrors.tuna.tsinghua.edu.<span class="keyword">cn</span>/raspbian/raspbian/ stretch main contrib non-free rpi</span><br></pre></td></tr></table></figure><p>ctrl+x 保存并退出。</p><h2><span id="2-编辑raspilist">2. 编辑raspi.list</span></h2><p>sudo nano &#x2F;etc&#x2F;apt&#x2F;sources.list.d&#x2F;raspi.list<br>用#注释或直接删除原有的内容，新增两条：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">deb</span> http://mirror.tuna.tsinghua.edu.<span class="keyword">cn</span>/raspberrypi/ stretch main ui</span><br><span class="line">#deb-src http://mirror.tuna.tsinghua.edu.<span class="keyword">cn</span>/raspberrypi/ stretch main ui</span><br></pre></td></tr></table></figure><p>ctrl+x 保存并退出。</p><h2><span id="3-更新软件源列表">3. 更新软件源列表</span></h2><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-<span class="built_in">get</span> <span class="keyword">update</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;又是老问题？（笑&lt;/p&gt;</summary>
    
    
    
    <category term="树莓派" scheme="https://values.keys.moe/categories/%E6%A0%91%E8%8E%93%E6%B4%BE/"/>
    
    
    <category term="树莓派" scheme="https://values.keys.moe/tags/%E6%A0%91%E8%8E%93%E6%B4%BE/"/>
    
  </entry>
  
  <entry>
    <title>树莓派空气质量检测仪-攀藤G5003ST的连接与使用</title>
    <link href="https://values.keys.moe/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%E6%A3%80%E6%B5%8B%E4%BB%AA-%E6%94%80%E8%97%A4G5003ST%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%B8%8E%E4%BD%BF%E7%94%A8/"/>
    <id>https://values.keys.moe/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%E6%A3%80%E6%B5%8B%E4%BB%AA-%E6%94%80%E8%97%A4G5003ST%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%B8%8E%E4%BD%BF%E7%94%A8/</id>
    <published>2019-03-02T01:23:34.000Z</published>
    <updated>2022-09-28T05:44:45.667Z</updated>
    
    <content type="html"><![CDATA[<img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%E6%A3%80%E6%B5%8B%E4%BB%AA-%E6%94%80%E8%97%A4G5003ST%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%B8%8E%E4%BD%BF%E7%94%A8/5003ST正.jpg" alt><span id="more"></span><img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%E6%A3%80%E6%B5%8B%E4%BB%AA-%E6%94%80%E8%97%A4G5003ST%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%B8%8E%E4%BD%BF%E7%94%A8/5003ST反.jpg" alt><h1><span id="攀藤g5003st数字接口定义">攀藤G5003ST数字接口定义</span></h1><p><strong>pin1(VCC)</strong>:连接电压为3.3V&#x2F;5V电压<br><strong>pin2(GND)</strong> 电源-<br>**pin3(SET待机位置)**，设置管脚?&#x2F;TTL电平@3.3V，高电平或悬空为正常工作状态，低电平为休眠状态，该引脚可悬空。<br><strong>pin4(RXD串口接收管脚)</strong> 传感器接收来自树莓派的信号数据，如果不需要可以悬空<br><strong>pin5(TXD串口发送管脚)</strong> 传感器将信号发送给树莓派<br><strong>pin6(RESET)</strong> 模块复位信号&#x2F;TTL电平@3.3V，低复位，如果不需要使用可以悬空<br><strong>pin7(NC)</strong> : No internal connection. 无内部连接，不需连接。<br><strong>Pin8(NC&#x2F;PWM)</strong> : PWM周期为1s，其中低电平对应大气环境下的PM2.5质量浓度数据，每1ms低电平代表1ug&#x2F;m?。例如低电平时间长度为210ms，则代表此时PM2.5质量浓度值（大气环境）为210ug&#x2F;m?<br><strong>（pin7和pin8为程序内部调试使用，应用电路中应该使其悬空）</strong></p><p>树莓派Pi3的UART（ttyAMA0）是被蓝牙默认占用的，更改起来十分困难，在实体机上尝试多次无果后决定使用USB TO TTL转接口，直接将PMS5003ST接至树莓派的USB接口上，这样可以直接在&#x2F;dev&#x2F;tty中直接检索到USB0，即为传感器。</p><img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%E6%A3%80%E6%B5%8B%E4%BB%AA-%E6%94%80%E8%97%A4G5003ST%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%B8%8E%E4%BD%BF%E7%94%A8/接线.jpg" alt><img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%E6%A3%80%E6%B5%8B%E4%BB%AA-%E6%94%80%E8%97%A4G5003ST%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%B8%8E%E4%BD%BF%E7%94%A8/接线2.jpg" alt><img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%E6%A3%80%E6%B5%8B%E4%BB%AA-%E6%94%80%E8%97%A4G5003ST%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%B8%8E%E4%BD%BF%E7%94%A8/pin口.png" alt><h1><span id="攀藤g5003st技术指标">攀藤G5003ST技术指标</span></h1><img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%E6%A3%80%E6%B5%8B%E4%BB%AA-%E6%94%80%E8%97%A4G5003ST%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%B8%8E%E4%BD%BF%E7%94%A8/技术指标1.png" alt><img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%E6%A3%80%E6%B5%8B%E4%BB%AA-%E6%94%80%E8%97%A4G5003ST%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%B8%8E%E4%BD%BF%E7%94%A8/技术指标2.png" alt><img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%E6%A3%80%E6%B5%8B%E4%BB%AA-%E6%94%80%E8%97%A4G5003ST%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%B8%8E%E4%BD%BF%E7%94%A8/技术指标3.png" alt><img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%E6%A3%80%E6%B5%8B%E4%BB%AA-%E6%94%80%E8%97%A4G5003ST%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%B8%8E%E4%BD%BF%E7%94%A8/技术指标4.png" alt><h1><span id="攀藤g5003st输出结果分析">攀藤G5003ST输出结果分析</span></h1><p>1.颗粒物浓度：主要输出为单位体积内各浓度颗粒物质量以及个数，其中颗粒物个数的单位体积为0.1升，质量浓度单位为：微克&#x2F;立方米（μg&#x2F;m3）。<br>此外传感器输出分为主动输出和被动输出两种状态。<br>传感器上电后默认状态为主动输出，即传感器主动向主机发送串行数据，时间间隔为200 ~ 800ms，空气中颗粒物浓度越高，时间间隔越短。<br>主动输出又分为两种模式：平稳模式和快速模式。<br>在空气中颗粒物浓度变化较小时，传感器输出为平稳模式，即每三次输出同样的一组数值，实际数据更新周期约为2s。<br>当空气中颗粒物浓度变化较大时，传感器输出自动切换为快速模式，每次输出都是新的数值，实际数据更新周期为200~800ms。<br>PWM输出：PMS3XXXP系列产品带有PWM输出，PWM周期为1秒，低电平时间长度代表PM2.5浓度（大气环境下），每1ms低电平代表1ug&#x2F;m3。<br>例如：低电平时间长度为210ms，则代表此时PM2.5质量浓度值（大气环境）为210ug&#x2F;m3<br>2.甲醛浓度输出：单位体积内甲醛质量浓度，单位为毫克&#x2F;立方米<br>3.温湿度输出：输出吸入传感器内部的采样空气温度及湿度。</p><h1><span id="攀藤g5003st对外输出格式">攀藤G5003ST对外输出格式</span></h1><img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%E6%A3%80%E6%B5%8B%E4%BB%AA-%E6%94%80%E8%97%A4G5003ST%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%B8%8E%E4%BD%BF%E7%94%A8/输出格式0.png" alt><img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%E6%A3%80%E6%B5%8B%E4%BB%AA-%E6%94%80%E8%97%A4G5003ST%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%B8%8E%E4%BD%BF%E7%94%A8/输出格式1.png" alt><img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%E6%A3%80%E6%B5%8B%E4%BB%AA-%E6%94%80%E8%97%A4G5003ST%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%B8%8E%E4%BD%BF%E7%94%A8/输出格式2.png" alt><img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%E6%A3%80%E6%B5%8B%E4%BB%AA-%E6%94%80%E8%97%A4G5003ST%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%B8%8E%E4%BD%BF%E7%94%A8/输出格式3.png" alt><h1><span id="python程序实现">python程序实现</span></h1><p>读取数据：<br><img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%E6%A3%80%E6%B5%8B%E4%BB%AA-%E6%94%80%E8%97%A4G5003ST%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%B8%8E%E4%BD%BF%E7%94%A8/读取数据.png" alt></p><p>分析数据：<br><img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%E6%A3%80%E6%B5%8B%E4%BB%AA-%E6%94%80%E8%97%A4G5003ST%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%B8%8E%E4%BD%BF%E7%94%A8/分析数据.png" alt></p><p>得出数据结论：<br><img src="/2019/03/02/%E6%A0%91%E8%8E%93%E6%B4%BE%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%E6%A3%80%E6%B5%8B%E4%BB%AA-%E6%94%80%E8%97%A4G5003ST%E7%9A%84%E8%BF%9E%E6%8E%A5%E4%B8%8E%E4%BD%BF%E7%94%A8/数据结论.png" alt></p>]]></content>
    
    
    <summary type="html">入门介绍</summary>
    
    
    
    <category term="树莓派" scheme="https://values.keys.moe/categories/%E6%A0%91%E8%8E%93%E6%B4%BE/"/>
    
    
    <category term="树莓派" scheme="https://values.keys.moe/tags/%E6%A0%91%E8%8E%93%E6%B4%BE/"/>
    
    <category term="Python" scheme="https://values.keys.moe/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>使用lirc红外控制树莓派</title>
    <link href="https://values.keys.moe/2019/03/02/%E4%BD%BF%E7%94%A8lirc%E7%BA%A2%E5%A4%96%E6%8E%A7%E5%88%B6%E6%A0%91%E8%8E%93%E6%B4%BE/"/>
    <id>https://values.keys.moe/2019/03/02/%E4%BD%BF%E7%94%A8lirc%E7%BA%A2%E5%A4%96%E6%8E%A7%E5%88%B6%E6%A0%91%E8%8E%93%E6%B4%BE/</id>
    <published>2019-03-02T01:20:33.000Z</published>
    <updated>2022-09-28T06:09:54.964Z</updated>
    
    <content type="html"><![CDATA[<img src="/2019/03/02/%E4%BD%BF%E7%94%A8lirc%E7%BA%A2%E5%A4%96%E6%8E%A7%E5%88%B6%E6%A0%91%E8%8E%93%E6%B4%BE/红外传感器.png" alt><span id="more"></span><p>红外传感器<br>引脚从上到下分别为<br>IO GND VCC</p><h1><span id="lirc的安装与使用">lirc的安装与使用</span></h1><p>使用红外，首先需要安装树莓派的lirc模块<br>LIRC (Linux Infrared remote control)是一个linux系统下开源的软件包。这个软件可以让Linux系统接收及发送红外线信号。<br>注意事项：<br><strong>安装：</strong></p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-<span class="built_in">get</span> install lirc</span><br></pre></td></tr></table></figure><p>修改以下几处：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo leafpad /etc/lirc/hardware.<span class="keyword">conf</span></span><br><span class="line">LIRCD_ATGS=<span class="string">&quot;&quot;</span></span><br><span class="line">DRIVER=<span class="string">&quot;default&quot;</span></span><br><span class="line">DEVICE=<span class="string">&quot;/dev/lirc0&quot;</span></span><br><span class="line">MODULES=<span class="comment">&quot;lirc-rpi</span></span><br></pre></td></tr></table></figure><p>终端执行</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo leafpad /etc/modules</span><br></pre></td></tr></table></figure><p>添加下面两行到模块配置文件：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">lirc-dev</span><br><span class="line"><span class="comment">#红外接收模块的OUT接口接到了树莓派的GPIO18</span></span><br><span class="line"><span class="comment">#因为本例中未用到红外发射模块，所以后面的gpio_out_pin可以不写</span></span><br><span class="line">lirc-rpi gpio_in_pin=<span class="number">18</span> gpio_out_pin=<span class="number">17</span></span><br></pre></td></tr></table></figure><p><strong>如测试时报错-ERROR: could not insert ‘lirc_rpi’: No&amp;nbs</strong><br>解决办法：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo <span class="keyword">vi</span> /boot/config.txt</span><br></pre></td></tr></table></figure><p>找到：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">#dtoverlay=lirc-rpi</span><br></pre></td></tr></table></figure><p>把前面的“#”号去掉， 然后重启系统即可</p><p><strong>测试红外线接收功能</strong><br>首先关闭lirc软件，然后执行如下命令：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo /etc/init.d/lirc <span class="keyword">stop</span></span><br><span class="line">mode2 -d /dev/lirc0</span><br></pre></td></tr></table></figure><p>这时候提示</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">no</span> such <span class="keyword">file</span> <span class="built_in">or</span> directory</span><br></pre></td></tr></table></figure><p>经过查看发现 &#x2F;dev 下面没有 lirc0 这个module，发现在&#x2F;boot&#x2F;config.txt里面dtoverlay&#x3D;lirc-rpi<br>取消注释，然后reboot，问题解决。</p><p>再次执行</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mode2 -d /dev/lirc0</span><br></pre></td></tr></table></figure><p>如果弹出</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Partial <span class="keyword">read</span> <span class="number">8</span> bytes <span class="keyword">on</span> /dev/lirc0pi@raspberrypi:~ $</span><br></pre></td></tr></table></figure><p>发生错误，解决方案：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">changed the following two lines in </span><br><span class="line">    /etc/lirc/lirc_options.<span class="keyword">conf</span></span><br><span class="line">driver = default</span><br></pre></td></tr></table></figure><p>（尝试过程中第一次仍然无效，但是第二次重装系统后正常，目前未知原理）<br>如果显示下面内容</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">pulse <span class="number">629</span></span><br><span class="line">space <span class="number">518</span></span><br><span class="line">pulse <span class="number">627</span></span><br><span class="line">space <span class="number">523</span></span><br><span class="line">pulse <span class="number">628</span></span><br><span class="line">space <span class="number">523</span></span><br><span class="line">pulse <span class="number">631</span></span><br><span class="line">space <span class="number">517</span></span><br><span class="line">pulse <span class="number">629</span></span><br></pre></td></tr></table></figure><p>则说明接收正常.</p><p>#协议<br>采用脉宽调制的串行码，以脉宽为0.565ms、间隔0.56ms、周期为1.125ms的组合表示二进制的”0”；以脉宽为0.565ms、间隔1.685ms、周期为2.25ms的组合表示二进制的”1<br><img src="/2019/03/02/%E4%BD%BF%E7%94%A8lirc%E7%BA%A2%E5%A4%96%E6%8E%A7%E5%88%B6%E6%A0%91%E8%8E%93%E6%B4%BE/协议.png" alt><br>协议：<br>上述“0”和“1”组成的32位二进制码经38kHz的载频进行二次调制以提高发射效率，达到降低电源功耗的目的。然后再通过红外发射二极管产生红外线向空间发射，如下图。<br><img src="/2019/03/02/%E4%BD%BF%E7%94%A8lirc%E7%BA%A2%E5%A4%96%E6%8E%A7%E5%88%B6%E6%A0%91%E8%8E%93%E6%B4%BE/协议2.png" alt><br>|    引导码    |  用户识别码   |用户识别码反码 |   操作码    |  操作码反码   |<br>一个命令只发送一次，即使遥控器上的按键一直按着。但是会每110mS发送一次代码，直到遥控器按键释放。</p><p>重复码比较简单：一个9mS的AGC脉冲、2.25mS间隔、560uS脉冲。<br><img src="/2019/03/02/%E4%BD%BF%E7%94%A8lirc%E7%BA%A2%E5%A4%96%E6%8E%A7%E5%88%B6%E6%A0%91%E8%8E%93%E6%B4%BE/协议3.png" alt><br><img src="/2019/03/02/%E4%BD%BF%E7%94%A8lirc%E7%BA%A2%E5%A4%96%E6%8E%A7%E5%88%B6%E6%A0%91%E8%8E%93%E6%B4%BE/协议4.png" alt></p><h1><span id="读取并校验接收-对应到的红外信号">读取并校验接收、对应到的红外信号</span></h1><img src="/2019/03/02/%E4%BD%BF%E7%94%A8lirc%E7%BA%A2%E5%A4%96%E6%8E%A7%E5%88%B6%E6%A0%91%E8%8E%93%E6%B4%BE/代码1.png" alt>]]></content>
    
    
    <summary type="html">入门介绍</summary>
    
    
    
    <category term="树莓派" scheme="https://values.keys.moe/categories/%E6%A0%91%E8%8E%93%E6%B4%BE/"/>
    
    
    <category term="树莓派" scheme="https://values.keys.moe/tags/%E6%A0%91%E8%8E%93%E6%B4%BE/"/>
    
    <category term="Python" scheme="https://values.keys.moe/tags/Python/"/>
    
  </entry>
  
</feed>
