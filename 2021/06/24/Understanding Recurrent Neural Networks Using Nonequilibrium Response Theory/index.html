<!DOCTYPE html>
<html lang="zh">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    
    <title>Understanding Recurrent Neural Networks Using Nonequilibrium Response Theory | Angelo的代码工坊</title>
    <meta name="description" content="Angelo's blog."/>
    <meta name="keywords" content="hexo,theme,otakism,otaku"/>
    <meta name="HandheldFriendly" content="True"/>
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="google-site-verification" content=""/>
    <meta name="baidu-site-verification" content=""/>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="description" content="使用非平衡响应理论理解循环神经网络引用Lim S H . Understanding Recurrent Neural Networks Using Nonequilibrium Response Theory[J]. 2020. 摘要循环神经网络（RNN）是一种受大脑启发的模型，其广泛的应用于机器学习，以进行连续数据的分析。本工作有助于使用非平衡学说的响应理论更深度地理解RNN如何处理输入信">
<meta property="og:type" content="article">
<meta property="og:title" content="Understanding Recurrent Neural Networks Using Nonequilibrium Response Theory">
<meta property="og:url" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/index.html">
<meta property="og:site_name" content="Angelo的代码工坊">
<meta property="og:description" content="使用非平衡响应理论理解循环神经网络引用Lim S H . Understanding Recurrent Neural Networks Using Nonequilibrium Response Theory[J]. 2020. 摘要循环神经网络（RNN）是一种受大脑启发的模型，其广泛的应用于机器学习，以进行连续数据的分析。本工作有助于使用非平衡学说的响应理论更深度地理解RNN如何处理输入信">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image002.jpg">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image004.jpg">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image005.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image007.jpg">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image009.jpg">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image010.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image011.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image012.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image014.jpg">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image015.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image016.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image018.jpg">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image019.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image020.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image021.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image022.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image023.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image024.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image025.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image026.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image027.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image028.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image030.jpg">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image032.jpg">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image034.jpg">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image036.jpg">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image037.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image038.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image039.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image040.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image041.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image042.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image043.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image044.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image045.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image042.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image044.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image046.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image047.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image048.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image049.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image050.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image051.png">
<meta property="og:image" content="file:///C:/Users/Angelo/AppData/Local/Temp/msohtmlclip1/01/clip_image049.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image052.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image053.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image054.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image055.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image056.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image057.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image058.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image059.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image060.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image061.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image062.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image063.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image064.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image065.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image066.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image067.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image068.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image069.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image070.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image071.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image073.jpg">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image074.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image076.jpg">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image078.jpg">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image080.jpg">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image081.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image082.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image083.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image084.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image086.jpg">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image087.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image088.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image090.jpg">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image091.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image093.jpg">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image095.jpg">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image096.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image098.jpg">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image099.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image100.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image101.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image102.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image103.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image104.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image105.png">
<meta property="og:image" content="file:///C:/Users/Angelo/AppData/Local/Temp/msohtmlclip1/01/clip_image104.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image106.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image107.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image108.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image109.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image110.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image111.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image112.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image113.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image114.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image115.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image116.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image117.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image118.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image119.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image121.jpg">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image123.jpg">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image124.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image125.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image126.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image127.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image129.jpg">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image130.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image131.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image132.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image133.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image135.jpg">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image137.jpg">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image138.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image139.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image141.jpg">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image142.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image144.jpg">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image145.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image146.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image147.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image148.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image150.jpg">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image151.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image152.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image154.jpg">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image155.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image156.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image157.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image158.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image160.jpg">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image161.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image163.jpg">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image164.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image165.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image166.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image168.jpg">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image170.jpg">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image171.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image172.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image173.png">
<meta property="og:image" content="file:///C:/Users/Angelo/AppData/Local/Temp/msohtmlclip1/01/clip_image173.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image174.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image175.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image176.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image177.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image178.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image180.jpg">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image181.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image182.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image183.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image184.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image185.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image186.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image187.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image188.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image189.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image190.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image191.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image192.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image194.jpg">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image195.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image196.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image197.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image198.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image199.png">
<meta property="og:image" content="file:///C:/Users/Angelo/AppData/Local/Temp/msohtmlclip1/01/clip_image198.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image200.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image201.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image202.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image203.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image204.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image205.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image206.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image207.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image208.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image209.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image210.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image211.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image212.png">
<meta property="og:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image213.png">
<meta property="article:published_time" content="2021-06-24T02:27:01.000Z">
<meta property="article:modified_time" content="2023-10-22T10:40:35.075Z">
<meta property="article:author" content="Angelo">
<meta property="article:tag" content="论文阅读">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://values.keys.moe/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image002.jpg">
    
        <link rel="alternative" href="/atom.xml" title="Angelo的代码工坊" type="application/atom+xml">
    

    <!-- Favicon -->
    

    <!-- Font -->
    <link href="https://fonts.googleapis.com/css?family=Inconsolata|Roboto:300,400,700" rel="stylesheet">

    
<link rel="stylesheet" href="/style.css">

    <script>
      function setLoadingBarProgress(num) {
        document.getElementById('loading-bar').style.width = num + "%";
      }
    </script>

    
<meta name="generator" content="Hexo 6.3.0"></head>

<body>

<div id="loading-bar-wrapper">
  <div id="loading-bar"></div>
</div>

<script>setLoadingBarProgress(20)</script>

<div id="site-wrapper">

    <header id="header">
    <div id="header-wrapper" class="clearfix">
        <a id="logo" href="/">
            <img src="/img/logo.png"/>
            <span id="site-desc">
                I shut my eyes in order to see.
            </span>
        </a>
        <button id="site-nav-switch">
            <span class="icon icon-menu"></span>
        </button>
    </div>
</header>
    <script>setLoadingBarProgress(40);</script>

    <main id="main" role="main">
        <article id="post-Understanding Recurrent Neural Networks Using Nonequilibrium Response Theory"
         class="post article white-box article-type-post"
         itemscope itemprop="blogPost">
    <h2 class="title">
        <a href="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/">
            Understanding Recurrent Neural Networks Using Nonequilibrium Response Theory
        </a>
    </h2>
    <time>
        6月 24, 2021
    </time>
    <section class="content">
        <div class="article-entry" itemprop="articleBody">
            <div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image002.jpg" alt></div>

<h1><span id="使用非平衡响应理论理解循环神经网络">使用非平衡响应理论理解循环神经网络</span></h1><h1><span id="引用">引用</span></h1><p>Lim S H . Understanding Recurrent Neural Networks Using Nonequilibrium Response Theory[J]. 2020.</p>
<h2><span id="摘要">摘要</span></h2><p>循环神经网络（RNN）是一种受大脑启发的模型，其广泛的应用于机器学习，以进行连续数据的分析。本工作有助于使用非平衡学说的响应理论更深度地理解RNN如何处理输入信号。对于一类由输入信号驱动的连续时间随机RNN（SRNN），我们为其输出推导出一个沃尔泰拉级数的序列表示。这种表示法是可解释的，并将输入信号从SRNN结构中分离出来。序列的核是一些递归定义的相关函数，其与完全决定输出的无扰动动力学相关。利用这种表示的联系及其对粗糙路径理论的影响，我们确定了一个通用特征——响应特征，其被证明是输入信号的张量积的特征与自然支撑基础。特别地，我们展示了仅优化了读出层的权重，而隐藏层的权重保持固定、未被优化的SRNN，这可被看作是在与响应特征相关的再生核希尔伯特空间中执行的核机器。</p>
<span id="more"></span>

<h2><span id="介绍">介绍</span></h2><p>从时间序列分析到自然语言处理，序列化数据出现在广泛的场景中。在没有数学模型的情况下，从数据中提取有用信息，以学习一个数据生成系统是很重要的。</p>
<p>循环神经网络（RNN）是一类受大脑启发的模型，其专门为学习序列数据而设计，被广泛地应用于从物理学到金融的各个领域。RNN是具有反馈连接的神经元网络，从生物学角度比其他适应性模型更具说服力。特别地，RNN可以使用它们的隐藏状态（记忆）来处理输入的可变长度序列。它们是动力系统的通用逼近器，且其本身可被视为一类开放动力系统。</p>
<p>尽管RNN近期在储备池计算、深度学习和神经生物学方面取得了创新和巨大的经验成功，但很少有研究关注RNN工作机制的理论基础。缺乏严格的分析限制了RNN在解决科学问题方面的实用性，并可能阻碍下一代网络的系统设计。因此，深入了解该机制对于阐明大型自适应架构的特性，以及彻底改变我们对这些系统的理解而言至关重要。</p>
<p>特别地，人们可能会问的两个自然且基础的问题是：</p>
<p>Q1：随着时间推移的输入信号如何驱动RNN产生输出？</p>
<p>Q2：它们的响应是否有一个普遍的机制？</p>
<p>本工作的主要目标之一是解决上述问题，以非平衡统计动力学中的非线性响应理论为出发点，针对连续时间RNN的随机版本，简称SRNN（其隐藏状态被注入了高斯白噪声）进行分析。我们的方法是跨学科的，为现有的RNN理论增加了令人耳目一新的观点。</p>
<h2><span id="随机循环神经网络srnn">随机循环神经网络（SRNN）</span></h2><p>本文固定过滤概率空间（filtered probability space）<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image004.jpg" alt></div>，E代表对P的期望，T&gt;0。C(E, F)代表从E到F的连续映射的巴拿赫空间，其中E和F是巴拿赫空间。<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image005.png" alt></div>表示Rn上所有有界连续函数的空间。N:&#x3D;{0, 1, 2, . . . }，Z+:&#x3D;{1, 2, . . . }且R+:&#x3D; [0, ∞)。上标T表示转置，∗表示邻接。</p>
<h3><span id="模型">模型</span></h3><p>我们对我们的SRNN考虑如下模型。所谓激活函数，是指一个非常数的、利普希茨连续且有界的实值函数。激活函数的例子包括sigmoid函数，如实践中常使用的双曲切线等。</p>
<p><strong>定义2.1</strong>（连续时间SRNN）令t ∈ [0, T]，<div class="small-img"><div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image007.jpg" alt></div></div>为确定的输入信号。连续时间的SRNN描述为以下空间状态的模型：</p>
<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image009.jpg" alt></div>

<p>其中，公式1是隐藏状态<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image010.png" alt></div>的随机微分方程（SDE），带有漂移系数φ：<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image011.png" alt></div>、噪声系数<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image012.png" alt></div>和定义在<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image014.jpg" alt></div>上的r维维纳过程<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image015.png" alt></div>，而公式2定义了一个可观测的激活函数<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image016.png" alt></div>。</p>
<p>我们考虑SRNN的输入仿射版本，其中：</p>
<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image018.jpg" alt></div>

<p>其中，<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image019.png" alt></div>是正稳定的，<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image020.png" alt></div>为激活函数，<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image021.png" alt></div>和<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image022.png" alt></div>为常量，<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image023.png" alt></div>为转换输入信号的常量矩阵。</p>
<p>从现在开始，我们将 SRNN 称为由（1）-（3）定义的系统。SRNN的隐藏状态描述了一个处理输入信号的非自主随机动力系统。常数Γ、W、b、C、σ和f中的参数（如果有的话）定义了SRNN（架构）的（可学习）参数或权重。对于 T &gt; 0，与 SRNN 相关联的是输出函数 <div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image024.png" alt></div>，其定义为可观测的f的期望值（集合平均值）：</p>
<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image025.png" alt></div>

<h3><span id="srnn的非平衡响应理论">SRNN的非平衡响应理论</span></h3><p><strong>预备知识和符号</strong></p>
<p>在本小节中，我们简要回顾马尔可夫过程的预备知识并介绍我们的一些符号。</p>
<p>令t ∈ [0, T]，<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image026.png" alt></div>且<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image027.png" alt></div>是归一化的输入信号。在SRNN（1）-（3）中，我们认为信号<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image028.png" alt></div>是驱动SDE的小振幅γ（t）的扰动：</p>
<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image030.jpg" alt></div>

<p>未扰动的SDE是Cu设置为零的系统：</p>
<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image032.jpg" alt></div>

<p>其中，<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image034.jpg" alt></div>且<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image036.jpg" alt></div>。过程 h 是时间齐次马尔可夫过程<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image037.png" alt></div>的扰动，它不一定是稳定的。</p>
<p>扩散过程h和<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image038.png" alt></div>分别与一族无穷小生成元<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image039.png" alt></div>和<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image040.png" alt></div>相关，它们是二阶椭圆算子，定义为：</p>
<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image041.png" alt></div>

<p>对于任何可观察的<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image042.png" alt></div>，其中<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image043.png" alt></div>。我们将与 h 关联的转移算子<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image044.png" alt></div>定义为：</p>
<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image045.png" alt></div>

<p>对于<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image042.png" alt></div>，和转移算子<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image044.png" alt></div>（其为一个马尔科夫半群），它们都是与<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image046.png" alt></div>相关联的。</p>
<p>此外，可以在概率测度空间上定义上述生成元和转移算子的L2伴随矩阵。我们分别用<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image047.png" alt></div>和<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image048.png" alt></div>表示与h和<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image049.png" alt></div>关联的伴随生成器，分别用<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image050.png" alt></div>和<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image051.png" alt></div>表示与h和<div class="small-img"><img src="file:///C:/Users/Angelo/AppData/Local/Temp/msohtmlclip1/01/clip_image049.png" alt></div>关联的伴随转移算子。我们假设初始测度和过程定律具有关于勒贝格测度的密度。将初始密度表示为<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image052.png" alt></div>，<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image053.png" alt></div>满足与<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image054.png" alt></div>关联的前向柯尔莫果洛夫方程（FKE）。</p>
<p>我们采取自然的假设，即扰动和未扰动过程都有相同的初始分布<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image055.png" alt></div>，这通常不是无扰动动力学的不变分布<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image056.png" alt></div>。</p>
<p><strong>关键思想和形式推导</strong></p>
<p>首先，我们将推导出SRNN的输出函数在驱动输入信号方面的表示。我们的方法源于非平衡统计动力学的响应理论。在下文中，我们假设任何无限级数都是明确定义的，且求和和积分之间的任何互换都是合理的。</p>
<p>固定一个T&gt;0，令<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image057.png" alt></div>足够小并且</p>
<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image058.png" alt></div>

<p>首先，请注意概率密度<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image059.png" alt></div>的FKE是：</p>
<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image060.png" alt></div>

<p>其中<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image061.png" alt></div>，而：</p>
<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image062.png" alt></div>

<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image063.png" alt></div>

<p>关键思想是，由于ε&gt; 0很小，我们寻求形式为ρ的微扰展开：</p>
<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image064.png" alt></div>

<p>将其代入FKE并匹配ε中的阶数，我们得到以下方程层次：</p>
<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image065.png" alt></div>

<p>ρn的形式解可以通过迭代获得。形式化的描述，我们记<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image066.png" alt></div>。在不变分布是稳定的特殊情况下，<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image067.png" alt></div>与时间无关。</p>
<p>请注意，n ≥ 2时，<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image068.png" alt></div>，在n ≥ 2时，解ρn通过递归关系而得：</p>
<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image069.png" alt></div>

<p>因此，假设下面的无穷级数绝对收敛，我们有：</p>
<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image070.png" alt></div>

<p>接下来，我们考虑SRNN的隐性动力学的标量值观测值<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image071.png" alt></div>，并研究输入信号扰动引起的该观测值的平均偏差：</p>
<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image073.jpg" alt></div>

<p>对于扰动动力学的可观察值的平均值可写为：</p>
<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image074.png" alt></div>

<p>在不丧失一般性的情况下，我们在下文中取<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image076.jpg" alt></div>，即f(h)被认为是均值为零的（相对于ρinit）。</p>
<p>我们有：</p>
<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image078.jpg" alt></div>

<p>其中</p>
<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image080.jpg" alt></div>

<p>是一阶响应核，它们是相对于 ρinit 的仅无扰动动力学函数的平均值。请注意，为了获得上面的最后一行，我们分部积分并假设ρinit&gt;0。</p>
<p>该式表达了阿加瓦尔型的非平衡波动-耗散关系。在平稳不变分布的情况下，我们使用（向量值）响应核，恢复统计力学中众所周知的平衡波动-耗散关系：</p>
<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image081.png" alt></div>

<p>其中<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image082.png" alt></div>。在线性 SRNN（即φ(h, t)在h中是线性的）和f(h) &#x3D; h的特殊情况下，其可简化为<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image083.png" alt></div>的协方差函数（相对于ρ∞）。</p>
<p>到目前为止，我们已经研究了线性响应机制，其中，响应线性地依赖于输入。现在我们通过将上述推导扩展到n≥2的情况。我们表示<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image084.png" alt></div>，可得</p>
<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image086.jpg" alt></div>

<p>其中<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image087.png" alt></div>，<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image088.png" alt></div>是n阶响应核：</p>
<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image090.jpg" alt></div>

<p>且</p>
<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image091.png" alt></div>

<p>n &#x3D; 2, 3, . . .时，</p>
<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image093.jpg" alt></div>

<p>请注意，这些高阶响应核与一阶响应核类似，是相对于 ρinit 的一些仅无扰动动力学的函数的平均值。</p>
<p>基于上述结果可得：</p>
<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image095.jpg" alt></div>

<p>其中<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image096.png" alt></div>是递归定义的时间相关核。更重要的是，这些核完全由SRNN的未扰动动力学以明确的方式确定。因此，SRNN 的输出函数可以写成（实际上是唯一的）上述一系列形式。该陈述在后文中得到了精确表述，从而解决了 (Q1)。</p>
<p>现在我们关注(Q2)。通过展开技术，我们可以得到：</p>
<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image098.jpg" alt></div>

<p>其中，<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image099.png" alt></div>是与时间和信号<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image100.png" alt></div>无关的常数。该表达式以系统的方式将驱动输入信号从 SRNN 架构中分离出来。粗略地说，它告诉我们，SRNN对输入信号的响应可以通过将两部分的乘积相加得到，其中一个描述了SRNN的未扰动部分，一个是经过时间变换的输入信号的迭加积分。这一声明在后续得到了更精确的阐述，它是解决(Q2)的起点。</p>
<h2><span id="主要结果">主要结果</span></h2><h3><span id="假设">假设</span></h3><p>为了简单和直观，我们对SRNN使用以下相当严格的假设。 这些假设可以通过增加技术成本（我们不在这里追求）或通过计算近似结果来证明是合理的。</p>
<p>回想一下，我们正在处理确定性输入信号<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image101.png" alt></div>。</p>
<p><strong>假设4.1</strong> 固定T&gt;0并让U成为<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image102.png" alt></div>的开集。</p>
<p>(a) <div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image103.png" alt></div>对所有t∈[0, T]来说都是足够小的。</p>
<p>(b) 在所有<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image104.png" alt></div>时，<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image105.png" alt></div>，并且以概率1存在一个紧集K⊂U，使得在所有<div class="small-img"><img src="file:///C:/Users/Angelo/AppData/Local/Temp/msohtmlclip1/01/clip_image104.png" alt></div>情况下，<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image106.png" alt></div>。</p>
<p>(c) 系数a：<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image107.png" alt></div>和f：<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image108.png" alt></div>为分析函数。</p>
<p>(d) <div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image109.png" alt></div>是正定的，<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image110.png" alt></div>是正稳定的（即，Γ 的所有特征值的实部都是正的）。</p>
<p>(e) 初始状态<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image111.png" alt></div>是一个根据概率密度ρinit分布的随机变量。</p>
<p>假设4.1(a)意味着我们使用幅度足够小的输入信号。这对于确保某些无穷级数以足够大的收敛半径绝对收敛非常重要。(b) 和 (c) 确保一些理想的规律性和有界性。特别地，它们意味着a、f和它们所有的偏导数都是有界的，且在整个t∈[0, T]上，ht和<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image112.png" alt></div>利普希茨连续。(d) 意味着系统受到的是非退化噪声的抑制和驱动，这确保了无扰动系统可以指数稳定。(e)是我们分析的自然假设，因为h是<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image113.png" alt></div>的一个扰动。</p>
<p>除非另有说明，否则假设4.1是本文中隐含的假设。</p>
<p>进一步符号化。我们现在提供一个空间及其符号的列表：</p>
<p>* L(E1, E2)：从E1到E2的有界线性算子的巴拿赫空间（其中||·||表示适当空间上的范数）</p>
<p>* <div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image114.png" alt></div>：具有紧支撑的类<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image115.png" alt></div>的实值函数空间</p>
<p>* <div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image116.png" alt></div>：类<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image117.png" alt></div>有界实值函数空间</p>
<p>* <div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image118.png" alt></div>：<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image119.png" alt></div>上有界绝对连续度量的空间，其中<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image121.jpg" alt></div>，ρ表示度量µ的密度</p>
<p>* <div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image123.jpg" alt></div>：ρ加权的Lp空间，即函数f的空间，使得<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image124.png" alt></div>，其中ρ是加权函数。</p>
<h3><span id="srnn-输出泛函的表示方法">SRNN 输出泛函的表示方法</span></h3><p>在保证不丧失一般性的情况下，我们将在下文取p&#x3D;1并假设<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image125.png" alt></div>。</p>
<p><strong>定义4.1</strong> （响应函数） 令<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image126.png" alt></div>是一个有界的可观察对象。对于t∈[0,T]，令Ft是C([0, t],R)上的泛函，定义为<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image127.png" alt></div>，<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image129.jpg" alt></div>表示Ft相对于γ的n阶泛函导数。对于n∈Z+，如果存在局部可积函数<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image130.png" alt></div>，对于所有测试函数<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image131.png" alt></div>，使得</p>
<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image132.png" alt></div>

<p>则<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image133.png" alt></div>被称为可观测f的n阶响应函数。</p>
<p>接下来，在t∈[0,T]中，令<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image135.jpg" alt></div>是任意可观察函数，且<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image137.jpg" alt></div>。</p>
<p><strong>命题4.1</strong> （响应函数的显式表达式） 对于n∈Z+，令<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image138.png" alt></div>为f的n阶响应函数。那么，对于<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image139.png" alt></div>：</p>
<p>(a) <div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image141.jpg" alt></div></p>
<p>(b) （高阶A-FDT）此外，如果 ρinit 为正，则</p>
<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image142.png" alt></div>

<p>其中</p>
<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image144.jpg" alt></div>

<p><strong>推论4.1</strong> 令n∈Z+，且<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image145.png" alt></div>。假定在<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image146.png" alt></div>上有另一个函数<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image147.png" alt></div>，使得对于所有的<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image148.png" alt></div>，有</p>
<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image150.jpg" alt></div>

<p>那么<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image151.png" alt></div>几乎处处成立。</p>
<p><strong>定理4.1</strong> （记忆表示） 令t∈[0,T]，SRNN的输出泛函<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image152.png" alt></div>是N→∞的极限：</p>
<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image154.jpg" alt></div>

<p>其中<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image155.png" alt></div>在命题4.1中给出。该极限存在，且是唯一的收敛的沃尔泰拉级数。如果Gt是另一个具有响应函数<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image156.png" alt></div>的这样的级数，那么Ft&#x3D;Gt。</p>
<p><strong>定理4.2</strong> （无记忆表示） 假设算子<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image157.png" alt></div>有一个明确定义的本征函数展开。那么，SRNN的输出函数<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image158.png" alt></div>有一个收敛级数展开，这就是N, M→∞的极限：</p>
<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image160.jpg" alt></div>

<p>其中<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image161.png" alt></div>是常数系数，取决于pi、li、<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image163.jpg" alt></div>的特征值和特征函数、f和ρinit，但与输入信号和时间无关。在这里，pi∈{0, 1, . . . , M}、li∈{1, 2, . . . , m}。</p>
<p><strong>命题4.2</strong> （确定的深度SRNN的表示） 令Ft和Gt是两个SRNN的输出函数，相关的截断沃尔泰拉级数分别具有响应核<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image164.png" alt></div>核<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image165.png" alt></div>，n&#x3D;1,…,N，m&#x3D;1,…,M。那么<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image166.png" alt></div>是具有N+M个响应核的截断沃尔泰拉级数：</p>
<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image168.jpg" alt></div>

<p>当且仅当r&#x3D;1,…,N+M，其中</p>
<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image170.jpg" alt></div>

<p>如果Ft和Gt是沃尔泰拉级数（即N，M&#x3D;∞），则在r &#x3D; 1, 2, . . . 上，<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image171.png" alt></div>是具有上述响应核<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image172.png" alt></div>的沃尔泰拉级数（只要它是明确定义的）。</p>
<p>此外，定理4.2中的陈述适用于<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image173.png" alt></div>，即<div class="small-img"><img src="file:///C:/Users/Angelo/AppData/Local/Temp/msohtmlclip1/01/clip_image173.png" alt></div>在定理4.2的假设下允许指定形式的收敛级数展开。</p>
<p><strong>定义4.2</strong> （路径特征） 令X∈C([0, T], E)为有界变差路径。X的特征是T((E))的元素S，定义为</p>
<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image174.png" alt></div>

<p>其中</p>
<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image175.png" alt></div>

<p>当且仅当n ∈ Z+，<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image176.png" alt></div>。</p>
<p>令<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image177.png" alt></div>为<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image178.png" alt></div>的典范基，那么我们有：</p>
<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image180.jpg" alt></div>

<p>用<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image181.png" alt></div>表示对偶配对，有</p>
<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image182.png" alt></div>

<p><strong>定理4.3</strong> （特征方面的无记忆表示） 设p是一个正整数，并假设输入信号u是一个有界变差路径。那么SRNN的输出函数Ft是<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image183.png" alt></div>在p→∞的极限，其是路径特征的线性泛函，<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image184.png" alt></div>（可通过向量化与<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image185.png" alt></div>进行识别），其中<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image186.png" alt></div>，即</p>
<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image187.png" alt></div>

<p>其中，bn(t)仅取决于t的系数。</p>
<h3><span id="将srnn表述为核机器">将SRNN表述为核机器</span></h3><p>我们现在考虑一个监督学习（回归或分类）的环境，我们给定N个训练输入输出对<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image188.png" alt></div>，其中un∈χ，为<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image189.png" alt></div>中有界变差的路径空间，yn∈R，使得对于所有n，有<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image190.png" alt></div>，这里FT是一个连续目标映射。</p>
<p>考虑优化问题：</p>
<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image191.png" alt></div>

<p>其中G是具有范数<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image192.png" alt></div>的假设（巴拿赫）空间，<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image194.jpg" alt></div>为一个损失函数，R(x)是一个在x中严格增加的实值函数。</p>
<p>受定理4.3的启发（将G视为由SRNN引入的假设空间）我们将表明，该问题的解决方案可以表示为对训练样本的核扩展。</p>
<p>在下文中，考虑希尔伯特空间：</p>
<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image195.png" alt></div>

<p>其中P是适当加权的<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image196.png" alt></div>序列空间，其遵循序列形式为<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image197.png" alt></div>，其中</p>
<p>Pn(t)是[0, T]上的正交多项式。令<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image198.png" alt></div>表示H上的对称福克空间，<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image199.png" alt></div>表示L∈Z+时<div class="small-img"><img src="file:///C:/Users/Angelo/AppData/Local/Temp/msohtmlclip1/01/clip_image198.png" alt></div>的L折张量积。</p>
<p><strong>命题4.3</strong> 令L∈Z+。考虑映射<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image200.png" alt></div>，定义为：</p>
<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image201.png" alt></div>

<p>其中K是H上的核，存在一个唯一的RKHS，表示为具有范数<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image202.png" alt></div>的<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image203.png" alt></div>，其中K为再生核。</p>
<p><strong>定理4.4</strong> （表示定理） 考虑时间增加的路径<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image204.png" alt></div>，其中un是χ中<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image205.png" alt></div>值的输入路径，v是P中<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image206.png" alt></div>值向量。那么：</p>
<p>(a) 假设空间为<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image207.png" alt></div>的前文所述优化问题的任何解都允许以下形式的表示：</p>
<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image208.png" alt></div>

<p>其中cn∈R，N是训练输入-输出对的数量。</p>
<p>(b) 令L ∈ Z+。如果我们转而考虑路径，表示为<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image209.png" alt></div>，在时间ti∈[0, T]上，通过对L+1个数据点进行线性插值获得<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image210.png" alt></div>，则相应优化问题的任何解都具有<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image211.png" alt></div>的假设空间，表示形式为：</p>
<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image212.png" alt></div>

<p>其中αn∈R，l&#x3D;1,…,L时，<div class="small-img"><img src="/2021/06/24/Understanding%20Recurrent%20Neural%20Networks%20Using%20Nonequilibrium%20Response%20Theory/clip_image213.png" alt></div>。</p>
<h2><span id="结论">结论</span></h2><p>在本文中，我们使用非平衡统计动力学的非线性响应理论作为起点，解决了关于一类随机循环神经网络 (SRNN) 的两个基本问题，这些网络可以是人工或生物网络的模型。特别地，我们能够以系统的、逐级的方式来描述SRNN对扰动的确定性输入信号的响应，为这些SRNN的输出函数推导出两种类型的序列表示，以及在驱动输入信号方面的深度变体。这提供了对由这些驱动网络所引起的记忆和无记忆表示的性质的探究。此外，通过将这些表示与路径特征的概念联系起来，我们发现响应特征集是 SRNN 在处理输入信号时从中提取信息的构建块，揭示了SRNN运行的普遍机制。特别地，我们通过表示定理表明，SRNN可以被看作是在与响应特征相关的再生核希尔伯特空间上运行的核机器。</p>
<p>从数学的角度来看，放宽这里的假设，并在驱动输入信号是粗略路径的一般设置中工作会很有趣，输入信号的规律性可能会发挥重要作用。人们还可以通过采用此处开发的技术来研究 SRNN 如何响应输入信号和噪声驱动（正则化）中的扰动。到目前为止，我们一直专注于介绍中提到的“公式化优先”方法。这里获得的结果表明，可以通过设计有效的算法来利用离散化响应特征和相关特征在涉及时间数据的机器学习任务中的使用，来研究”离散化的下一步”，例如在科学与工程中预测由复杂动力系统产生的时间序列。</p>

        </div>
        <div class="article-tags tags">
            
                <a class="tag-none-link" href="/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" rel="tag">论文阅读</a>
            
        </div>
    </section>
</article>





        <script>setLoadingBarProgress(60);</script>
    </main>

    <footer id="footer" class="clearfix">

    

    <div class="social-wrapper">
        
            
                <a href="liuguandi@126.com" class="social email"
                   target="_blank" rel="external">
                    <span class="icon icon-email"></span>
                </a>
            
                <a href="https://github.com/Liuguandi" class="social github"
                   target="_blank" rel="external">
                    <span class="icon icon-github"></span>
                </a>
            
                <a href="/atom.xml" class="social rss"
                   target="_blank" rel="external">
                    <span class="icon icon-rss"></span>
                </a>
            
        
    </div>

    <div class="theme-by">Theme <span class="codename">Memory</span> designed by <a href="https://artifact.me/"
                                                                                    target="_blank">Art Chen</a>, modified by <a href="https://values.keys.moe/"
                                                                                    target="_blank">Angelo</a>.
    </div>
    <div>&copy; <a href="/">Angelo的代码工坊</a></div>

</footer>


    <script>setLoadingBarProgress(80);</script>
    <div class="overlay"></div>
</div>

<div class="site-sidebar" id="site-sidebar">

    

    <div class="sidebar-switch clearfix "
         style="display: none">
        <a class="dark-btn active" data-toggle="toc">
            <span class="icon icon-list"></span>
            <span class="text">Index</span>
        </a>
        <a class="dark-btn" data-toggle="bio">
            <span class="icon icon-person"></span>
            <span class="text">Bio</span>
        </a>
    </div>

    <div class="site-toc "
         style="display: none">
        
            <div class="no-index">No Index</div>
        
    </div>

    <div class="site-bio show"
         style="display: block">

        <div class="about-me clearfix">

            <div class="title"><a href="/">Angelo的代码工坊</a></div>
            
            <p class="subtitle">
                I shut my eyes in order to see.
            </p>
            

            <div class="info">
                <!-- <a class="name dark-btn" href="/about"> -->
                    <!-- Angelo -->
                <!-- </a> -->
                <div class="item desc" style="padding-right:20px">
                    <div class="description">
                        身处寒夜，把握星光。
                    </div>
                    <div class="author">
                        Angelo
                    </div>
                </div>

                <div class="avatar">
                    <a href="/About">
                        <img src="/img/avatar.png"/>
                    </a>
                </div>
            </div>
                

            
    
        </div>

        <!-- <div class="menu section">
            <ul class="clearfix">
                
                    <li class="left">
                        <a href="/About"
                           onfocus="this.blur();"
                           class="nav-about dark-btn block">
                            About
                        </a>
                    </li>
                
                    <li class="right">
                        <a href="/archives"
                           onfocus="this.blur();"
                           class="nav-archives dark-btn block">
                            Archives
                        </a>
                    </li>
                
            </ul>
        </div> -->


        <div class="body">
            
              
                <ul class="nav">
                  
                    
                      <li class="category-list-container">
                        <a href="javascript:;">Category</a>
                        <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Ubuntu/">Ubuntu</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9D%82%E8%B0%88/">杂谈</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%A0%91%E8%8E%93%E6%B4%BE/">树莓派</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">论文阅读</a><span class="category-list-count">6</span></li></ul>
                      </li>
                    
                  
                    
                      <li class="tag-list-container">
                        <a href="javascript:;">Tag</a>
                        <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Python/" rel="tag">Python</a><span class="tag-list-count">6</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Ubuntu/" rel="tag">Ubuntu</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9D%82%E8%B0%88/" rel="tag">杂谈</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%A0%91%E8%8E%93%E6%B4%BE/" rel="tag">树莓派</a><span class="tag-list-count">7</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" rel="tag">论文阅读</a><span class="tag-list-count">6</span></li></ul>
                      </li>
                    
                  
                    
                      <li class="archive-list-container">
                        <a href="javascript:;">Archive</a>
                        <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/">2021</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/">2020</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/">2019</a><span class="archive-list-count">8</span></li></ul>
                      </li>
                    
                  
                </ul>
              
                <ul class="nav">
                  
                    
                      <li>
                        <a href="/" title="Blog" external="false">Blog</a>
                      </li>
                    
                  
                    
                      <li>
                        <a href="/archives" title="By Year" external="false">By Year</a>
                      </li>
                    
                  
                </ul>
              
                <ul class="nav">
                  
                    
                      <li>
                        <a href="https://github.com/Liuguandi" title="Github" target="_blank" rel="noopener">Github</a>
                      </li>
                    
                  
                    
                      <li>
                        <a href="/atom.xml" title="RSS" external="false">RSS</a>
                      </li>
                    
                  
                </ul>
              
                <ul class="nav">
                  
                    
                      <li>
                        <a href="https://keys.moe/" title="Home" target="_blank" rel="noopener">Home</a>
                      </li>
                    
                  
                    
                      <li>
                        <a href="https://values.keys.moe/About/" title="About" target="_blank" rel="noopener">About</a>
                      </li>
                    
                  
                </ul>
              
            
          </div>

    </div>

    <div class="shortcuts">
        <a href="#header" class="top window-nav dark-btn" id="go-top">
            <span class="icon icon-chevron-thin-up"></span>
        </a>
        <a class="close dark-btn" id="sidebar-close">
            <span class="icon icon-close"></span>
        </a>
        <a href="#footer" class="top window-nav dark-btn" id="go-bottom">
            <span class="icon icon-chevron-thin-down"></span>
        </a>
    </div>

</div>





<script src="https://code.jquery.com/jquery-2.1.4.min.js"></script>
<script>window.jQuery || document.write('<script src="/js/jquery.min.js"><\/script>')</script>


<script src="/js/jquery.fitvids.js"></script>

<script>
  var GOOGLE_CUSTOM_SEARCH_API_KEY = "";
  var GOOGLE_CUSTOM_SEARCH_ENGINE_ID = "";
  var ALGOLIA_API_KEY = "";
  var ALGOLIA_APP_ID = "";
  var ALGOLIA_INDEX_NAME = "";
  var AZURE_SERVICE_NAME = "";
  var AZURE_INDEX_NAME = "";
  var AZURE_QUERY_KEY = "";
  var SEARCH_SERVICE = "";
  var universalSearchConfig = {};
  if (SEARCH_SERVICE === 'google') {
    universalSearchConfig = {
      searchService: SEARCH_SERVICE,
      apiKey: GOOGLE_CUSTOM_SEARCH_API_KEY,
      engineId: GOOGLE_CUSTOM_SEARCH_ENGINE_ID,
      imagePath: "/img/"
    };
  } else if (SEARCH_SERVICE === 'algolia') {
    universalSearchConfig = {
      searchService: SEARCH_SERVICE,
      apiKey: ALGOLIA_API_KEY,
      appId: ALGOLIA_APP_ID,
      indexName: ALGOLIA_INDEX_NAME,
      imagePath: "/img/"
    };
  } else if (SEARCH_SERVICE === 'azure') {
    universalSearchConfig = {
      searchService: SEARCH_SERVICE,
      serviceName: AZURE_SERVICE_NAME,
      indexName: AZURE_INDEX_NAME,
      apiKey: AZURE_QUERY_KEY,
      imagePath: "/img/"
    };
  }
</script>

<script src="/js/app.js"></script>


<script src="/js/search.js"></script>





<script src="/scripts/main.js"></script>

<script>setLoadingBarProgress(100);</script>

</body>
</html>
